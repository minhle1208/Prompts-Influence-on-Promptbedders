{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13012708,"sourceType":"datasetVersion","datasetId":8238371}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q bitsandbytes peft accelerate\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip -q install huggingface_hub==0.25.2 faiss-cpu ir_datasets >/dev/null\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install rank_bm25\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip -q install ir_datasets\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers -U","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, json, math, random\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Any, Optional\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom tqdm import tqdm\n\nfrom transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\nimport bitsandbytes as bnb\nfrom peft import LoraConfig, get_peft_model, TaskType\n\nTRAIN_JSONL   = \"/kaggle/input/final-dataset-train/instructions_one_best_per_id.jsonl\"\nOUT_DIR       = \"/kaggle/working/mE5_lora8bit_infoNCE_v1\"\nBASE_MODEL    = \"intfloat/multilingual-e5-base\"    \n\nSEED          = 42\nEPOCHS        = 1\nBATCH_SIZE    = 8        \nACCUM_STEPS   = 1\nMAX_LEN       = 256\nLR            = 1e-4      \nWARMUP_RATIO  = 0.1\nWEIGHT_DECAY  = 0.0\nTEMP          = 0.05\nMIX_HARD_NEGS = True\nMAX_HNEG      = 4\n\nLORA_R        = 16\nLORA_ALPHA    = 32\nLORA_DROPOUT  = 0.05\nLORA_TARGETS  = [\"query\",\"key\",\"value\",\"dense\"]   \n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nrandom.seed(SEED); torch.manual_seed(SEED)\nif DEVICE == \"cuda\":\n    torch.cuda.manual_seed_all(SEED)\n\nos.makedirs(OUT_DIR, exist_ok=True)\nprint(\"Device:\", DEVICE)\n\ndef load_jsonl(path: str) -> List[Dict[str, Any]]:\n    out = []\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line: continue\n            try:\n                out.append(json.loads(line))\n            except:\n                pass\n    return out\n\nrows = load_jsonl(TRAIN_JSONL)\nprint(\"Loaded rows:\", len(rows))\n\nclass JSONLRetrievalDataset(Dataset):\n    \n    def __init__(self, data: List[Dict[str,Any]], max_hard_negs: int = MAX_HNEG):\n        self.items = []\n        for r in data:\n            q = str(r.get(\"query_ru\") or \"\").strip()\n            inst = str(r.get(\"instruction\") or \"\").strip()\n            p = str(r.get(\"positive_ru\") or \"\").strip()\n            if not q or not p:\n                continue\n            negs = r.get(\"hard_negs_ru\") or []\n            negs = [str(x).strip() for x in negs if str(x).strip()][:max_hard_negs]\n            self.items.append({\n                \"q\": f\"query: {q} {inst}\".strip(),\n                \"p\": f\"passage: {p}\".strip(),\n                \"negs\": [f\"passage: {t}\" for t in negs]\n            })\n    def __len__(self): return len(self.items)\n    def __getitem__(self, idx): return self.items[idx]\n\ndataset = JSONLRetrievalDataset(rows)\nprint(\"Samples:\", len(dataset))\n\ntok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n\n@dataclass\nclass Batch:\n    q: Dict[str, torch.Tensor]\n    p: Dict[str, torch.Tensor]\n    n: Optional[Dict[str, torch.Tensor]]\n    B: int\n\ndef collate(items: List[Dict[str,Any]]) -> Batch:\n    q_texts = [it[\"q\"] for it in items]\n    p_texts = [it[\"p\"] for it in items]\n    n_texts = [n for it in items for n in it[\"negs\"]] if MIX_HARD_NEGS else []\n\n    q = tok(q_texts, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n    p = tok(p_texts, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n    n = tok(n_texts, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\") if n_texts else None\n    return Batch(q, p, n, len(items))\n\nidx = list(range(len(dataset)))\nrandom.Random(SEED).shuffle(idx)\nvalN = max(200, int(0.1 * len(idx)))\nval_ids, train_ids = idx[:valN], idx[valN:]\ntrain_loader = DataLoader(Subset(dataset, train_ids), batch_size=BATCH_SIZE, shuffle=True, drop_last=True, collate_fn=collate)\nval_loader   = DataLoader(Subset(dataset, val_ids),   batch_size=32,       shuffle=False,                 collate_fn=collate)\n\ndef load_lora_8bit(model_name: str):\n    torch.cuda.empty_cache()\n    base = AutoModel.from_pretrained(\n        model_name,\n        load_in_8bit=True,\n        device_map=\"auto\",          \n        low_cpu_mem_usage=True,\n    )\n    lcfg = LoraConfig(\n        task_type=TaskType.FEATURE_EXTRACTION,   \n        r=LORA_R,\n        lora_alpha=LORA_ALPHA,\n        lora_dropout=LORA_DROPOUT,\n        target_modules=LORA_TARGETS,             \n        bias=\"none\",\n    )\n    peft_model = get_peft_model(base, lcfg)\n    peft_model.print_trainable_parameters()\n    return peft_model\n\nmodel = load_lora_8bit(BASE_MODEL)\nmodel.train()\n\ndef mean_pooling(last_hidden_state, attention_mask):\n    mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n    return (last_hidden_state * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1e-9)\n\ndef encode_batch(enc_dict: Dict[str, torch.Tensor]) -> torch.Tensor:\n    dev = next(model.parameters()).device\n    for k in enc_dict:\n        enc_dict[k] = enc_dict[k].to(dev)\n    out = model(**enc_dict).last_hidden_state\n    emb = mean_pooling(out, enc_dict[\"attention_mask\"])\n    return F.normalize(emb, p=2, dim=1)\n\ntrainable_params = [p for p in model.parameters() if p.requires_grad]\noptimizer = bnb.optim.AdamW8bit(trainable_params, lr=LR, weight_decay=WEIGHT_DECAY)\n\nsteps_per_epoch = max(1, len(train_loader) // ACCUM_STEPS)\ntotal_steps = steps_per_epoch * EPOCHS\nscheduler = get_linear_schedule_with_warmup(optimizer, int(WARMUP_RATIO * total_steps), total_steps)\n\nce = nn.CrossEntropyLoss()\n\ndef local_validate():\n    model.eval()\n    hit1=rec=mrr=ndcg=n=0.0\n    with torch.no_grad():\n        for b in val_loader:\n            q = encode_batch(b.q)   # [B,d]\n            p = encode_batch(b.p)   # [B,d]\n            sims = (q @ p.t())      # [B,B]\n            for i in range(sims.size(0)):\n                row = sims[i]\n                ranks = torch.argsort(row, descending=True)\n                pos_rank = (ranks == i).nonzero(as_tuple=False).item() + 1\n                k = 10\n                hit1 += 1.0 if pos_rank == 1 else 0.0\n                rec  += 1.0 if pos_rank <= k else 0.0\n                mrr  += (1.0 / pos_rank) if pos_rank <= k else 0.0\n                ndcg += (1.0 / math.log2(1 + pos_rank)) if pos_rank <= k else 0.0\n                n += 1\n    model.train()\n    if n == 0: return {}\n    return {\"N\": int(n), \"Hit@1\": hit1/n, \"Recall@10\": rec/n, \"MRR@10\": mrr/n, \"nDCG@10\": ndcg/n}\n\nbest_mrr = -1.0\nglobal_step = 0\n\nfor epoch in range(1, EPOCHS+1):\n    print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n    running = 0.0\n    optimizer.zero_grad(set_to_none=True)\n    pbar = tqdm(enumerate(train_loader, start=1), total=len(train_loader))\n\n    for step_idx, b in pbar:\n        q = encode_batch(b.q)                \n        p = encode_batch(b.p)               \n        logits = (q @ p.t()) / TEMP          \n        labels = torch.arange(b.B, device=logits.device)\n        loss = ce(logits, labels)\n\n        if MIX_HARD_NEGS and b.n is not None and b.n[\"input_ids\"].numel() > 0:\n            n_emb = encode_batch(b.n)                       \n            cand  = torch.cat([p, n_emb], dim=0)            \n            logits2 = (q @ cand.t()) / TEMP\n            loss2 = ce(logits2, labels)\n            loss = 0.5 * loss + 0.5 * loss2\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(trainable_params, max_norm=1.0)\n\n        if (step_idx % ACCUM_STEPS) == 0:\n            optimizer.step()\n            optimizer.zero_grad(set_to_none=True)\n            scheduler.step()\n            global_step += 1\n\n        running += loss.item()\n        if step_idx % 50 == 0:\n            pbar.set_postfix_str(f\"loss={running/50:.4f} lr={scheduler.get_last_lr()[0]:.2e}\")\n            running = 0.0\n\n    ep_dir = os.path.join(OUT_DIR, f\"epoch{epoch}\")\n    os.makedirs(ep_dir, exist_ok=True)\n    model.save_pretrained(ep_dir)\n    tok.save_pretrained(ep_dir)\n\n    metrics = local_validate()\n    print(\"[VAL]\", {k:(round(v,4) if isinstance(v,float) else v) for k,v in metrics.items()})\n    if metrics and metrics.get(\"MRR@10\", -1) > best_mrr:\n        best_mrr = metrics[\"MRR@10\"]\n        best_dir = os.path.join(OUT_DIR, \"best\")\n        os.makedirs(best_dir, exist_ok=True)\n        model.save_pretrained(best_dir)\n        tok.save_pretrained(best_dir)\n        print(f\"[BEST] MRR@10={best_mrr:.4f} saved -> {best_dir}\")\n\nmodel.save_pretrained(OUT_DIR)\ntok.save_pretrained(OUT_DIR)\nprint(\"Saved to:\", OUT_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T00:18:00.944136Z","iopub.execute_input":"2025-09-10T00:18:00.944403Z","iopub.status.idle":"2025-09-10T00:21:45.608621Z","shell.execute_reply.started":"2025-09-10T00:18:00.944373Z","shell.execute_reply":"2025-09-10T00:21:45.608010Z"}},"outputs":[{"name":"stderr","text":"2025-09-10 00:18:12.721125: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757463492.951430     147 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757463493.015057     147 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Device: cuda\nLoaded rows: 2052\nSamples: 2052\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"609cea6ba2d248eab770f2108f4cf878"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"980e1baf7ce24f7287a51bea7d8e905d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02e089378fd14184aabb2fd02646b657"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25b5142856584051b4b4e764cdc103f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"219fd5ec04fa46e8abe54d1b99c2c8ae"}},"metadata":{}},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4cd3582162f4873a32be92bc95ccb6e"}},"metadata":{}},{"name":"stdout","text":"trainable params: 2,678,784 || all params: 280,722,432 || trainable%: 0.9542\n\nEpoch 1/1\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 230/230 [03:06<00:00,  1.23it/s, loss=0.1943 lr=1.45e-05]\n","output_type":"stream"},{"name":"stdout","text":"[VAL] {'N': 205, 'Hit@1': 0.9951, 'Recall@10': 1.0, 'MRR@10': 0.9976, 'nDCG@10': 0.9982}\n[BEST] MRR@10=0.9976 saved -> /kaggle/working/mE5_lora8bit_infoNCE_v1/best\nSaved to: /kaggle/working/mE5_lora8bit_infoNCE_v1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os, json, math, random\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Any, Optional\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom tqdm import tqdm\n\nfrom transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\nimport bitsandbytes as bnb\nfrom peft import LoraConfig, get_peft_model, TaskType\n\nTRAIN_JSONL   = \"/kaggle/input/final-dataset-train/instructions_one_best_per_id.jsonl\"\nOUT_DIR       = \"/kaggle/working/mE5_lora8bit_queryonly_v1\"\nBASE_MODEL    = \"intfloat/multilingual-e5-base\"  \n\nSEED          = 42\nEPOCHS        = 1\nBATCH_SIZE    = 8          \nACCUM_STEPS   = 1\nMAX_LEN       = 256        \nLR            = 1e-4       \nWARMUP_RATIO  = 0.1\nWEIGHT_DECAY  = 0.0\nTEMP          = 0.05\nMIX_HARD_NEGS = True\nMAX_HNEG      = 4\n\nLORA_R        = 16\nLORA_ALPHA    = 32\nLORA_DROPOUT  = 0.05\nLORA_TARGETS  = [\"query\",\"key\",\"value\",\"dense\"]\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nrandom.seed(SEED); torch.manual_seed(SEED)\nif DEVICE == \"cuda\":\n    torch.cuda.manual_seed_all(SEED)\n\nos.makedirs(OUT_DIR, exist_ok=True)\nprint(\"Device:\", DEVICE)\n\ndef load_jsonl(path: str) -> List[Dict[str, Any]]:\n    out = []\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line: continue\n            try: out.append(json.loads(line))\n            except: pass\n    return out\n\nrows = load_jsonl(TRAIN_JSONL)\nprint(\"Loaded rows:\", len(rows))\n\nclass QueryOnlyRetrievalDataset(Dataset):\n\n    def __init__(self, data: List[Dict[str,Any]], max_hard_negs: int = MAX_HNEG):\n        self.items = []\n        for r in data:\n            q = str(r.get(\"query_ru\") or \"\").strip()\n            p = str(r.get(\"positive_ru\") or \"\").strip()\n            if not q or not p:\n                continue\n            negs = r.get(\"hard_negs_ru\") or []\n            negs = [str(x).strip() for x in negs if str(x).strip()][:max_hard_negs]\n            self.items.append({\n                \"q\": f\"query: {q}\".strip(),         \n                \"p\": f\"passage: {p}\".strip(),\n                \"negs\": [f\"passage: {t}\" for t in negs]\n            })\n    def __len__(self): return len(self.items)\n    def __getitem__(self, idx): return self.items[idx]\n\ndataset = QueryOnlyRetrievalDataset(rows)\nprint(\"Samples:\", len(dataset))\n\ntok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n\n@dataclass\nclass Batch:\n    q: Dict[str, torch.Tensor]\n    p: Dict[str, torch.Tensor]\n    n: Optional[Dict[str, torch.Tensor]]\n    B: int\n\ndef collate(items: List[Dict[str,Any]]) -> Batch:\n    q_texts = [it[\"q\"] for it in items]\n    p_texts = [it[\"p\"] for it in items]\n    n_texts = [n for it in items for n in it[\"negs\"]] if MIX_HARD_NEGS else []\n\n    q = tok(q_texts, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n    p = tok(p_texts, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n    n = tok(n_texts, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\") if n_texts else None\n    return Batch(q, p, n, len(items))\n\nidx = list(range(len(dataset)))\nrandom.Random(SEED).shuffle(idx)\nvalN = max(200, int(0.1 * len(idx)))\nval_ids, train_ids = idx[:valN], idx[valN:]\ntrain_loader = DataLoader(Subset(dataset, train_ids), batch_size=BATCH_SIZE, shuffle=True, drop_last=True, collate_fn=collate)\nval_loader   = DataLoader(Subset(dataset, val_ids),   batch_size=32,       shuffle=False,                 collate_fn=collate)\n\ndef load_lora_8bit(model_name: str):\n    torch.cuda.empty_cache()\n    base = AutoModel.from_pretrained(\n        model_name,\n        load_in_8bit=True,\n        device_map=\"auto\",\n        low_cpu_mem_usage=True,\n    )\n    lcfg = LoraConfig(\n        task_type=TaskType.FEATURE_EXTRACTION,\n        r=LORA_R,\n        lora_alpha=LORA_ALPHA,\n        lora_dropout=LORA_DROPOUT,\n        target_modules=LORA_TARGETS,\n        bias=\"none\",\n    )\n    peft_model = get_peft_model(base, lcfg)\n    peft_model.print_trainable_parameters()\n    return peft_model\n\nmodel = load_lora_8bit(BASE_MODEL)\nmodel.train()\n\ndef mean_pooling(last_hidden_state, attention_mask):\n    mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n    return (last_hidden_state * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1e-9)\n\ndef encode_batch(enc_dict: Dict[str, torch.Tensor]) -> torch.Tensor:\n    dev = next(model.parameters()).device\n    for k in enc_dict:\n        enc_dict[k] = enc_dict[k].to(dev)\n    out = model(**enc_dict).last_hidden_state\n    emb = mean_pooling(out, enc_dict[\"attention_mask\"])\n    return F.normalize(emb, p=2, dim=1)\n\ntrainable_params = [p for p in model.parameters() if p.requires_grad]\noptimizer = bnb.optim.AdamW8bit(trainable_params, lr=LR, weight_decay=WEIGHT_DECAY)\n\nsteps_per_epoch = max(1, len(train_loader) // ACCUM_STEPS)\ntotal_steps = steps_per_epoch * EPOCHS\nscheduler = get_linear_schedule_with_warmup(optimizer, int(WARMUP_RATIO * total_steps), total_steps)\n\nce = nn.CrossEntropyLoss()\n\ndef local_validate():\n    model.eval()\n    hit1=rec=mrr=ndcg=n=0.0\n    with torch.no_grad():\n        for b in val_loader:\n            q = encode_batch(b.q)   # [B,d]\n            p = encode_batch(b.p)   # [B,d]\n            sims = (q @ p.t())      # [B,B]\n            for i in range(sims.size(0)):\n                row = sims[i]\n                ranks = torch.argsort(row, descending=True)\n                pos_rank = (ranks == i).nonzero(as_tuple=False).item() + 1\n                k = 10\n                hit1 += 1.0 if pos_rank == 1 else 0.0\n                rec  += 1.0 if pos_rank <= k else 0.0\n                mrr  += (1.0 / pos_rank) if pos_rank <= k else 0.0\n                ndcg += (1.0 / math.log2(1 + pos_rank)) if pos_rank <= k else 0.0\n                n += 1\n    model.train()\n    if n == 0: return {}\n    return {\"N\": int(n), \"Hit@1\": hit1/n, \"Recall@10\": rec/n, \"MRR@10\": mrr/n, \"nDCG@10\": ndcg/n}\n\nbest_mrr = -1.0\nglobal_step = 0\n\nfor epoch in range(1, EPOCHS+1):\n    print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n    running = 0.0\n    optimizer.zero_grad(set_to_none=True)\n    pbar = tqdm(enumerate(train_loader, start=1), total=len(train_loader))\n\n    for step_idx, b in pbar:\n        q = encode_batch(b.q)                \n        p = encode_batch(b.p)                \n        logits = (q @ p.t()) / TEMP          \n        labels = torch.arange(b.B, device=logits.device)\n        loss = ce(logits, labels)\n\n        if MIX_HARD_NEGS and b.n is not None and b.n[\"input_ids\"].numel() > 0:\n            n_emb = encode_batch(b.n)                       \n            cand  = torch.cat([p, n_emb], dim=0)            \n            logits2 = (q @ cand.t()) / TEMP\n            loss2 = ce(logits2, labels)\n            loss = 0.5 * loss + 0.5 * loss2\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(trainable_params, max_norm=1.0)\n\n        if (step_idx % ACCUM_STEPS) == 0:\n            optimizer.step()\n            optimizer.zero_grad(set_to_none=True)\n            scheduler.step()\n            global_step += 1\n\n        running += loss.item()\n        if step_idx % 50 == 0:\n            pbar.set_postfix_str(f\"loss={running/50:.4f} lr={scheduler.get_last_lr()[0]:.2e}\")\n            running = 0.0\n\n    ep_dir = os.path.join(OUT_DIR, f\"epoch{epoch}\")\n    os.makedirs(ep_dir, exist_ok=True)\n    model.save_pretrained(ep_dir)\n    tok.save_pretrained(ep_dir)\n\n    metrics = local_validate()\n    print(\"[VAL]\", {k:(round(v,4) if isinstance(v,float) else v) for k,v in metrics.items()})\n    if metrics and metrics.get(\"MRR@10\", -1) > best_mrr:\n        best_mrr = metrics[\"MRR@10\"]\n        best_dir = os.path.join(OUT_DIR, \"best\")\n        os.makedirs(best_dir, exist_ok=True)\n        model.save_pretrained(best_dir)\n        tok.save_pretrained(best_dir)\n        print(f\"[BEST] MRR@10={best_mrr:.4f} saved -> {best_dir}\")\n\nmodel.save_pretrained(OUT_DIR)\ntok.save_pretrained(OUT_DIR)\nprint(\"Saved to:\", OUT_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T00:22:23.316183Z","iopub.execute_input":"2025-09-10T00:22:23.316887Z","iopub.status.idle":"2025-09-10T00:25:33.539083Z","shell.execute_reply.started":"2025-09-10T00:22:23.316835Z","shell.execute_reply":"2025-09-10T00:25:33.538222Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nLoaded rows: 2052\nSamples: 2052\n","output_type":"stream"},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 2,678,784 || all params: 280,722,432 || trainable%: 0.9542\n\nEpoch 1/1\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 230/230 [03:03<00:00,  1.25it/s, loss=0.4411 lr=1.45e-05]\n","output_type":"stream"},{"name":"stdout","text":"[VAL] {'N': 205, 'Hit@1': 0.9854, 'Recall@10': 0.9951, 'MRR@10': 0.9902, 'nDCG@10': 0.9915}\n[BEST] MRR@10=0.9902 saved -> /kaggle/working/mE5_lora8bit_queryonly_v1/best\nSaved to: /kaggle/working/mE5_lora8bit_queryonly_v1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import ir_datasets as irds, json\n\nout_dir = \"mrtydi_ru_prepared\"\nimport os; os.makedirs(out_dir, exist_ok=True)\n\nds = irds.load(\"mr-tydi/ru/test\")  \n\nwith open(f\"{out_dir}/corpus.jsonl\", \"w\", encoding=\"utf-8\") as fc:\n    for d in ds.docs_iter():\n        j = {\"_id\": str(d.doc_id), \"title\": getattr(d, \"title\", \"\") or \"\", \"text\": d.text or \"\"}\n        fc.write(json.dumps(j, ensure_ascii=False) + \"\\n\")\n\nwith open(f\"{out_dir}/queries.jsonl\", \"w\", encoding=\"utf-8\") as fq:\n    for q in ds.queries_iter():\n        j = {\"_id\": str(q.query_id), \"text\": q.text}\n        fq.write(json.dumps(j, ensure_ascii=False) + \"\\n\")\n\nwith open(f\"{out_dir}/qrels.tsv\", \"w\", encoding=\"utf-8\") as fr:\n    for r in ds.qrels_iter():\n        rel = int(getattr(r, \"relevance\", 1) > 0)\n        fr.write(f\"{r.query_id}\\t{r.doc_id}\\t{rel}\\n\")\n\nprint(\"Wrote:\", out_dir, \"->\", os.listdir(out_dir))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T00:27:14.455680Z","iopub.execute_input":"2025-09-10T00:27:14.456032Z","iopub.status.idle":"2025-09-10T00:30:55.296123Z","shell.execute_reply.started":"2025-09-10T00:27:14.456007Z","shell.execute_reply":"2025-09-10T00:30:55.295299Z"}},"outputs":[{"name":"stderr","text":"[INFO] If you have a local copy of https://git.uwaterloo.ca/jimmylin/mr.tydi/-/raw/master/data/mrtydi-v1.0-russian.tar.gz, you can symlink it here to avoid downloading it again: /root/.ir_datasets/downloads/fab64459133bc93a0bec2f0559bfb423\n[INFO] [starting] https://git.uwaterloo.ca/jimmylin/mr.tydi/-/raw/master/data/mrtydi-v1.0-russian.tar.gz\n[INFO] [finished] https://git.uwaterloo.ca/jimmylin/mr.tydi/-/raw/master/data/mrtydi-v1.0-russian.tar.gz: [00:25] [1.55GB] [61.0MB/s]\n[INFO] [starting] extracting from tar file                                                                         \n[INFO] [finished] extracting from tar file [2.52s]\n","output_type":"stream"},{"name":"stdout","text":"Wrote: mrtydi_ru_prepared -> ['corpus.jsonl', 'qrels.tsv', 'queries.jsonl']\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os, json, random\n\nDATA_DIR = \"/kaggle/working/mrtydi_ru_prepared\"  # same as in your eval script\nQ_PATH   = os.path.join(DATA_DIR, \"queries.jsonl\")\nOUT_PATH = os.path.join(DATA_DIR, \"instructions.jsonl\")\n\nTEMPLATES = [\n    \"Ты — поисковая система. Найди наиболее релевантные документы, которые помогают ответить на запрос. Возвращай документы, а не готовый ответ.\",\n    \"Найди статьи/пассажи, содержащие информацию для ответа на вопрос. Отдавай приоритет точности и контекстной релевантности.\",\n    \"Выполни поиск по корпусу и верни документы с наибольшей вероятностью содержать ответ. Избегай нерелевантных совпадений.\"\n]\n\nrandom.seed(42)\nn = 0\nwith open(Q_PATH, \"r\", encoding=\"utf-8\") as fin, open(OUT_PATH, \"w\", encoding=\"utf-8\") as fout:\n    for line in fin:\n        j = json.loads(line)\n        qid = str(j.get(\"_id\") or j.get(\"id\"))\n        if not qid: \n            continue\n        instr = TEMPLATES[hash(qid) % len(TEMPLATES)]   # stable pseudo-random choice per qid\n        out = {\"_id\": qid, \"instruction\": instr}\n        fout.write(json.dumps(out, ensure_ascii=False) + \"\\n\")\n        n += 1\n\nprint(f\"Wrote {n} instructions to:\", OUT_PATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T01:00:54.692621Z","iopub.execute_input":"2025-09-10T01:00:54.692980Z","iopub.status.idle":"2025-09-10T01:00:54.709971Z","shell.execute_reply.started":"2025-09-10T01:00:54.692955Z","shell.execute_reply":"2025-09-10T01:00:54.709162Z"}},"outputs":[{"name":"stdout","text":"Wrote 995 instructions to: /kaggle/working/mrtydi_ru_prepared/instructions.jsonl\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Mr.TyDi-RU eval (BM25 + Base + LoRA adapters)\n#  - p-MRR@k = MRR@k(q+instr) − MRR@k(q) \n# ===========================================================\nimport os, io, json, gzip, math, random, re, time, hashlib\nfrom collections import defaultdict\nfrom typing import Dict, List, Tuple\n\nos.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\nos.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom rank_bm25 import BM25Okapi\nfrom transformers import AutoTokenizer, AutoModel\nfrom peft import PeftModel\n\nDEVICE         = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE_DOC = 64\nBATCH_SIZE_Q   = 64\nMAX_LEN        = 256\nK_LIST         = [1, 5, 10]\nSEED           = 42\n\nDOC_CAP     = 20000     \nFAST_Q_CAP  = None       \n\nUSE_PREFIXES = True\n\n# Models\nBASE_MODEL          = \"intfloat/multilingual-e5-base\"\nFINETUNED_INST_DIR  = \"/kaggle/working/mE5_lora8bit_infoNCE_v1/best\"       \nFINETUNED_QUERY_DIR = \"/kaggle/working/mE5_lora8bit_queryonly_v1/best\"    \n\nLOCAL_DATA_DIR = \"/kaggle/working/mrtydi_ru_prepared\"\nINSTR_JSONL    = os.path.join(LOCAL_DATA_DIR, \"instructions.jsonl\")\n\nROOT_OUT  = \"/kaggle/working/mrtydi_ru_eval\"\nEMB_CACHE = os.path.join(ROOT_OUT, \"emb_cache\")\nos.makedirs(ROOT_OUT, exist_ok=True)\nos.makedirs(EMB_CACHE, exist_ok=True)\n\nrandom.seed(SEED); np.random.seed(SEED)\nprint(f\"Device: {DEVICE} | DOC_CAP={DOC_CAP} | FAST_Q_CAP={FAST_Q_CAP}\")\n\ndef tic(msg: str):\n    print(msg, end=\"\", flush=True)\n    return time.time()\n\ndef toc(t0: float, prefix: str = \"done\"):\n    dt = time.time() - t0\n    print(f\" {prefix} in {dt:.1f}s.\")\n\ndef smart_open(path):\n    if path.endswith(\".gz\"):\n        f = gzip.open(path, \"rb\")\n        return io.TextIOWrapper(f, encoding=\"utf-8\")\n    return open(path, \"r\", encoding=\"utf-8\")\n\ndef _tqdm_lines(path: str, desc: str):\n    \"\"\"Yield lines and update tqdm by bytes read to show ETA without pre-counting lines.\"\"\"\n    total = os.path.getsize(path) if os.path.exists(path) else None\n    with smart_open(path) as f, tqdm(total=total, unit=\"B\", unit_scale=True, desc=desc) as pbar:\n        for line in f:\n            pbar.update(len(line.encode(\"utf-8\")))\n            yield line\n\ndef load_corpus(path: str) -> Dict[str, str]:\n    corpus = {}\n    for line in _tqdm_lines(path, \"Loading corpus\"):\n        j = json.loads(line)\n        cid   = str(j.get(\"_id\") or j.get(\"id\"))\n        title = (j.get(\"title\") or \"\").strip()\n        text  = (j.get(\"text\")  or \"\").strip()\n        corpus[cid] = (title + \" \" + text).strip()\n    return corpus\n\ndef load_queries(path: str) -> Dict[str, str]:\n    queries = {}\n    for line in _tqdm_lines(path, \"Loading queries\"):\n        j = json.loads(line)\n        qid  = str(j.get(\"_id\") or j.get(\"id\"))\n        text = (j.get(\"text\") or \"\").strip()\n        queries[qid] = text\n    return queries\n\ndef load_instructions_optional(path: str) -> Dict[str, str]:\n    \"\"\"Optional JSONL with instruction text per qid. Tries several key names.\"\"\"\n    if not os.path.exists(path):\n        return {}\n    inst_map = {}\n    for line in _tqdm_lines(path, \"Loading instructions\"):\n        j = json.loads(line)\n        qid = str(j.get(\"_id\") or j.get(\"id\") or j.get(\"qid\") or \"\").strip()\n        instr = (j.get(\"instruction\") or j.get(\"instruction_og\") or j.get(\"instr\") or \"\").strip()\n        if qid and instr:\n            inst_map[qid] = instr\n    return inst_map\n\ndef load_qrels_tsv(path: str) -> Dict[str, Dict[str, int]]:\n    qrels = defaultdict(dict)\n    for row in _tqdm_lines(path, \"Loading qrels\"):\n        row = row.strip()\n        if not row or row.startswith(\"#\"): continue\n        parts = row.split(\"\\t\")\n        if len(parts) < 3: continue\n        qid, did, rel = parts[0].strip(), parts[1].strip(), parts[2].strip()\n        try:\n            rel = int(float(rel))\n        except:\n            continue\n        if rel > 0:\n            qrels[qid][did] = 1\n    return dict(qrels)\n\nfor fn in [\"corpus.jsonl\", \"queries.jsonl\", \"qrels.tsv\"]:\n    if not os.path.exists(os.path.join(LOCAL_DATA_DIR, fn)):\n        raise FileNotFoundError(f\"Missing {fn} under {LOCAL_DATA_DIR}\")\n\nt0 = tic(\"Loading dataset files…\")\ncorpus  = load_corpus(os.path.join(LOCAL_DATA_DIR, \"corpus.jsonl\"))\nqueries = load_queries(os.path.join(LOCAL_DATA_DIR, \"queries.jsonl\"))\nqrels   = load_qrels_tsv(os.path.join(LOCAL_DATA_DIR, \"qrels.tsv\"))\ninstructions = load_instructions_optional(INSTR_JSONL)  # may be empty\ntoc(t0, prefix=f\"Loaded (docs={len(corpus):,}, queries={len(queries):,}, qrels_q={len(qrels):,}, instr_q={len(instructions):,})\")\n\nrng = random.Random(SEED)\n\nqids_all = [qid for qid in qrels.keys() if qid in queries]\n\nall_pos_doc_ids = set()\nfor qid in qids_all:\n    all_pos_doc_ids.update(qrels[qid].keys())\n\npos_doc_ids = [d for d in all_pos_doc_ids if d in corpus]\n\nif DOC_CAP is None:\n    doc_ids = list(corpus.keys())\n    neg_added = \"N/A\"\nelse:\n    if len(pos_doc_ids) >= DOC_CAP:\n        rng.shuffle(pos_doc_ids)\n        doc_ids = pos_doc_ids[:DOC_CAP]\n        neg_added = 0\n    else:\n        need = DOC_CAP - len(pos_doc_ids)\n        neg_pool = [d for d in corpus.keys() if d not in all_pos_doc_ids]\n        rng.shuffle(neg_pool)\n        neg_added = min(need, len(neg_pool))\n        doc_ids = pos_doc_ids + neg_pool[:neg_added]\n\ncorpus = {d: corpus[d] for d in doc_ids}\ndoc_texts_raw  = [corpus[d] for d in doc_ids]\n\nqids_all = [qid for qid in qids_all if any((did in corpus) for did in qrels[qid].keys())]\n\nif FAST_Q_CAP and FAST_Q_CAP < len(qids_all):\n    rng.shuffle(qids_all)\n    qids_all = qids_all[:FAST_Q_CAP]\n\nnum_q_with_pos = len(qids_all)\nprint(\n    f\"Eval set -> docs: {len(doc_ids):,} \"\n    f\"(positives kept: {len(pos_doc_ids):,}, random negatives added: {neg_added}) \"\n    f\"| queries: {num_q_with_pos:,}\"\n)\nif num_q_with_pos == 0:\n    raise RuntimeError(\"No queries left after subsetting. Increase DOC_CAP, disable caps, or verify your qrels/doc ids.\")\n\ndef add_prefix(s: str, kind: str) -> str:\n    if not USE_PREFIXES: return s\n    return ((\"query: \" if kind == \"query\" else \"passage: \") + s).strip()\n\ndoc_texts_pref = [add_prefix(t, \"passage\") for t in doc_texts_raw]\nquery_map_raw  = {qid: queries[qid] for qid in qids_all}\nquery_map_pref_plain = {qid: add_prefix(queries[qid], \"query\") for qid in qids_all}\n\nqids_with_instr = [qid for qid in qids_all if qid in instructions]\nquery_map_pref_instr = {qid: add_prefix((queries[qid] + \" \" + instructions[qid]).strip(), \"query\")\n                        for qid in qids_with_instr}\n\ndef eval_multi_k(run, qrels_map, ks):\n    ks = sorted(set(ks))\n    stats = {\"N\": 0, \"Hit@1\": 0.0}\n    for k in ks:\n        stats.update({f\"Recall@{k}\":0.0, f\"MRR@{k}\":0.0, f\"nDCG@{k}\":0.0})\n    n = 0\n    for qid, ranking in run.items():\n        if qid not in qrels_map: continue\n        n += 1\n        rels = qrels_map[qid]\n        pos_ranks = [r for r,(did,_) in enumerate(ranking, start=1) if did in rels]\n        pos_ranks.sort()\n        if pos_ranks and pos_ranks[0] == 1:\n            stats[\"Hit@1\"] += 1.0\n        for k in ks:\n            stats[f\"Recall@{k}\"] += float(any(r <= k for r in pos_ranks))\n            stats[f\"MRR@{k}\"]    += (1.0/pos_ranks[0]) if (pos_ranks and pos_ranks[0] <= k) else 0.0\n            dcg  = sum(1.0/math.log2(r+1) for r in pos_ranks if r <= k)\n            m    = len(rels)\n            idcg = sum(1.0/math.log2(r+1) for r in range(1, min(m, k)+1))\n            stats[f\"nDCG@{k}\"] += (dcg/idcg) if idcg > 0 else 0.0\n    if n == 0:\n        return stats\n    stats[\"N\"] = n\n    for k in [\"Hit@1\"]+[f\"Recall@{x}\" for x in ks]+[f\"MRR@{x}\" for x in ks]+[f\"nDCG@{x}\" for x in ks]:\n        stats[k] /= n\n    return stats\n\ndef fmt(d): \n    return {k:(round(v,4) if isinstance(v,float) else v) for k,v in d.items()}\n\ndef pmrr(metrics_plain: Dict[str,float], metrics_instr: Dict[str,float], ks: List[int]) -> Dict[str,float]:\n    out = {}\n    for k in ks:\n        a = metrics_plain.get(f\"MRR@{k}\", 0.0)\n        b = metrics_instr.get(f\"MRR@{k}\", 0.0)\n        out[f\"pMRR@{k}\"] = b - a\n    return out\n\ndef mean_pool(last_hidden, attention_mask):\n    m = attention_mask.unsqueeze(-1).expand_as(last_hidden).float()\n    return (last_hidden * m).sum(1) / m.sum(1).clamp(min=1e-9)\n\ndef corpus_signature(doc_ids: List[str]) -> str:\n    head = \"||\".join(doc_ids[:50]); tail = \"||\".join(doc_ids[-50:]) if len(doc_ids) > 50 else \"\"\n    return hashlib.md5((head + \"|\" + tail + f\"|{len(doc_ids)}\").encode()).hexdigest()[:8]\n\ndef signature(model_or_adapter: str) -> str:\n    name = os.path.basename(model_or_adapter.rstrip(\"/\")) if os.path.isdir(model_or_adapter) else model_or_adapter.replace(\"/\", \"_\")\n    return f\"{corpus_signature(doc_ids)}__L{MAX_LEN}__{name}\"\n\ndef load_tok_model_any(model_or_adapter: str, base_model: str):\n    \"\"\"Load tokenizer + model\"\"\"\n    is_adapter = os.path.isdir(model_or_adapter) and os.path.exists(os.path.join(model_or_adapter, \"adapter_config.json\"))\n    tok_src = model_or_adapter if (os.path.isdir(model_or_adapter) and os.path.exists(os.path.join(model_or_adapter, \"tokenizer.json\"))) else base_model\n    tok = AutoTokenizer.from_pretrained(tok_src, use_fast=True)\n\n    def _load_base(to_device: str):\n        dtype = torch.float16 if to_device == \"cuda\" else None\n        return AutoModel.from_pretrained(base_model, torch_dtype=dtype, low_cpu_mem_usage=True).to(to_device).eval()\n\n    try:\n        base = _load_base(\"cuda\" if DEVICE == \"cuda\" else \"cpu\")\n    except RuntimeError as e:\n        print(f\"[warn] GPU load failed ({type(e).__name__}): falling back to CPU.\")\n        base = _load_base(\"cpu\")\n\n    if is_adapter:\n        mdl = PeftModel.from_pretrained(base, model_or_adapter).eval()\n    else:\n        mdl = base\n    return tok, mdl\n\n@torch.no_grad()\ndef encode_texts(mdl, tok, texts: List[str], bs: int, max_len: int) -> torch.Tensor:\n    dev = next(mdl.parameters()).device\n    outs = []\n    for i in tqdm(range(0, len(texts), bs), total=(len(texts)+bs-1)//bs, desc=f\"Encode@{dev}\"):\n        batch = texts[i:i+bs]\n        enc = tok(batch, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\")\n        for k in enc: enc[k] = enc[k].to(dev)\n        last = mdl(**enc).last_hidden_state\n        pooled = mean_pool(last, enc[\"attention_mask\"])\n        outs.append(F.normalize(pooled, p=2, dim=1).cpu())\n    return torch.cat(outs, 0) if outs else torch.empty(0, mdl.config.hidden_size)\n\ndef build_index(doc_embs_cpu: torch.Tensor):\n    try:\n        import faiss\n        index = faiss.IndexFlatIP(doc_embs_cpu.shape[1])\n        index.add(doc_embs_cpu.numpy().astype(\"float32\"))\n        return index, True\n    except Exception:\n        return doc_embs_cpu, False\n\n@torch.no_grad()\ndef search_index(index, is_faiss: bool, q_embs: torch.Tensor, topk: int):\n    if is_faiss:\n        import faiss\n        D, I = index.search(q_embs.numpy().astype(\"float32\"), topk)\n        return I, D\n    sims = torch.mm(q_embs, index.t())\n    D, I = torch.topk(sims, k=min(topk, sims.size(1)), dim=1)\n    return I.numpy(), D.numpy()\n\ndef eval_model(model_or_adapter: str, label: str,\n               queries_map_pref: Dict[str,str], qids_list: List[str]):\n    print(f\"\\n=== {label} ===\")\n    t0 = tic(\"Loading model/tokenizer…\")\n    tok, mdl = load_tok_model_any(model_or_adapter, BASE_MODEL)\n    toc(t0)\n\n    sig = signature(model_or_adapter)\n    cache_path = os.path.join(EMB_CACHE, f\"docs__{sig}.pt\")\n\n    if os.path.exists(cache_path):\n        doc_embs = torch.load(cache_path, map_location=\"cpu\")\n        print(f\"[cache] doc embs loaded: {doc_embs.shape}\")\n    else:\n        t1 = tic(f\"Encoding docs (N={len(doc_texts_pref):,})…\")\n        doc_embs = encode_texts(mdl, tok, doc_texts_pref, BATCH_SIZE_DOC, MAX_LEN)\n        toc(t1)\n        torch.save(doc_embs, cache_path)\n        print(f\"[cache] saved -> {cache_path}\")\n\n    index, is_faiss = build_index(doc_embs)\n\n    q_texts = [queries_map_pref[qid] for qid in qids_list]\n    t2 = tic(f\"Encoding queries (N={len(q_texts):,})…\")\n    q_embs  = encode_texts(mdl, tok, q_texts, BATCH_SIZE_Q, MAX_LEN)\n    toc(t2)\n\n    t3 = tic(\"Searching…\")\n    I, D = search_index(index, is_faiss, q_embs, topk=max(K_LIST))\n    toc(t3)\n\n    run = {}\n    for i, qid in enumerate(qids_list):\n        ids = I[i]; scores = D[i]\n        run[qid] = [(doc_ids[j], float(scores[k])) for k, j in enumerate(ids)]\n\n    m = eval_multi_k(run, qrels, K_LIST)\n    print(\"Metrics:\", fmt(m))\n    return m\n\n# BM25 (raw text, no prefixes)\ntoken_re = re.compile(r\"[a-zа-яё0-9]+\", re.IGNORECASE)\ndef ru_tok(s: str): return token_re.findall(s.lower())\n\ndef bm25_run(doc_texts: List[str], doc_ids: List[str], qids: List[str], queries_map: Dict[str, str], topk: int):\n    toks = []\n    for t in tqdm(doc_texts, desc=\"BM25 tokenize docs\", total=len(doc_texts)):\n        toks.append(ru_tok(t))\n    t0 = tic(\"Building BM25 index…\")\n    bm = BM25Okapi(toks)\n    toc(t0)\n\n    run = {}\n    for qid in tqdm(qids, desc=\"BM25 search\", total=len(qids)):\n        q_tokens = ru_tok(queries_map[qid])\n        scores = bm.get_scores(q_tokens)\n        if topk >= len(doc_ids):\n            idx = np.argsort(scores)[::-1]\n        else:\n            part = np.argpartition(scores, -topk)[-topk:]\n            idx = part[np.argsort(scores[part])[::-1]]\n        run[qid] = [(doc_ids[j], float(scores[j])) for j in idx]\n    return run\n\nresults = {}\n\n# 1) BM25 (plain)\nprint(\"\\n>>> BM25 (plain)\")\nbm_run_plain = bm25_run(doc_texts_raw, doc_ids, qids_all, query_map_raw, topk=max(K_LIST))\nresults[\"bm25_plain\"] = eval_multi_k(bm_run_plain, qrels, K_LIST)\nprint(\"BM25 (plain) ->\", fmt(results[\"bm25_plain\"]))\n\nif qids_with_instr:\n    print(\"\\n>>> BM25 (q+instr)  [on queries with instructions only]\")\n    raw_instr_map = {qid: (queries[qid] + \" \" + instructions[qid]).strip() for qid in qids_with_instr}\n    bm_run_instr = bm25_run(doc_texts_raw, doc_ids, qids_with_instr, raw_instr_map, topk=max(K_LIST))\n    results[\"bm25_instr\"] = eval_multi_k(bm_run_instr, qrels, K_LIST)\n    print(\"BM25 (instr) ->\", fmt(results[\"bm25_instr\"]))\n\n# 2) Baseline encoder (no adapters) — plain\nresults[\"base_plain\"] = eval_model(BASE_MODEL, \"Baseline (mE5-base) :: plain\", query_map_pref_plain, qids_all)\n\n# 2b) Baseline encoder — q+instr \nif qids_with_instr:\n    results[\"base_instr\"] = eval_model(BASE_MODEL, \"Baseline (mE5-base) :: q+instr\", query_map_pref_instr, qids_with_instr)\n\n# 3) LoRA adapters\nif os.path.isdir(FINETUNED_INST_DIR):\n    results[\"ft_inst_plain\"] = eval_model(FINETUNED_INST_DIR, \"Finetuned (instruction-aware, LoRA) :: plain\", query_map_pref_plain, qids_all)\n    if qids_with_instr:\n        results[\"ft_inst_instr\"] = eval_model(FINETUNED_INST_DIR, \"Finetuned (instruction-aware, LoRA) :: q+instr\", query_map_pref_instr, qids_with_instr)\nelse:\n    print(f\"[skip] {FINETUNED_INST_DIR} not found.\")\n\nif os.path.isdir(FINETUNED_QUERY_DIR):\n    results[\"ft_query_plain\"] = eval_model(FINETUNED_QUERY_DIR, \"Finetuned (query-only, LoRA) :: plain\", query_map_pref_plain, qids_all)\n    if qids_with_instr:\n        results[\"ft_query_instr\"] = eval_model(FINETUNED_QUERY_DIR, \"Finetuned (query-only, LoRA) :: q+instr\", query_map_pref_instr, qids_with_instr)\nelse:\n    print(f\"[skip] {FINETUNED_QUERY_DIR} not found.\")\n\n# Summary\nprint(\"\\n=== SUMMARY (Mr.TyDi-RU) ===\")\nfor k, v in results.items():\n    print(f\"{k:14s} ->\", fmt(v))\n\n# p-MRR \nif qids_with_instr:\n    print(\"\\n=== p-MRR (instr − plain) on queries with instructions only ===\")\n    def safe_pmrr(tag_plain, tag_instr):\n        if tag_plain in results and tag_instr in results:\n            return pmrr(results[tag_plain], results[tag_instr], K_LIST)\n        return None\n\n    pmrr_base = safe_pmrr(\"base_plain\", \"base_instr\")\n    if pmrr_base:\n        print(\"Baseline:\", fmt(pmrr_base))\n\n    pmrr_ft_inst = safe_pmrr(\"ft_inst_plain\", \"ft_inst_instr\")\n    if pmrr_ft_inst:\n        print(\"FT-Inst:\", fmt(pmrr_ft_inst))\n\n    pmrr_ft_q = safe_pmrr(\"ft_query_plain\", \"ft_query_instr\")\n    if pmrr_ft_q:\n        print(\"FT-Query:\", fmt(pmrr_ft_q))\n\n    pmrr_bm25 = safe_pmrr(\"bm25_plain\", \"bm25_instr\")\n    if pmrr_bm25:\n        print(\"BM25:   \", fmt(pmrr_bm25))\nelse:\n    print(\"\\n[p-MRR] No instructions.jsonl found or no overlap with qrels — skipping p-MRR.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T01:00:57.329306Z","iopub.execute_input":"2025-09-10T01:00:57.329599Z","iopub.status.idle":"2025-09-10T01:05:36.030237Z","shell.execute_reply.started":"2025-09-10T01:00:57.329576Z","shell.execute_reply":"2025-09-10T01:05:36.029494Z"}},"outputs":[{"name":"stdout","text":"Device: cuda | DOC_CAP=20000 | FAST_Q_CAP=None\nLoading dataset files…","output_type":"stream"},{"name":"stderr","text":"Loading corpus: 100%|██████████| 5.84G/5.84G [01:19<00:00, 73.4MB/s]\nLoading queries: 100%|██████████| 113k/113k [00:00<00:00, 29.2MB/s]\nLoading qrels: 100%|██████████| 19.0k/19.0k [00:00<00:00, 8.20MB/s]\nLoading instructions: 100%|██████████| 268k/268k [00:00<00:00, 56.0MB/s]\n","output_type":"stream"},{"name":"stdout","text":" Loaded (docs=9,597,504, queries=995, qrels_q=995, instr_q=995) in 79.6s.\nEval set -> docs: 20,000 (positives kept: 1,100, random negatives added: 18900) | queries: 995\n\n>>> BM25 (plain)\n","output_type":"stream"},{"name":"stderr","text":"BM25 tokenize docs: 100%|██████████| 20000/20000 [00:00<00:00, 20717.27it/s]","output_type":"stream"},{"name":"stdout","text":"Building BM25 index…","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":" done in 0.4s.\n","output_type":"stream"},{"name":"stderr","text":"BM25 search: 100%|██████████| 995/995 [00:38<00:00, 25.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"BM25 (plain) -> {'N': 995, 'Hit@1': 0.4171, 'Recall@1': 0.4171, 'MRR@1': 0.4171, 'nDCG@1': 0.4171, 'Recall@5': 0.5548, 'MRR@5': 0.4713, 'nDCG@5': 0.4676, 'Recall@10': 0.599, 'MRR@10': 0.477, 'nDCG@10': 0.4813}\n\n>>> BM25 (q+instr)  [on queries with instructions only]\n","output_type":"stream"},{"name":"stderr","text":"BM25 tokenize docs: 100%|██████████| 20000/20000 [00:00<00:00, 45590.95it/s]","output_type":"stream"},{"name":"stdout","text":"Building BM25 index…","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":" done in 0.5s.\n","output_type":"stream"},{"name":"stderr","text":"BM25 search: 100%|██████████| 995/995 [02:04<00:00,  7.97it/s]","output_type":"stream"},{"name":"stdout","text":"BM25 (instr) -> {'N': 995, 'Hit@1': 0.3789, 'Recall@1': 0.3789, 'MRR@1': 0.3789, 'nDCG@1': 0.3789, 'Recall@5': 0.5025, 'MRR@5': 0.4265, 'nDCG@5': 0.4215, 'Recall@10': 0.5457, 'MRR@10': 0.4322, 'nDCG@10': 0.4346}\n\n=== Baseline (mE5-base) :: plain ===\nLoading model/tokenizer…","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":" done in 3.2s.\n[cache] doc embs loaded: torch.Size([20000, 768])\nEncoding queries (N=995)…","output_type":"stream"},{"name":"stderr","text":"Encode@cuda:0: 100%|██████████| 16/16 [00:00<00:00, 31.24it/s]","output_type":"stream"},{"name":"stdout","text":" done in 0.5s.\nSearching…","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":" done in 0.1s.\nMetrics: {'N': 995, 'Hit@1': 0.808, 'Recall@1': 0.808, 'MRR@1': 0.808, 'nDCG@1': 0.808, 'Recall@5': 0.9357, 'MRR@5': 0.862, 'nDCG@5': 0.8686, 'Recall@10': 0.9588, 'MRR@10': 0.8653, 'nDCG@10': 0.8782}\n\n=== Baseline (mE5-base) :: q+instr ===\nLoading model/tokenizer… done in 3.1s.\n[cache] doc embs loaded: torch.Size([20000, 768])\nEncoding queries (N=995)…","output_type":"stream"},{"name":"stderr","text":"Encode@cuda:0: 100%|██████████| 16/16 [00:00<00:00, 18.40it/s]","output_type":"stream"},{"name":"stdout","text":" done in 0.9s.\nSearching…","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":" done in 0.1s.\nMetrics: {'N': 995, 'Hit@1': 0.7779, 'Recall@1': 0.7779, 'MRR@1': 0.7779, 'nDCG@1': 0.7779, 'Recall@5': 0.9156, 'MRR@5': 0.8352, 'nDCG@5': 0.8426, 'Recall@10': 0.9357, 'MRR@10': 0.8381, 'nDCG@10': 0.8507}\n\n=== Finetuned (instruction-aware, LoRA) :: plain ===\nLoading model/tokenizer… done in 1.8s.\n[cache] doc embs loaded: torch.Size([20000, 768])\nEncoding queries (N=995)…","output_type":"stream"},{"name":"stderr","text":"Encode@cuda:0: 100%|██████████| 16/16 [00:01<00:00, 14.88it/s]","output_type":"stream"},{"name":"stdout","text":" done in 1.1s.\nSearching…","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":" done in 0.1s.\nMetrics: {'N': 995, 'Hit@1': 0.7357, 'Recall@1': 0.7357, 'MRR@1': 0.7357, 'nDCG@1': 0.7357, 'Recall@5': 0.8975, 'MRR@5': 0.803, 'nDCG@5': 0.8124, 'Recall@10': 0.9276, 'MRR@10': 0.8071, 'nDCG@10': 0.8233}\n\n=== Finetuned (instruction-aware, LoRA) :: q+instr ===\nLoading model/tokenizer… done in 1.7s.\n[cache] doc embs loaded: torch.Size([20000, 768])\nEncoding queries (N=995)…","output_type":"stream"},{"name":"stderr","text":"Encode@cuda:0: 100%|██████████| 16/16 [00:01<00:00,  8.43it/s]","output_type":"stream"},{"name":"stdout","text":" done in 1.9s.\nSearching…","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":" done in 0.1s.\nMetrics: {'N': 995, 'Hit@1': 0.7477, 'Recall@1': 0.7477, 'MRR@1': 0.7477, 'nDCG@1': 0.7477, 'Recall@5': 0.8894, 'MRR@5': 0.8084, 'nDCG@5': 0.8135, 'Recall@10': 0.9216, 'MRR@10': 0.8126, 'nDCG@10': 0.8257}\n\n=== Finetuned (query-only, LoRA) :: plain ===\nLoading model/tokenizer… done in 1.8s.\n[cache] doc embs loaded: torch.Size([20000, 768])\nEncoding queries (N=995)…","output_type":"stream"},{"name":"stderr","text":"Encode@cuda:0: 100%|██████████| 16/16 [00:01<00:00, 14.99it/s]","output_type":"stream"},{"name":"stdout","text":" done in 1.1s.\nSearching…","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":" done in 0.1s.\nMetrics: {'N': 995, 'Hit@1': 0.7518, 'Recall@1': 0.7518, 'MRR@1': 0.7518, 'nDCG@1': 0.7518, 'Recall@5': 0.8975, 'MRR@5': 0.8131, 'nDCG@5': 0.8201, 'Recall@10': 0.9286, 'MRR@10': 0.8171, 'nDCG@10': 0.8308}\n\n=== Finetuned (query-only, LoRA) :: q+instr ===\nLoading model/tokenizer… done in 1.8s.\n[cache] doc embs loaded: torch.Size([20000, 768])\nEncoding queries (N=995)…","output_type":"stream"},{"name":"stderr","text":"Encode@cuda:0: 100%|██████████| 16/16 [00:01<00:00,  8.39it/s]","output_type":"stream"},{"name":"stdout","text":" done in 1.9s.\nSearching…","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":" done in 0.1s.\nMetrics: {'N': 995, 'Hit@1': 0.7427, 'Recall@1': 0.7427, 'MRR@1': 0.7427, 'nDCG@1': 0.7427, 'Recall@5': 0.8915, 'MRR@5': 0.8046, 'nDCG@5': 0.8118, 'Recall@10': 0.9166, 'MRR@10': 0.8081, 'nDCG@10': 0.8217}\n\n=== SUMMARY (Mr.TyDi-RU) ===\nbm25_plain     -> {'N': 995, 'Hit@1': 0.4171, 'Recall@1': 0.4171, 'MRR@1': 0.4171, 'nDCG@1': 0.4171, 'Recall@5': 0.5548, 'MRR@5': 0.4713, 'nDCG@5': 0.4676, 'Recall@10': 0.599, 'MRR@10': 0.477, 'nDCG@10': 0.4813}\nbm25_instr     -> {'N': 995, 'Hit@1': 0.3789, 'Recall@1': 0.3789, 'MRR@1': 0.3789, 'nDCG@1': 0.3789, 'Recall@5': 0.5025, 'MRR@5': 0.4265, 'nDCG@5': 0.4215, 'Recall@10': 0.5457, 'MRR@10': 0.4322, 'nDCG@10': 0.4346}\nbase_plain     -> {'N': 995, 'Hit@1': 0.808, 'Recall@1': 0.808, 'MRR@1': 0.808, 'nDCG@1': 0.808, 'Recall@5': 0.9357, 'MRR@5': 0.862, 'nDCG@5': 0.8686, 'Recall@10': 0.9588, 'MRR@10': 0.8653, 'nDCG@10': 0.8782}\nbase_instr     -> {'N': 995, 'Hit@1': 0.7779, 'Recall@1': 0.7779, 'MRR@1': 0.7779, 'nDCG@1': 0.7779, 'Recall@5': 0.9156, 'MRR@5': 0.8352, 'nDCG@5': 0.8426, 'Recall@10': 0.9357, 'MRR@10': 0.8381, 'nDCG@10': 0.8507}\nft_inst_plain  -> {'N': 995, 'Hit@1': 0.7357, 'Recall@1': 0.7357, 'MRR@1': 0.7357, 'nDCG@1': 0.7357, 'Recall@5': 0.8975, 'MRR@5': 0.803, 'nDCG@5': 0.8124, 'Recall@10': 0.9276, 'MRR@10': 0.8071, 'nDCG@10': 0.8233}\nft_inst_instr  -> {'N': 995, 'Hit@1': 0.7477, 'Recall@1': 0.7477, 'MRR@1': 0.7477, 'nDCG@1': 0.7477, 'Recall@5': 0.8894, 'MRR@5': 0.8084, 'nDCG@5': 0.8135, 'Recall@10': 0.9216, 'MRR@10': 0.8126, 'nDCG@10': 0.8257}\nft_query_plain -> {'N': 995, 'Hit@1': 0.7518, 'Recall@1': 0.7518, 'MRR@1': 0.7518, 'nDCG@1': 0.7518, 'Recall@5': 0.8975, 'MRR@5': 0.8131, 'nDCG@5': 0.8201, 'Recall@10': 0.9286, 'MRR@10': 0.8171, 'nDCG@10': 0.8308}\nft_query_instr -> {'N': 995, 'Hit@1': 0.7427, 'Recall@1': 0.7427, 'MRR@1': 0.7427, 'nDCG@1': 0.7427, 'Recall@5': 0.8915, 'MRR@5': 0.8046, 'nDCG@5': 0.8118, 'Recall@10': 0.9166, 'MRR@10': 0.8081, 'nDCG@10': 0.8217}\n\n=== p-MRR (instr − plain) on queries with instructions only ===\nBaseline: {'pMRR@1': -0.0302, 'pMRR@5': -0.0269, 'pMRR@10': -0.0272}\nFT-Inst: {'pMRR@1': 0.0121, 'pMRR@5': 0.0053, 'pMRR@10': 0.0055}\nFT-Query: {'pMRR@1': -0.009, 'pMRR@5': -0.0085, 'pMRR@10': -0.009}\nBM25:    {'pMRR@1': -0.0382, 'pMRR@5': -0.0447, 'pMRR@10': -0.0449}\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# RuBQ_2.0 \nfrom pathlib import Path\nimport requests, zipfile, io, shutil, os\n\nTARGET = Path(\"/kaggle/working/RuBQ_2.0\")\n\ndef ensure_rubq_2_0(target: Path = TARGET) -> Path:\n    target.mkdir(parents=True, exist_ok=True)\n    has_any = any(target.rglob(\"*.json\")) or any(target.rglob(\"*.jsonl\"))\n    if has_any:\n        print(f\"[ok] RuBQ_2.0 already present at: {target}\")\n        return target\n\n    url = \"https://codeload.github.com/vladislavneon/RuBQ/zip/refs/heads/master\"\n    print(\"[fetch] downloading RuBQ repo zip…\")\n    r = requests.get(url, timeout=90)\n    r.raise_for_status()\n\n    with zipfile.ZipFile(io.BytesIO(r.content)) as zf:\n        top_levels = {name.split(\"/\")[0] for name in zf.namelist() if \"/\" in name}\n        if not top_levels:\n            raise RuntimeError(\"Unexpected zip layout (no top-level folder).\")\n        root_dir = sorted(top_levels)[0]  # e.g., 'RuBQ-master'\n        inner_prefix = f\"{root_dir}/RuBQ_2.0/\"\n        members = [m for m in zf.namelist() if m.startswith(inner_prefix)]\n        if not members:\n            raise RuntimeError(\"Couldn’t find 'RuBQ_2.0/' inside the repo zip.\")\n        extract_base = Path(\"/kaggle/working/_rubq_tmp\")\n        if extract_base.exists():\n            shutil.rmtree(extract_base, ignore_errors=True)\n        extract_base.mkdir(parents=True, exist_ok=True)\n        zf.extractall(extract_base)\n\n    src = extract_base / inner_prefix\n    if target.exists():\n        shutil.rmtree(target, ignore_errors=True)\n    shutil.move(str(src), str(target))\n    shutil.rmtree(extract_base, ignore_errors=True)\n\n    print(f\"[ok] RuBQ_2.0 ready at: {target}\")\n    return target\n\nRUBQ_DIR = str(ensure_rubq_2_0())\n\njson_count  = sum(1 for _ in Path(RUBQ_DIR).rglob(\"*.json\"))\njsonl_count = sum(1 for _ in Path(RUBQ_DIR).rglob(\"*.jsonl\"))\nprint(f\"[scan] Found JSON: {json_count:,} | JSONL: {jsonl_count:,}\")\nprint(\"Set RUBQ_DIR =\", RUBQ_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T01:34:38.145411Z","iopub.execute_input":"2025-09-10T01:34:38.145672Z","iopub.status.idle":"2025-09-10T01:34:42.472518Z","shell.execute_reply.started":"2025-09-10T01:34:38.145655Z","shell.execute_reply":"2025-09-10T01:34:42.471688Z"}},"outputs":[{"name":"stdout","text":"[fetch] downloading RuBQ repo zip…\n[ok] RuBQ_2.0 ready at: /kaggle/working/RuBQ_2.0\n[scan] Found JSON: 3 | JSONL: 0\nSet RUBQ_DIR = /kaggle/working/RuBQ_2.0\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# RuBQ_2.0 -> Minimal IR Eval (BM25 + Base + LoRA)\n\nimport os, json, re, math, random, hashlib\nfrom collections import defaultdict, Counter\nfrom typing import Dict, List, Tuple\n\nos.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n\nimport numpy as np\nfrom tqdm import tqdm\nfrom rank_bm25 import BM25Okapi\n\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModel\nfrom peft import PeftModel\n\nDEVICE         = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE_DOC = 64\nBATCH_SIZE_Q   = 64\nMAX_LEN        = 256\nK_LIST         = [1, 5, 10]\nSEED           = 42\n\nDOC_CAP        = None     \nFAST_Q_CAP     = None      \n\nUSE_PREFIXES   = True\n\nBASE_MODEL          = \"intfloat/multilingual-e5-base\"\nFINETUNED_INST_DIR  = \"/kaggle/working/mE5_lora8bit_infoNCE_v1/best\"      # instruction-aware\nFINETUNED_QUERY_DIR = \"/kaggle/working/mE5_lora8bit_queryonly_v1/best\"    # query-only\n\nRUBQ_DIR = \"/kaggle/working/RuBQ_2.0\"\n\nROOT_OUT  = \"/kaggle/working/rubq_ir_eval\"\nEMB_CACHE = os.path.join(ROOT_OUT, \"emb_cache\")\nos.makedirs(ROOT_OUT, exist_ok=True)\nos.makedirs(EMB_CACHE, exist_ok=True)\n\nrandom.seed(SEED); np.random.seed(SEED)\nprint(f\"Device: {DEVICE} | DOC_CAP={DOC_CAP} | FAST_Q_CAP={FAST_Q_CAP}\")\n\ndef add_prefix(s: str, kind: str) -> str:\n    if not USE_PREFIXES: return s\n    return ((\"query: \" if kind == \"query\" else \"passage: \") + s).strip()\n\ndef fmt(d): return {k:(round(v,4) if isinstance(v,float) else v) for k,v in d.items()}\n\ndef signature(doc_ids: List[str], name: str):\n    head=\"||\".join(doc_ids[:50]); tail=\"||\".join(doc_ids[-50:]) if len(doc_ids)>50 else \"\"\n    return hashlib.md5((head+\"|\"+tail+f\"|{len(doc_ids)}\").encode()).hexdigest()[:8] + f\"__L{MAX_LEN}__{name.replace('/','_')}\"\n\n# Parse RuBQ JSONs\ndef first_nonempty(d: dict, keys: List[str]):\n    for k in keys:\n        v = d.get(k)\n        if isinstance(v, str) and v.strip():\n            return v.strip()\n    return None\n\ndef extract_evidence_texts(item) -> List[str]:\n    texts = []\n    for k in [\"evidence\", \"evidences\"]:\n        if k in item:\n            ev = item[k]\n            if isinstance(ev, str) and ev.strip():\n                texts.append(ev.strip())\n            elif isinstance(ev, list):\n                for e in ev:\n                    if isinstance(e, str) and e.strip():\n                        texts.append(e.strip())\n                    elif isinstance(e, dict):\n                        t = first_nonempty(e, [\"text\", \"snippet\", \"abstract\", \"passage\", \"content\"])\n                        if t: texts.append(t)\n    return texts\n\ndef extract_answer_texts(item) -> List[str]:\n    out = []\n    if \"answers\" in item and isinstance(item[\"answers\"], list):\n        for a in item[\"answers\"]:\n            if isinstance(a, str) and a.strip():\n                out.append(a.strip())\n            elif isinstance(a, dict):\n                t = first_nonempty(a, [\"label\", \"text\", \"name\", \"description\"])\n                if t: out.append(t)\n    if \"answer\" in item and isinstance(item[\"answer\"], dict):\n        t = first_nonempty(item[\"answer\"], [\"label\", \"text\", \"name\", \"description\"])\n        if t: out.append(t)\n    for k in [\"label\", \"description\"]:\n        if isinstance(item.get(k), str) and item[k].strip():\n            out.append(item[k].strip())\n    uniq = []\n    seen = set()\n    for s in out:\n        s2 = s.strip()\n        if s2 and s2 not in seen:\n            seen.add(s2); uniq.append(s2)\n    return uniq\n\ndef scan_jsons(root: str) -> List[dict]:\n    all_items = []\n    if not os.path.isdir(root):\n        return []\n    for dirpath, _, files in os.walk(root):\n        for fn in files:\n            if not fn.lower().endswith(\".json\"): continue\n            fp = os.path.join(dirpath, fn)\n            try:\n                data = json.load(open(fp, \"r\", encoding=\"utf-8\"))\n                if isinstance(data, list):\n                    all_items.extend(data)\n                elif isinstance(data, dict):\n                    for v in data.values():\n                        if isinstance(v, list):\n                            all_items.extend(v)\n            except Exception as e:\n                print(f\"[warn] bad JSON: {fp} ({type(e).__name__})\")\n    return all_items\n\nprint(\"\\n[prep] Scanning RuBQ JSON…\")\nraw_items = scan_jsons(RUBQ_DIR)\nprint(f\"[prep] Loaded raw entries: {len(raw_items)}\")\n\ncorpus: Dict[str, str] = {}\nqueries: Dict[str, str] = {}\nqrels: Dict[str, Dict[str, int]] = defaultdict(dict)\n\ndef add_doc_get_id(text: str, text2id: Dict[str, str], prefix=\"D\") -> str:\n    h = hashlib.md5(text.encode(\"utf-8\")).hexdigest()[:16]\n    did = f\"{prefix}_{h}\"\n    text2id[did] = text\n    return did\n\nq_missing = 0\nhas_evidence = 0\nused_fallback = 0\n\nfor i, item in enumerate(tqdm(raw_items, desc=\"Parsing rows\")):\n    q = first_nonempty(item, [\"question\", \"question_text\", \"question_ru\", \"text\"])\n    if not q:\n        q_missing += 1\n        continue\n    qid = f\"Q_{i:08d}\"\n    queries[qid] = q\n\n    ev_txts = extract_evidence_texts(item)\n    if ev_txts:\n        has_evidence += 1\n        for t in ev_txts:\n            did = add_doc_get_id(t, corpus, prefix=\"E\")\n            qrels[qid][did] = 1\n        continue\n\n    ans_txts = extract_answer_texts(item)\n    if ans_txts:\n        used_fallback += 1\n        for t in ans_txts:\n            did = add_doc_get_id(t, corpus, prefix=\"A\")\n            qrels[qid][did] = 1\n\nprint(f\"[prep] Done. Docs={len(corpus)} | Queries={len(queries)} | QrelsQ={len(qrels)}\")\nprint(f\"[prep] Items without question: {q_missing}\")\nprint(f\"[prep] Items with 'evidence*': {has_evidence} | using fallback(Label/Desc): {used_fallback}\")\n\nif len(corpus) == 0 or len(queries) == 0 or len(qrels) == 0:\n    print(\"\\n[STOP] Parsed dataset is too small/empty for IR.\")\n    print(\" - Ensure your RuBQ JSONs include 'evidence/evidences' or answer labels/descriptions.\")\n    raise SystemExit(0)\n\ndoc_ids = list(corpus.keys())\nif DOC_CAP and DOC_CAP < len(doc_ids):\n    rng = random.Random(SEED); rng.shuffle(doc_ids); doc_ids = doc_ids[:DOC_CAP]\ncorpus = {d: corpus[d] for d in doc_ids}\n\nqids_all = [qid for qid, rels in qrels.items() if any(d in corpus for d in rels)]\nif FAST_Q_CAP and FAST_Q_CAP < len(qids_all):\n    rng = random.Random(SEED); rng.shuffle(qids_all); qids_all = qids_all[:FAST_Q_CAP]\n\nprint(f\"\\n[eval] Final set -> docs={len(doc_ids)} | queries={len(qids_all)}\")\n\n# Metrics\ndef eval_multi_k(run, qrels_map, ks):\n    ks = sorted(set(ks))\n    stats = {\"N\": 0, \"Hit@1\": 0.0}\n    for k in ks:\n        stats.update({\n            f\"Recall@{k}\": 0.0,\n            f\"MRR@{k}\":    0.0,\n            f\"nDCG@{k}\":   0.0,\n            f\"MAP@{k}\":    0.0,   \n        })\n\n    n = 0\n    for qid, ranking in run.items():\n        if qid not in qrels_map:\n            continue\n        n += 1\n        rels = qrels_map[qid]                 \n        pos_ranks = [r for r, (did, _) in enumerate(ranking, start=1) if did in rels]\n        pos_ranks.sort()\n\n        # Hit@1\n        if pos_ranks and pos_ranks[0] == 1:\n            stats[\"Hit@1\"] += 1.0\n\n        m = len(rels)                         \n\n        for k in ks:\n            stats[f\"Recall@{k}\"] += float(any(r <= k for r in pos_ranks))\n\n            stats[f\"MRR@{k}\"] += (1.0 / pos_ranks[0]) if (pos_ranks and pos_ranks[0] <= k) else 0.0\n\n            dcg  = sum(1.0 / math.log2(r + 1) for r in pos_ranks if r <= k)\n            idcg = sum(1.0 / math.log2(r + 1) for r in range(1, min(m, k) + 1))\n            stats[f\"nDCG@{k}\"] += (dcg / idcg) if idcg > 0 else 0.0\n\n            denom = min(k, m)\n            if denom > 0:\n                ap_k = 0.0\n                rel_found_so_far = 0\n                j = 0\n                for r in pos_ranks:\n                    if r > k:\n                        break\n                    j += 1                    \n                    rel_found_so_far = j\n                    prec_at_r = rel_found_so_far / r\n                    ap_k += prec_at_r\n                stats[f\"MAP@{k}\"] += ap_k / denom\n    if n == 0:\n        return stats\n\n    stats[\"N\"] = n\n    for key in [\"Hit@1\"] + \\\n               [f\"Recall@{x}\" for x in ks] + \\\n               [f\"MRR@{x}\" for x in ks] + \\\n               [f\"nDCG@{x}\" for x in ks] + \\\n               [f\"MAP@{x}\" for x in ks]:\n        stats[key] /= n\n    return stats\n\n# BM25 \ntoken_re = re.compile(r\"[a-zа-яё0-9]+\", re.IGNORECASE)\ndef ru_tok(s: str): return token_re.findall(s.lower())\n\ndef bm25_run(doc_texts: List[str], doc_ids: List[str], qids: List[str], qmap: Dict[str,str], topk: int):\n    print(\"\\n[BM25] Building index over docs…\")\n    bm = BM25Okapi([ru_tok(t) for t in tqdm(doc_texts, desc=\"BM25 build\")])\n    run={}\n    for qid in tqdm(qids, desc=\"BM25 search\"):\n        toks = ru_tok(qmap[qid])\n        scores = bm.get_scores(toks)\n        if topk >= len(doc_ids): idx = np.argsort(scores)[::-1]\n        else:\n            part = np.argpartition(scores, -topk)[-topk:]\n            idx = part[np.argsort(scores[part])[::-1]]\n        run[qid]=[(doc_ids[j], float(scores[j])) for j in idx]\n    return run\n\ndef mean_pool(last_hidden, attention_mask):\n    m = attention_mask.unsqueeze(-1).expand_as(last_hidden).float()\n    return (last_hidden * m).sum(1) / m.sum(1).clamp(min=1e-9)\n\ndef load_tok_model_any(model_or_adapter: str, base_model: str):\n    is_adapter = os.path.isdir(model_or_adapter) and os.path.exists(os.path.join(model_or_adapter, \"adapter_config.json\"))\n    tok_src = model_or_adapter if (os.path.isdir(model_or_adapter) and os.path.exists(os.path.join(model_or_adapter, \"tokenizer.json\"))) else base_model\n    tok = AutoTokenizer.from_pretrained(tok_src, use_fast=True)\n    if DEVICE==\"cuda\":\n        base = AutoModel.from_pretrained(base_model, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(\"cuda\").eval()\n    else:\n        base = AutoModel.from_pretrained(base_model, low_cpu_mem_usage=True).to(\"cpu\").eval()\n    mdl = PeftModel.from_pretrained(base, model_or_adapter).eval() if is_adapter else base\n    return tok, mdl\n\n@torch.no_grad()\ndef encode_texts(mdl, tok, texts: List[str], bs: int, max_len: int, device: str):\n    outs=[]\n    for i in tqdm(range(0,len(texts),bs), desc=f\"Encode@{device}\"):\n        batch=texts[i:i+bs]\n        enc=tok(batch,padding=True,truncation=True,max_length=max_len,return_tensors=\"pt\")\n        for k in enc: enc[k]=enc[k].to(device)\n        last=mdl(**enc).last_hidden_state\n        pooled=mean_pool(last, enc[\"attention_mask\"])\n        outs.append(F.normalize(pooled,p=2,dim=1).cpu())\n    return torch.cat(outs,0) if outs else torch.empty(0, mdl.config.hidden_size)\n\ndef build_index(doc_embs_cpu: torch.Tensor):\n    try:\n        import faiss\n        index = faiss.IndexFlatIP(doc_embs_cpu.shape[1])\n        index.add(doc_embs_cpu.numpy().astype(\"float32\"))\n        return index, True\n    except Exception:\n        return doc_embs_cpu, False\n\n@torch.no_grad()\ndef search_index(index, is_faiss: bool, q_embs: torch.Tensor, topk: int):\n    if is_faiss:\n        import faiss\n        D, I = index.search(q_embs.numpy().astype(\"float32\"), topk)\n        return I, D\n    sims = torch.mm(q_embs, index.t())\n    D, I = torch.topk(sims, k=min(topk, sims.size(1)), dim=1)\n    return I.numpy(), D.numpy()\n\ndef eval_model(model_or_adapter: str, label: str, doc_texts_pref: List[str], doc_ids: List[str],\n               qids: List[str], qmap_pref: Dict[str,str]):\n    print(f\"\\n=== {label} ===\")\n    tok, mdl = load_tok_model_any(model_or_adapter, BASE_MODEL)\n    dev = str(next(mdl.parameters()).device)\n    sig = signature(doc_ids, os.path.basename(model_or_adapter) if os.path.isdir(model_or_adapter) else model_or_adapter)\n    cache_p = os.path.join(EMB_CACHE, f\"docs__{sig}.pt\")\n    if os.path.exists(cache_p):\n        doc_embs = torch.load(cache_p, map_location=\"cpu\")\n        print(f\"[cache] doc embs: {doc_embs.shape}\")\n    else:\n        doc_embs = encode_texts(mdl, tok, doc_texts_pref, BATCH_SIZE_DOC, MAX_LEN, dev)\n        torch.save(doc_embs, cache_p)\n        print(f\"[cache] saved doc embs: {doc_embs.shape}\")\n    index, is_faiss = build_index(doc_embs)\n    q_texts = [qmap_pref[qid] for qid in qids]\n    q_embs  = encode_texts(mdl, tok, q_texts, BATCH_SIZE_Q, MAX_LEN, dev)\n    I, D = search_index(index, is_faiss, q_embs, topk=max(K_LIST))\n    run={}\n    for i,qid in enumerate(qids):\n        ids = I[i]; scores = D[i]\n        run[qid]=[(doc_ids[j], float(scores[k])) for k,j in enumerate(ids)]\n    m = eval_multi_k(run, qrels, K_LIST)\n    print(fmt(m))\n    return m\n\ndoc_ids = list(corpus.keys())\ndoc_texts_raw  = [corpus[d] for d in doc_ids]\ndoc_texts_pref = [add_prefix(t, \"passage\") for t in doc_texts_raw]\n\nqids = list(qids_all)\nquery_map_pref = {qid: add_prefix(queries[qid], \"query\") for qid in qids}\n\n# 1) BM25 ----------------\nraw_query_map = {qid: queries[qid] for qid in qids}\nbm_run = bm25_run(doc_texts_raw, doc_ids, qids, raw_query_map, topk=max(K_LIST))\nbm25_metrics = eval_multi_k(bm_run, qrels, K_LIST)\nprint(\"\\nBM25 ->\", fmt(bm25_metrics))\n\n# 2) Baseline encoder ----------------\nbase_metrics = eval_model(BASE_MODEL, \"Baseline (mE5-base)\", doc_texts_pref, doc_ids, qids, query_map_pref)\n\n# 3) Adapters\nif os.path.isdir(FINETUNED_INST_DIR):\n    inst_metrics = eval_model(FINETUNED_INST_DIR, \"Finetuned (instruction-aware, LoRA)\", doc_texts_pref, doc_ids, qids, query_map_pref)\nelse:\n    inst_metrics = None\n\nif os.path.isdir(FINETUNED_QUERY_DIR):\n    qo_metrics = eval_model(FINETUNED_QUERY_DIR, \"Finetuned (query-only, LoRA)\", doc_texts_pref, doc_ids, qids, query_map_pref)\nelse:\n    qo_metrics = None\n\nprint(\"\\n=== SUMMARY (RuBQ-derived IR) ===\")\nprint(\"BM25   ->\", fmt(bm25_metrics))\nprint(\"BASE   ->\", fmt(base_metrics))\nif inst_metrics is not None: print(\"FT-inst->\", fmt(inst_metrics))\nif qo_metrics   is not None: print(\"FT-qonly->\", fmt(qo_metrics))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T01:39:26.582624Z","iopub.execute_input":"2025-09-10T01:39:26.582923Z","iopub.status.idle":"2025-09-10T01:39:47.991332Z","shell.execute_reply.started":"2025-09-10T01:39:26.582901Z","shell.execute_reply":"2025-09-10T01:39:47.990505Z"}},"outputs":[{"name":"stdout","text":"Device: cuda | DOC_CAP=None | FAST_Q_CAP=None\n\n[prep] Scanning RuBQ JSON…\n[prep] Loaded raw entries: 59862\n","output_type":"stream"},{"name":"stderr","text":"Parsing rows: 100%|██████████| 59862/59862 [00:00<00:00, 388857.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"[prep] Done. Docs=3139 | Queries=59862 | QrelsQ=2400\n[prep] Items without question: 0\n[prep] Items with 'evidence*': 0 | using fallback(Label/Desc): 2400\n\n[eval] Final set -> docs=3139 | queries=2400\n\n[BM25] Building index over docs…\n","output_type":"stream"},{"name":"stderr","text":"BM25 build: 100%|██████████| 3139/3139 [00:00<00:00, 512791.44it/s]\nBM25 search: 100%|██████████| 2400/2400 [00:07<00:00, 328.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nBM25 -> {'N': 2400, 'Hit@1': 0.0229, 'Recall@1': 0.0229, 'MRR@1': 0.0229, 'nDCG@1': 0.0229, 'MAP@1': 0.0229, 'Recall@5': 0.0396, 'MRR@5': 0.029, 'nDCG@5': 0.0246, 'MAP@5': 0.0207, 'Recall@10': 0.0425, 'MRR@10': 0.0294, 'nDCG@10': 0.025, 'MAP@10': 0.0206}\n\n=== Baseline (mE5-base) ===\n[cache] doc embs: torch.Size([3139, 768])\n","output_type":"stream"},{"name":"stderr","text":"Encode@cuda:0: 100%|██████████| 38/38 [00:01<00:00, 36.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"{'N': 2400, 'Hit@1': 0.0758, 'Recall@1': 0.0758, 'MRR@1': 0.0758, 'nDCG@1': 0.0758, 'MAP@1': 0.0758, 'Recall@5': 0.1787, 'MRR@5': 0.1135, 'nDCG@5': 0.1117, 'MAP@5': 0.0925, 'Recall@10': 0.2308, 'MRR@10': 0.1203, 'nDCG@10': 0.128, 'MAP@10': 0.0987}\n\n=== Finetuned (instruction-aware, LoRA) ===\n[cache] doc embs: torch.Size([3139, 768])\n","output_type":"stream"},{"name":"stderr","text":"Encode@cuda:0: 100%|██████████| 38/38 [00:02<00:00, 16.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"{'N': 2400, 'Hit@1': 0.0692, 'Recall@1': 0.0692, 'MRR@1': 0.0692, 'nDCG@1': 0.0692, 'MAP@1': 0.0692, 'Recall@5': 0.1671, 'MRR@5': 0.1045, 'nDCG@5': 0.1023, 'MAP@5': 0.0841, 'Recall@10': 0.2175, 'MRR@10': 0.1113, 'nDCG@10': 0.118, 'MAP@10': 0.0902}\n\n=== Finetuned (query-only, LoRA) ===\n[cache] doc embs: torch.Size([3139, 768])\n","output_type":"stream"},{"name":"stderr","text":"Encode@cuda:0: 100%|██████████| 38/38 [00:02<00:00, 16.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"{'N': 2400, 'Hit@1': 0.0779, 'Recall@1': 0.0779, 'MRR@1': 0.0779, 'nDCG@1': 0.0779, 'MAP@1': 0.0779, 'Recall@5': 0.1825, 'MRR@5': 0.1164, 'nDCG@5': 0.1139, 'MAP@5': 0.0944, 'Recall@10': 0.2267, 'MRR@10': 0.1221, 'nDCG@10': 0.1279, 'MAP@10': 0.0998}\n\n=== SUMMARY (RuBQ-derived IR) ===\nBM25   -> {'N': 2400, 'Hit@1': 0.0229, 'Recall@1': 0.0229, 'MRR@1': 0.0229, 'nDCG@1': 0.0229, 'MAP@1': 0.0229, 'Recall@5': 0.0396, 'MRR@5': 0.029, 'nDCG@5': 0.0246, 'MAP@5': 0.0207, 'Recall@10': 0.0425, 'MRR@10': 0.0294, 'nDCG@10': 0.025, 'MAP@10': 0.0206}\nBASE   -> {'N': 2400, 'Hit@1': 0.0758, 'Recall@1': 0.0758, 'MRR@1': 0.0758, 'nDCG@1': 0.0758, 'MAP@1': 0.0758, 'Recall@5': 0.1787, 'MRR@5': 0.1135, 'nDCG@5': 0.1117, 'MAP@5': 0.0925, 'Recall@10': 0.2308, 'MRR@10': 0.1203, 'nDCG@10': 0.128, 'MAP@10': 0.0987}\nFT-inst-> {'N': 2400, 'Hit@1': 0.0692, 'Recall@1': 0.0692, 'MRR@1': 0.0692, 'nDCG@1': 0.0692, 'MAP@1': 0.0692, 'Recall@5': 0.1671, 'MRR@5': 0.1045, 'nDCG@5': 0.1023, 'MAP@5': 0.0841, 'Recall@10': 0.2175, 'MRR@10': 0.1113, 'nDCG@10': 0.118, 'MAP@10': 0.0902}\nFT-qonly-> {'N': 2400, 'Hit@1': 0.0779, 'Recall@1': 0.0779, 'MRR@1': 0.0779, 'nDCG@1': 0.0779, 'MAP@1': 0.0779, 'Recall@5': 0.1825, 'MRR@5': 0.1164, 'nDCG@5': 0.1139, 'MAP@5': 0.0944, 'Recall@10': 0.2267, 'MRR@10': 0.1221, 'nDCG@10': 0.1279, 'MAP@10': 0.0998}\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import os, requests\n\nos.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")  \n\ndef ensure_lang_file(lang: str, out_dir: str) -> str:\n    os.makedirs(out_dir, exist_ok=True)\n    fp = os.path.join(out_dir, f\"{lang}.json\")\n    if os.path.exists(fp):\n        return fp\n\n    url = f\"https://raw.githubusercontent.com/google-research-datasets/lareqa/master/xquad-r/{lang}.json\"\n    print(f\"[fetch] downloading {lang}.json …\")\n    r = requests.get(url, timeout=30)\n    r.raise_for_status()\n    with open(fp, \"wb\") as f:\n        f.write(r.content)\n    print(f\"[fetch] saved -> {fp}\")\n    return fp\n\nLAREQA_DIR = \"/kaggle/working/lareqa_xquad_r\"  \nru_path = ensure_lang_file(\"ru\", LAREQA_DIR)\nen_path = ensure_lang_file(\"en\", LAREQA_DIR)\nprint(\"Files ready:\", ru_path, en_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T01:40:32.816219Z","iopub.execute_input":"2025-09-10T01:40:32.816772Z","iopub.status.idle":"2025-09-10T01:40:33.495520Z","shell.execute_reply.started":"2025-09-10T01:40:32.816737Z","shell.execute_reply":"2025-09-10T01:40:33.494957Z"}},"outputs":[{"name":"stdout","text":"[fetch] downloading ru.json …\n[fetch] saved -> /kaggle/working/lareqa_xquad_r/ru.json\n[fetch] downloading en.json …\n[fetch] saved -> /kaggle/working/lareqa_xquad_r/en.json\nFiles ready: /kaggle/working/lareqa_xquad_r/ru.json /kaggle/working/lareqa_xquad_r/en.json\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# LAReQA (XQuAD-R) RU↔EN — BM25 + Base + LoRA adapters\n\nimport os, io, re, json, math, time, random, hashlib, requests\nfrom collections import defaultdict\nfrom typing import Dict, List, Tuple\n\nos.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n\nimport numpy as np\nfrom tqdm import tqdm\nimport torch\nimport torch.nn.functional as F\n\nfrom rank_bm25 import BM25Okapi\nfrom transformers import AutoTokenizer, AutoModel\nfrom peft import PeftModel\n\nDEVICE           = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE_DOC   = 64\nBATCH_SIZE_Q     = 64\nMAX_LEN          = 256\nK_LIST           = [1, 5, 10]\nSEED             = 42\n\nDOC_CAP_TARGET   = None   \nFAST_Q_CAP       = None  \nUSE_PREFIXES     = True   \n\nFINETUNED_INST_DIR  = \"/kaggle/working/mE5_lora8bit_infoNCE_v1/best\"\nFINETUNED_QUERY_DIR = \"/kaggle/working/mE5_lora8bit_queryonly_v1/best\"\nBASE_MODEL          = \"intfloat/multilingual-e5-base\"\n\nLAREQA_DIR = \"/kaggle/working/lareqa_xquad_r\"   \nINSTR_JSONL = os.path.join(LAREQA_DIR, \"instructions.jsonl\")  \nos.makedirs(LAREQA_DIR, exist_ok=True)\n\ndef tic(msg): print(f\"\\n{msg}\"); return time.time()\ndef toc(t0, prefix=\"Elapsed\"): print(f\"{prefix}: {time.time()-t0:.1f}s\")\n\nrandom.seed(SEED); np.random.seed(SEED)\nprint(f\"Device: {DEVICE} | DOC_CAP_TARGET={DOC_CAP_TARGET} | FAST_Q_CAP={FAST_Q_CAP}\")\n\nRAW_URL = \"https://raw.githubusercontent.com/google-research-datasets/lareqa/master/dataset/xquad-r/{lang}.json\"\n\ndef ensure_lang_file(lang: str, out_dir: str) -> str:\n    fp = os.path.join(out_dir, f\"{lang}.json\")\n    if os.path.exists(fp):\n        return fp\n    try:\n        r = requests.get(RAW_URL.format(lang=lang), timeout=30)\n        r.raise_for_status()\n        with open(fp, \"w\", encoding=\"utf-8\") as f:\n            f.write(r.text)\n        return fp\n    except Exception as e:\n        print(f\"[warn] Could not download {lang}.json automatically: {e}\")\n        print(\"       Please upload en.json and ru.json into\", out_dir)\n        return fp  \n\nru_path = ensure_lang_file(\"ru\", LAREQA_DIR)\nen_path = ensure_lang_file(\"en\", LAREQA_DIR)\nif not (os.path.exists(ru_path) and os.path.exists(en_path)):\n    raise FileNotFoundError(\"Missing ru.json or en.json in LAREQA_DIR. Upload them and re-run.\")\n\ndef split_sentences(context: str, sent_field) -> List[str]:\n    if isinstance(sent_field, list) and len(sent_field) > 0 and isinstance(sent_field[0], list):\n        sents = []\n        for start, end in sent_field:\n            try:\n                s = context[start:end].strip()\n                if s: sents.append(s)\n            except Exception:\n                pass\n        return sents\n    if isinstance(sent_field, list) and sent_field and isinstance(sent_field[0], str):\n        return [s.strip() for s in sent_field if str(s).strip()]\n    return re.split(r'\\s*(?<=\\.|\\?|!)\\s+', context.strip())\n\ndef load_xquadr(path: str):\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n    qa_map = {}     \n    para_sents = []\n    for art in tqdm(data.get(\"data\", []), desc=f\"Parse {os.path.basename(path)}\"):\n        for par in art.get(\"paragraphs\", []):\n            context = par.get(\"context\", \"\") or \"\"\n            sents   = split_sentences(context, par.get(\"sentences\"))\n            base = len(para_sents)\n            para_sents.extend(sents)\n            for qa in par.get(\"qas\", []):\n                qid   = qa.get(\"id\") or qa.get(\"qid\") or \"\"\n                qtext = qa.get(\"question\", \"\").strip()\n                answers = [a.get(\"text\", \"\").strip() for a in qa.get(\"answers\", []) if str(a.get(\"text\",\"\")).strip()]\n                qa_map[qid] = {'question': qtext, 'answers': answers}\n    return para_sents, qa_map\n\nt0 = tic(\"[load] Reading ru.json & en.json…\")\nru_sents, ru_qas = load_xquadr(ru_path)\nen_sents, en_qas = load_xquadr(en_path)\ntoc(t0)\n\nprint(f\"Sents: RU={len(ru_sents):,} | EN={len(en_sents):,}\")\nprint(f\"QAs:   RU={len(ru_qas):,} | EN={len(en_qas):,}\")\n\ndef normalize(s): return re.sub(r\"\\s+\", \" \", s.strip().lower())\n\ndef build_corpus_and_qrels(q_lang: str, cand_lang: str,\n                           qas_src: Dict[str,dict], answers_tgt: Dict[str,dict],\n                           cand_sents: List[str],\n                           doc_cap_target: int = DOC_CAP_TARGET,\n                           fast_q_cap: int = FAST_Q_CAP):\n    doc_ids = [f\"{cand_lang}-s{ix}\" for ix in range(len(cand_sents))]\n    if doc_cap_target and doc_cap_target < len(doc_ids):\n        rng = random.Random(SEED)\n        order = list(range(len(doc_ids))); rng.shuffle(order)\n        keep = sorted(order[:doc_cap_target])\n        cand_sents = [cand_sents[i] for i in keep]\n        doc_ids    = [doc_ids[i]    for i in keep]\n\n    qids_all = sorted(set(qas_src.keys()) & set(answers_tgt.keys()))\n    if fast_q_cap and fast_q_cap < len(qids_all):\n        rng = random.Random(SEED); rng.shuffle(qids_all)\n        qids_all = sorted(qids_all[:fast_q_cap])\n\n    q_texts = {qid: qas_src[qid]['question'] for qid in qids_all}\n\n    cand_norm = [normalize(x) for x in cand_sents]\n    qrels = defaultdict(dict)\n    for qid in tqdm(qids_all, desc=f\"qrels {q_lang}->{cand_lang}\"):\n        golds = [normalize(a) for a in answers_tgt[qid].get('answers', []) if a.strip()]\n        if not golds: \n            continue\n        for si, s in enumerate(cand_norm):\n            if any(g and g in s for g in golds):\n                qrels[qid][doc_ids[si]] = 1\n    qids = [qid for qid in qids_all if qrels.get(qid)]\n    qrels = {qid: qrels[qid] for qid in qids}\n    return doc_ids, cand_sents, qids, q_texts, qrels\n\n# RU->EN\ndoc_ids_en, docs_en, qids_ru2en, q_ru_map, qrels_ru2en = build_corpus_and_qrels(\n    \"ru\", \"en\", qas_src=ru_qas, answers_tgt=en_qas, cand_sents=en_sents,\n    doc_cap_target=DOC_CAP_TARGET, fast_q_cap=FAST_Q_CAP\n)\nprint(f\"[RU→EN] candidates={len(doc_ids_en):,} | queries={len(qids_ru2en):,} | qrels_q={len(qrels_ru2en):,}\")\n\n# EN->RU\ndoc_ids_ru, docs_ru, qids_en2ru, q_en_map, qrels_en2ru = build_corpus_and_qrels(\n    \"en\", \"ru\", qas_src=en_qas, answers_tgt=ru_qas, cand_sents=ru_sents,\n    doc_cap_target=DOC_CAP_TARGET, fast_q_cap=FAST_Q_CAP\n)\nprint(f\"[EN→RU] candidates={len(doc_ids_ru):,} | queries={len(qids_en2ru):,} | qrels_q={len(qrels_en2ru):,}\")\n\ndef load_instructions(jsonl_path: str) -> Dict[str, str]:\n    \"\"\"\n    Accepts flexible fields per line:\n      {\"id\": \"QID\", \"instruction\": \"...\"}  OR\n      {\"qid\": \"QID\", \"inst\": \"...\"}        OR\n      {\"qid\": \"...\", \"instruction\": \"...\"} etc.\n    \"\"\"\n    m = {}\n    if not os.path.exists(jsonl_path):\n        return m\n    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line=line.strip()\n            if not line: continue\n            try:\n                j = json.loads(line)\n            except:\n                continue\n            qid = str(j.get(\"qid\") or j.get(\"id\") or \"\").strip()\n            inst = str(j.get(\"instruction\") or j.get(\"inst\") or \"\").strip()\n            if qid and inst:\n                m[qid] = inst\n    return m\n\ninstr_map = load_instructions(INSTR_JSONL)\nif instr_map:\n    print(f\"[instr] Loaded instructions for {len(instr_map):,} qids from {INSTR_JSONL}\")\nelse:\n    print(\"[instr] No instructions.jsonl found or empty — p-MRR will be skipped.\")\n\ndef add_prefix(s: str, kind: str) -> str:\n    if not USE_PREFIXES: return s\n    return ((\"query: \" if kind==\"query\" else \"passage: \") + s).strip()\n\ndef fmt(d):\n    return {k:(round(v,4) if isinstance(v,float) else v) for k,v in d.items()}\n\ndef mean_pool(last_hidden_state, attention_mask):\n    m = attention_mask.unsqueeze(-1).expand_as(last_hidden_state).float()\n    return (last_hidden_state * m).sum(dim=1) / m.sum(dim=1).clamp(min=1e-9)\n\ndef eval_multi_k(run, qrels: Dict[str, Dict[str,int]], ks: List[int]):\n    ks = sorted(set(ks))\n    stats = {\"N\": 0, \"Hit@1\": 0.0}\n    for k in ks:\n        stats.update({f\"Recall@{k}\":0.0, f\"MRR@{k}\":0.0, f\"nDCG@{k}\":0.0, f\"MAP@{k}\":0.0})\n    n=0\n    for qid, ranking in run.items():\n        if qid not in qrels: continue\n        n += 1\n        rels = qrels[qid]\n        pos = [r for r,(did,_) in enumerate(ranking, start=1) if did in rels]\n        pos.sort()\n\n        if pos and pos[0]==1:\n            stats[\"Hit@1\"] += 1.0\n\n        m_rel = len(rels)\n        for k in ks:\n            stats[f\"Recall@{k}\"] += float(any(r<=k for r in pos))\n            stats[f\"MRR@{k}\"]    += (1.0/pos[0]) if (pos and pos[0]<=k) else 0.0\n            dcg  = sum(1.0/math.log2(r+1) for r in pos if r<=k)\n            idcg = sum(1.0/math.log2(r+1) for r in range(1, min(m_rel,k)+1))\n            stats[f\"nDCG@{k}\"] += (dcg/idcg) if idcg>0 else 0.0\n\n            hits=0; ap_sum=0.0\n            for r,(did,_) in enumerate(ranking[:k], start=1):\n                if did in rels:\n                    hits += 1\n                    ap_sum += hits / r\n            denom = float(min(m_rel, k)) if m_rel>0 else 1.0\n            stats[f\"MAP@{k}\"] += (ap_sum/denom) if denom>0 else 0.0\n\n    if n==0: return stats\n    stats[\"N\"]=n\n    for k in [\"Hit@1\"]+[f\"Recall@{x}\" for x in ks]+[f\"MRR@{x}\" for x in ks]+[f\"nDCG@{x}\" for x in ks]+[f\"MAP@{x}\" for x in ks]:\n        stats[k]/=n\n    return stats\n\n# BM25\ntoken_re = re.compile(r\"[a-z\\u0400-\\u04FF0-9]+\", re.IGNORECASE)\ndef tok(s): return token_re.findall(s.lower())\n\ndef bm25_eval(doc_texts: List[str], doc_ids: List[str], qids: List[str], qmap: Dict[str,str], qrels: Dict[str,Dict[str,int]], topk: int):\n    bm = BM25Okapi([tok(t) for t in tqdm(doc_texts, desc=\"BM25 build\")])\n    run = {}\n    for qid in tqdm(qids, desc=\"BM25 search\"):\n        scores = bm.get_scores(tok(qmap[qid]))\n        if topk >= len(doc_ids):\n            idx = np.argsort(scores)[::-1]\n        else:\n            part = np.argpartition(scores, -topk)[-topk:]\n            idx  = part[np.argsort(scores[part])[::-1]]\n        run[qid] = [(doc_ids[j], float(scores[j])) for j in idx]\n    return eval_multi_k(run, qrels, K_LIST)\n\ndef load_tok_and_model(model_or_adapter: str, base_model: str):\n    is_adapter_dir = os.path.isdir(model_or_adapter) and os.path.exists(os.path.join(model_or_adapter, \"adapter_config.json\"))\n    tok_src = model_or_adapter if (os.path.isdir(model_or_adapter) and os.path.exists(os.path.join(model_or_adapter, \"tokenizer.json\"))) else base_model\n    tok = AutoTokenizer.from_pretrained(tok_src, use_fast=True)\n    if DEVICE==\"cuda\":\n        base = AutoModel.from_pretrained(base_model, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(\"cuda\").eval()\n    else:\n        base = AutoModel.from_pretrained(base_model, low_cpu_mem_usage=True).to(\"cpu\").eval()\n    mdl = PeftModel.from_pretrained(base, model_or_adapter).eval() if is_adapter_dir else base\n    return tok, mdl\n\n@torch.no_grad()\ndef encode_many(mdl, tok, texts: List[str], bs: int, max_len: int, device: str):\n    out = []\n    for i in tqdm(range(0, len(texts), bs), desc=f\"Encode@{device}\"):\n        batch = texts[i:i+bs]\n        enc = tok(batch, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\")\n        for k in enc: enc[k] = enc[k].to(device)\n        last = mdl(**enc).last_hidden_state\n        pooled = mean_pool(last, enc[\"attention_mask\"])\n        out.append(F.normalize(pooled, p=2, dim=1).cpu())\n    return torch.cat(out, 0) if out else torch.empty(0, mdl.config.hidden_size)\n\ndef build_index(doc_embs_cpu: torch.Tensor):\n    try:\n        import faiss\n        index = faiss.IndexFlatIP(doc_embs_cpu.shape[1])\n        index.add(doc_embs_cpu.numpy().astype(\"float32\"))\n        return index, True\n    except Exception:\n        return doc_embs_cpu, False\n\n@torch.no_grad()\ndef search(index, is_faiss: bool, q_embs: torch.Tensor, topk: int):\n    if is_faiss:\n        import faiss\n        D, I = index.search(q_embs.numpy().astype(\"float32\"), topk)\n        return I, D\n    sims = torch.mm(q_embs, index.t())\n    D, I = torch.topk(sims, k=min(topk, sims.size(1)), dim=1)\n    return I.numpy(), D.numpy()\n\ndef neural_eval(model_or_adapter: str, base_model: str,\n                doc_texts: List[str], doc_ids: List[str],\n                qids: List[str], qmap: Dict[str,str], qrels: Dict[str,Dict[str,int]]):\n    print(f\"\\n[neural] {os.path.basename(model_or_adapter) if os.path.isdir(model_or_adapter) else model_or_adapter}\")\n    tok, mdl = load_tok_and_model(model_or_adapter, base_model)\n    dev = next(mdl.parameters()).device\n\n    doc_texts_pref = [add_prefix(t, \"passage\") for t in doc_texts]\n    q_texts_pref   = [add_prefix(qmap[q], \"query\") for q in qids]\n\n    t0 = tic(\"Encode docs…\")\n    d_embs = encode_many(mdl, tok, doc_texts_pref, BATCH_SIZE_DOC, MAX_LEN, str(dev))\n    toc(t0)\n\n    index, is_faiss = build_index(d_embs)\n\n    t0 = tic(\"Encode queries…\")\n    q_embs = encode_many(mdl, tok, q_texts_pref, BATCH_SIZE_Q, MAX_LEN, str(dev))\n    toc(t0)\n\n    I, D = search(index, is_faiss, q_embs, topk=max(K_LIST))\n    run = {}\n    for i, q in enumerate(qids):\n        ids = I[i]; scores = D[i]\n        run[q] = [(doc_ids[j], float(scores[k])) for k, j in enumerate(ids)]\n    m = eval_multi_k(run, qrels, K_LIST)\n    print(\"Metrics:\", fmt(m))\n    return m\n\ndef p_mrr(stats_plain: dict, stats_instr: dict) -> dict:\n    out = {}\n    for k in list(stats_plain.keys()):\n        if k.startswith(\"MRR@\") and k in stats_instr:\n            out[f\"p{ k }\"] = float(stats_instr[k] - stats_plain[k])\n    return out\n\n# Run: RU→EN & EN→RU ----------------\ndef run_one_direction(tag, doc_ids, docs, qids, qmap, qrels):\n    print(f\"\\n=== Direction: {tag} ===\")\n    print(f\"Docs={len(doc_ids):,} | Queries={len(qids):,} | QrelsQ={len(qrels):,}\")\n\n    # 1) BM25 (plain)\n    bm_plain = bm25_eval(docs, doc_ids, qids, qmap, qrels, topk=max(K_LIST))\n    print(\"BM25 (plain) ->\", fmt(bm_plain))\n\n    # 2) Base / LoRAs (plain)\n    base_plain = neural_eval(BASE_MODEL, BASE_MODEL, docs, doc_ids, qids, qmap, qrels)\n    ft_i_plain = neural_eval(FINETUNED_INST_DIR, BASE_MODEL, docs, doc_ids, qids, qmap, qrels) if os.path.isdir(FINETUNED_INST_DIR) else None\n    ft_q_plain = neural_eval(FINETUNED_QUERY_DIR, BASE_MODEL, docs, doc_ids, qids, qmap, qrels) if os.path.isdir(FINETUNED_QUERY_DIR) else None\n\n    # Optional p-MRR with instructions ---\n    pmrr_block = {}\n    if instr_map:\n        qids_with_inst = [qid for qid in qids if qid in instr_map and instr_map[qid].strip()]\n        if qids_with_inst:\n            qmap_instr = {qid: (qmap[qid] + \" \" + instr_map[qid]).strip() for qid in qids_with_inst}\n            qrels_sub  = {qid: qrels[qid] for qid in qids_with_inst}\n            # BM25/BASE/LoRA with instruction-augmented queries\n            bm_instr   = bm25_eval(docs, doc_ids, qids_with_inst, qmap_instr, qrels_sub, topk=max(K_LIST))\n            base_instr = neural_eval(BASE_MODEL, BASE_MODEL, docs, doc_ids, qids_with_inst, qmap_instr, qrels_sub)\n            ft_i_instr = neural_eval(FINETUNED_INST_DIR, BASE_MODEL, docs, doc_ids, qids_with_inst, qmap_instr, qrels_sub) if os.path.isdir(FINETUNED_INST_DIR) else None\n            ft_q_instr = neural_eval(FINETUNED_QUERY_DIR, BASE_MODEL, docs, doc_ids, qids_with_inst, qmap_instr, qrels_sub) if os.path.isdir(FINETUNED_QUERY_DIR) else None\n\n            pmrr_block[\"BM25\"] = p_mrr({k:v for k,v in bm_plain.items() if k.startswith(\"MRR@\")}, {k:v for k,v in bm_instr.items() if k.startswith(\"MRR@\")})\n            pmrr_block[\"BASE\"] = p_mrr({k:v for k,v in base_plain.items() if k.startswith(\"MRR@\")}, {k:v for k,v in base_instr.items() if k.startswith(\"MRR@\")})\n            if ft_i_plain and ft_i_instr:\n                pmrr_block[\"FT_INST\"] = p_mrr({k:v for k,v in ft_i_plain.items() if k.startswith(\"MRR@\")}, {k:v for k,v in ft_i_instr.items() if k.startswith(\"MRR@\")})\n            if ft_q_plain and ft_q_instr:\n                pmrr_block[\"FT_QUERY\"] = p_mrr({k:v for k,v in ft_q_plain.items() if k.startswith(\"MRR@\")}, {k:v for k,v in ft_q_instr.items() if k.startswith(\"MRR@\")})\n        else:\n            print(\"[p-MRR] Instructions provided, but none overlap with current qids — skipping p-MRR.\")\n    else:\n        print(\"[p-MRR] No instructions available — skipping p-MRR.\")\n\n    # Summary\n    print(\"\\n-- Summary:\", tag)\n    print(\"BM25:\", fmt(bm_plain))\n    print(\"BASE:\", fmt(base_plain))\n    if ft_i_plain: print(\"FT_INST:\", fmt(ft_i_plain))\n    if ft_q_plain: print(\"FT_QUERY:\", fmt(ft_q_plain))\n    if pmrr_block:\n        print(\"\\n=== p-MRR (instr − plain) ===\")\n        for name, d in pmrr_block.items():\n            print(name + \":\", fmt(d))\n\n# RU→EN\nrun_one_direction(\"RU→EN\", doc_ids_en, docs_en, qids_ru2en, q_ru_map, qrels_ru2en)\n\n# EN→RU\nrun_one_direction(\"EN→RU\", doc_ids_ru, docs_ru, qids_en2ru, q_en_map, qrels_en2ru)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T01:48:22.107164Z","iopub.execute_input":"2025-09-10T01:48:22.107696Z","iopub.status.idle":"2025-09-10T01:49:08.902192Z","shell.execute_reply.started":"2025-09-10T01:48:22.107671Z","shell.execute_reply":"2025-09-10T01:49:08.901416Z"}},"outputs":[{"name":"stdout","text":"Device: cuda | DOC_CAP_TARGET=None | FAST_Q_CAP=None\n\n[load] Reading ru.json & en.json…\n","output_type":"stream"},{"name":"stderr","text":"Parse ru.json: 100%|██████████| 48/48 [00:00<00:00, 18651.71it/s]\nParse en.json: 100%|██████████| 48/48 [00:00<00:00, 18991.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Elapsed: 0.0s\nSents: RU=1,219 | EN=1,180\nQAs:   RU=1,190 | EN=1,190\n","output_type":"stream"},{"name":"stderr","text":"qrels ru->en: 100%|██████████| 1190/1190 [00:00<00:00, 1683.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"[RU→EN] candidates=1,180 | queries=1,187 | qrels_q=1,187\n","output_type":"stream"},{"name":"stderr","text":"qrels en->ru: 100%|██████████| 1190/1190 [00:00<00:00, 1706.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"[EN→RU] candidates=1,219 | queries=1,188 | qrels_q=1,188\n[instr] No instructions.jsonl found or empty — p-MRR will be skipped.\n\n=== Direction: RU→EN ===\nDocs=1,180 | Queries=1,187 | QrelsQ=1,187\n","output_type":"stream"},{"name":"stderr","text":"BM25 build: 100%|██████████| 1180/1180 [00:00<00:00, 88013.78it/s]\nBM25 search: 100%|██████████| 1187/1187 [00:02<00:00, 480.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"BM25 (plain) -> {'N': 1187, 'Hit@1': 0.0885, 'Recall@1': 0.0885, 'MRR@1': 0.0885, 'nDCG@1': 0.0885, 'MAP@1': 0.0885, 'Recall@5': 0.1457, 'MRR@5': 0.1083, 'nDCG@5': 0.1005, 'MAP@5': 0.0885, 'Recall@10': 0.1685, 'MRR@10': 0.1115, 'nDCG@10': 0.1036, 'MAP@10': 0.0887}\n\n[neural] intfloat/multilingual-e5-base\n\nEncode docs…\n","output_type":"stream"},{"name":"stderr","text":"Encode@cuda:0: 100%|██████████| 19/19 [00:01<00:00, 12.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Elapsed: 1.5s\n\nEncode queries…\n","output_type":"stream"},{"name":"stderr","text":"Encode@cuda:0: 100%|██████████| 19/19 [00:00<00:00, 34.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Elapsed: 0.6s\nMetrics: {'N': 1187, 'Hit@1': 0.7666, 'Recall@1': 0.7666, 'MRR@1': 0.7666, 'nDCG@1': 0.7666, 'MAP@1': 0.7666, 'Recall@5': 0.9334, 'MRR@5': 0.8353, 'nDCG@5': 0.7423, 'MAP@5': 0.6889, 'Recall@10': 0.9646, 'MRR@10': 0.8396, 'nDCG@10': 0.7488, 'MAP@10': 0.6877}\n\n[neural] best\n\nEncode docs…\n","output_type":"stream"},{"name":"stderr","text":"Encode@cuda:0: 100%|██████████| 19/19 [00:03<00:00,  5.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Elapsed: 3.6s\n\nEncode queries…\n","output_type":"stream"},{"name":"stderr","text":"Encode@cuda:0: 100%|██████████| 19/19 [00:01<00:00, 15.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Elapsed: 1.3s\nMetrics: {'N': 1187, 'Hit@1': 0.7195, 'Recall@1': 0.7195, 'MRR@1': 0.7195, 'nDCG@1': 0.7195, 'MAP@1': 0.7195, 'Recall@5': 0.9208, 'MRR@5': 0.8029, 'nDCG@5': 0.7181, 'MAP@5': 0.6617, 'Recall@10': 0.9596, 'MRR@10': 0.8081, 'nDCG@10': 0.7266, 'MAP@10': 0.6614}\n\n[neural] best\n\nEncode docs…\n","output_type":"stream"},{"name":"stderr","text":"Encode@cuda:0: 100%|██████████| 19/19 [00:03<00:00,  5.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Elapsed: 3.6s\n\nEncode queries…\n","output_type":"stream"},{"name":"stderr","text":"Encode@cuda:0: 100%|██████████| 19/19 [00:01<00:00, 14.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Elapsed: 1.3s\nMetrics: {'N': 1187, 'Hit@1': 0.7439, 'Recall@1': 0.7439, 'MRR@1': 0.7439, 'nDCG@1': 0.7439, 'MAP@1': 0.7439, 'Recall@5': 0.9301, 'MRR@5': 0.8198, 'nDCG@5': 0.736, 'MAP@5': 0.6805, 'Recall@10': 0.9638, 'MRR@10': 0.8242, 'nDCG@10': 0.7417, 'MAP@10': 0.6787}\n[p-MRR] No instructions available — skipping p-MRR.\n\n-- Summary: RU→EN\nBM25: {'N': 1187, 'Hit@1': 0.0885, 'Recall@1': 0.0885, 'MRR@1': 0.0885, 'nDCG@1': 0.0885, 'MAP@1': 0.0885, 'Recall@5': 0.1457, 'MRR@5': 0.1083, 'nDCG@5': 0.1005, 'MAP@5': 0.0885, 'Recall@10': 0.1685, 'MRR@10': 0.1115, 'nDCG@10': 0.1036, 'MAP@10': 0.0887}\nBASE: {'N': 1187, 'Hit@1': 0.7666, 'Recall@1': 0.7666, 'MRR@1': 0.7666, 'nDCG@1': 0.7666, 'MAP@1': 0.7666, 'Recall@5': 0.9334, 'MRR@5': 0.8353, 'nDCG@5': 0.7423, 'MAP@5': 0.6889, 'Recall@10': 0.9646, 'MRR@10': 0.8396, 'nDCG@10': 0.7488, 'MAP@10': 0.6877}\nFT_INST: {'N': 1187, 'Hit@1': 0.7195, 'Recall@1': 0.7195, 'MRR@1': 0.7195, 'nDCG@1': 0.7195, 'MAP@1': 0.7195, 'Recall@5': 0.9208, 'MRR@5': 0.8029, 'nDCG@5': 0.7181, 'MAP@5': 0.6617, 'Recall@10': 0.9596, 'MRR@10': 0.8081, 'nDCG@10': 0.7266, 'MAP@10': 0.6614}\nFT_QUERY: {'N': 1187, 'Hit@1': 0.7439, 'Recall@1': 0.7439, 'MRR@1': 0.7439, 'nDCG@1': 0.7439, 'MAP@1': 0.7439, 'Recall@5': 0.9301, 'MRR@5': 0.8198, 'nDCG@5': 0.736, 'MAP@5': 0.6805, 'Recall@10': 0.9638, 'MRR@10': 0.8242, 'nDCG@10': 0.7417, 'MAP@10': 0.6787}\n\n=== Direction: EN→RU ===\nDocs=1,219 | Queries=1,188 | QrelsQ=1,188\n","output_type":"stream"},{"name":"stderr","text":"BM25 build: 100%|██████████| 1219/1219 [00:00<00:00, 29365.16it/s]\nBM25 search: 100%|██████████| 1188/1188 [00:02<00:00, 398.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"BM25 (plain) -> {'N': 1188, 'Hit@1': 0.0581, 'Recall@1': 0.0581, 'MRR@1': 0.0581, 'nDCG@1': 0.0581, 'MAP@1': 0.0581, 'Recall@5': 0.1347, 'MRR@5': 0.0864, 'nDCG@5': 0.0853, 'MAP@5': 0.0719, 'Recall@10': 0.1709, 'MRR@10': 0.091, 'nDCG@10': 0.0959, 'MAP@10': 0.076}\n\n[neural] intfloat/multilingual-e5-base\n\nEncode docs…\n","output_type":"stream"},{"name":"stderr","text":"Encode@cuda:0: 100%|██████████| 20/20 [00:01<00:00, 10.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Elapsed: 2.0s\n\nEncode queries…\n","output_type":"stream"},{"name":"stderr","text":"Encode@cuda:0: 100%|██████████| 19/19 [00:00<00:00, 36.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Elapsed: 0.5s\nMetrics: {'N': 1188, 'Hit@1': 0.7449, 'Recall@1': 0.7449, 'MRR@1': 0.7449, 'nDCG@1': 0.7449, 'MAP@1': 0.7449, 'Recall@5': 0.9276, 'MRR@5': 0.8207, 'nDCG@5': 0.7613, 'MAP@5': 0.7149, 'Recall@10': 0.9596, 'MRR@10': 0.8251, 'nDCG@10': 0.7705, 'MAP@10': 0.7168}\n\n[neural] best\n\nEncode docs…\n","output_type":"stream"},{"name":"stderr","text":"Encode@cuda:0: 100%|██████████| 20/20 [00:04<00:00,  4.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Elapsed: 4.4s\n\nEncode queries…\n","output_type":"stream"},{"name":"stderr","text":"Encode@cuda:0: 100%|██████████| 19/19 [00:01<00:00, 16.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Elapsed: 1.2s\nMetrics: {'N': 1188, 'Hit@1': 0.6995, 'Recall@1': 0.6995, 'MRR@1': 0.6995, 'nDCG@1': 0.6995, 'MAP@1': 0.6995, 'Recall@5': 0.915, 'MRR@5': 0.786, 'nDCG@5': 0.7346, 'MAP@5': 0.6829, 'Recall@10': 0.9537, 'MRR@10': 0.7913, 'nDCG@10': 0.7469, 'MAP@10': 0.6866}\n\n[neural] best\n\nEncode docs…\n","output_type":"stream"},{"name":"stderr","text":"Encode@cuda:0: 100%|██████████| 20/20 [00:04<00:00,  4.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Elapsed: 4.5s\n\nEncode queries…\n","output_type":"stream"},{"name":"stderr","text":"Encode@cuda:0: 100%|██████████| 19/19 [00:01<00:00, 15.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Elapsed: 1.2s\nMetrics: {'N': 1188, 'Hit@1': 0.7222, 'Recall@1': 0.7222, 'MRR@1': 0.7222, 'nDCG@1': 0.7222, 'MAP@1': 0.7222, 'Recall@5': 0.9285, 'MRR@5': 0.8044, 'nDCG@5': 0.7511, 'MAP@5': 0.7005, 'Recall@10': 0.9588, 'MRR@10': 0.8086, 'nDCG@10': 0.7602, 'MAP@10': 0.7027}\n[p-MRR] No instructions available — skipping p-MRR.\n\n-- Summary: EN→RU\nBM25: {'N': 1188, 'Hit@1': 0.0581, 'Recall@1': 0.0581, 'MRR@1': 0.0581, 'nDCG@1': 0.0581, 'MAP@1': 0.0581, 'Recall@5': 0.1347, 'MRR@5': 0.0864, 'nDCG@5': 0.0853, 'MAP@5': 0.0719, 'Recall@10': 0.1709, 'MRR@10': 0.091, 'nDCG@10': 0.0959, 'MAP@10': 0.076}\nBASE: {'N': 1188, 'Hit@1': 0.7449, 'Recall@1': 0.7449, 'MRR@1': 0.7449, 'nDCG@1': 0.7449, 'MAP@1': 0.7449, 'Recall@5': 0.9276, 'MRR@5': 0.8207, 'nDCG@5': 0.7613, 'MAP@5': 0.7149, 'Recall@10': 0.9596, 'MRR@10': 0.8251, 'nDCG@10': 0.7705, 'MAP@10': 0.7168}\nFT_INST: {'N': 1188, 'Hit@1': 0.6995, 'Recall@1': 0.6995, 'MRR@1': 0.6995, 'nDCG@1': 0.6995, 'MAP@1': 0.6995, 'Recall@5': 0.915, 'MRR@5': 0.786, 'nDCG@5': 0.7346, 'MAP@5': 0.6829, 'Recall@10': 0.9537, 'MRR@10': 0.7913, 'nDCG@10': 0.7469, 'MAP@10': 0.6866}\nFT_QUERY: {'N': 1188, 'Hit@1': 0.7222, 'Recall@1': 0.7222, 'MRR@1': 0.7222, 'nDCG@1': 0.7222, 'MAP@1': 0.7222, 'Recall@5': 0.9285, 'MRR@5': 0.8044, 'nDCG@5': 0.7511, 'MAP@5': 0.7005, 'Recall@10': 0.9588, 'MRR@10': 0.8086, 'nDCG@10': 0.7602, 'MAP@10': 0.7027}\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# eval_mfollowir_full_with_pMRR.py\n# - Evaluates BASE and FINETUNED on mFollowIR (RU)\n# - Tests plain and (q + instruction_og) against qrels_og\n# - OOM-safe loading + caching doc embeddings\n# - Adds p-MRR@k (paired uplift: instr MRR - plain MRR)\n\nimport os, math, random, hashlib\nfrom collections import defaultdict\nfrom typing import Dict, List, Tuple\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModel\nfrom peft import PeftModel\nfrom tqdm import tqdm\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE_DOC = 32\nBATCH_SIZE_Q   = 64\nMAX_LEN        = 256\nK_LIST         = [1, 10, 20]\nSEED           = 42\nFAST_Q_CAP     = None\nDOC_CAP        = None\nFP16_ON_GPU    = True\nCACHE_DIR      = \"/kaggle/working/mfollowir_cache\"\nos.makedirs(CACHE_DIR, exist_ok=True)\n\nBASE_MODEL         = \"intfloat/multilingual-e5-base\"\nFINETUNED_INST_DIR = \"/kaggle/working/mE5_lora8bit_infoNCE_v1/best\"  # LoRA adapters folder or full model dir\n\ndef add_prefix(s, kind): \n    return ((\"query: \" if kind == \"query\" else \"passage: \") + s).strip()\n\n@torch.no_grad()\ndef encode_texts(model, tok, texts, batch_size, max_len, device):\n    outs = []\n    for i in tqdm(range(0, len(texts), batch_size), desc=f\"Encoding@{device}\", leave=False):\n        T = texts[i:i+batch_size]\n        enc = tok(T, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\")\n        for k in enc: enc[k] = enc[k].to(device)\n        out  = model(**enc).last_hidden_state\n        mask = enc[\"attention_mask\"].unsqueeze(-1).expand_as(out).float()\n        pooled = (out * mask).sum(1) / mask.sum(1).clamp(min=1e-9)\n        outs.append(F.normalize(pooled, p=2, dim=1).cpu())\n    return torch.cat(outs, 0) if outs else torch.empty(0)\n\ndef build_index(embs):\n    import faiss\n    idx = faiss.IndexFlatIP(embs.shape[1])\n    idx.add(embs.numpy().astype(\"float32\"))\n    return idx\n\ndef search(idx, q_embs, topk):\n    import faiss\n    D, I = idx.search(q_embs.numpy().astype(\"float32\"), topk)\n    return I, D\n\ndef is_adapter_dir(path: str) -> bool:\n    return (os.path.isdir(path) and os.path.exists(os.path.join(path, \"adapter_config.json\")))\n\ndef safe_load(model_or_adapter_path: str, base_model: str = BASE_MODEL):\n    \n    tok_src = model_or_adapter_path if (os.path.isdir(model_or_adapter_path) and os.path.exists(os.path.join(model_or_adapter_path, \"tokenizer.json\"))) else base_model\n    tok = AutoTokenizer.from_pretrained(tok_src, use_fast=True)\n\n    try:\n        mdl_base = AutoModel.from_pretrained(\n            base_model if is_adapter_dir(model_or_adapter_path) else model_or_adapter_path,\n            torch_dtype=(torch.float16 if (DEVICE == \"cuda\" and FP16_ON_GPU) else None),\n            low_cpu_mem_usage=True\n        ).to(DEVICE if DEVICE == \"cuda\" else \"cpu\").eval()\n    except RuntimeError:\n        mdl_base = AutoModel.from_pretrained(\n            base_model if is_adapter_dir(model_or_adapter_path) else model_or_adapter_path,\n            low_cpu_mem_usage=True\n        ).to(\"cpu\").eval()\n\n    if is_adapter_dir(model_or_adapter_path):\n        mdl = PeftModel.from_pretrained(mdl_base, model_or_adapter_path).eval()\n    else:\n        mdl = mdl_base\n    dev = \"cuda\" if next(mdl.parameters()).is_cuda else \"cpu\"\n    return tok, mdl, dev\n\ndef eval_multi_k_and_perquery(run, qrels, ks) -> Tuple[Dict, Dict[int, Dict[str, float]]]:\n\n    ks = sorted(set(ks))\n    stats = {\"N\": 0, \"Hit@1\": 0.0}\n    for k in ks: stats.update({f\"Recall@{k}\":0.0, f\"MRR@{k}\":0.0, f\"nDCG@{k}\":0.0})\n    per_mrr = {k:{} for k in ks}\n    n = 0\n    for qid, ranking in run.items():\n        if qid not in qrels: continue\n        n += 1\n        rels = qrels[qid]\n        pos = [r for r,(did,_) in enumerate(ranking, start=1) if did in rels and rels[did] > 0]\n        pos.sort()\n        if pos and pos[0] == 1: stats[\"Hit@1\"] += 1.0\n        for k in ks:\n            mrr_q = (1.0 / pos[0]) if (pos and pos[0] <= k) else 0.0\n            per_mrr[k][qid] = mrr_q\n            stats[f\"Recall@{k}\"] += float(any(r <= k for r in pos))\n            stats[f\"MRR@{k}\"]    += mrr_q\n            dcg  = sum(1.0/math.log2(r+1) for r in pos if r <= k)\n            m    = sum(1 for _ in rels if rels[_] > 0)\n            idcg = sum(1.0/math.log2(r+1) for r in range(1, min(m, k)+1))\n            stats[f\"nDCG@{k}\"] += (dcg/idcg) if idcg > 0 else 0.0\n    if n == 0: return stats, per_mrr\n    stats[\"N\"] = n\n    for k in [\"Hit@1\"]+[f\"Recall@{x}\" for x in ks]+[f\"MRR@{x}\" for x in ks]+[f\"nDCG@{x}\" for x in ks]:\n        stats[k] /= n\n    return stats, per_mrr\n\ndef compute_p_mrr(per_mrr_plain: Dict[str,float], per_mrr_instr: Dict[str,float]) -> float:\n\n    qids = sorted(set(per_mrr_plain.keys()) & set(per_mrr_instr.keys()))\n    if not qids: return 0.0\n    diffs = [(per_mrr_instr[q] - per_mrr_plain[q]) for q in qids]\n    return float(np.mean(diffs))\n\nprint(\"Loading mFollowIR (rus)…\")\nds_q = load_dataset(\"jhu-clsp/mFollowIR-parquet\", \"queries-rus\")[\"queries\"]\nds_d = load_dataset(\"jhu-clsp/mFollowIR-parquet\", \"corpus-rus\")[\"corpus\"]\nds_og = load_dataset(\"jhu-clsp/mFollowIR-parquet\", \"qrels_og-rus\")[\"test\"]\n\ndoc_ids = [r[\"_id\"] for r in ds_d]\ndoc_txt = [((r.get(\"title\") or \"\") + \" \" + (r.get(\"text\") or \"\")).strip() for r in ds_d]\nif DOC_CAP:\n    rng = random.Random(SEED)\n    idx = list(range(len(doc_ids))); rng.shuffle(idx); keep = sorted(idx[:DOC_CAP])\n    doc_ids = [doc_ids[i] for i in keep]\n    doc_txt = [doc_txt[i] for i in keep]\ndoc_txt = [add_prefix(t, \"passage\") for t in doc_txt]\ndoc_set = set(doc_ids)\n\ndef to_qrels(rows):\n    qd = defaultdict(dict)\n    for r in rows:\n        if r[\"corpus-id\"] in doc_set:\n            qd[r[\"query-id\"]][r[\"corpus-id\"]] = float(r[\"score\"])\n    return {k:v for k,v in qd.items() if v}\n\nqrels_og = to_qrels(ds_og)\n\nq_plain_map = {r[\"_id\"]: add_prefix(r[\"text\"].strip(), \"query\") for r in ds_q}\nq_og_map    = {r[\"_id\"]: add_prefix((r[\"text\"] + \" \" + r[\"instruction_og\"]).strip(), \"query\") for r in ds_q}\nqids = [qid for qid in q_plain_map if qid in qrels_og]\nif FAST_Q_CAP:\n    rng = random.Random(SEED); rng.shuffle(qids); qids = qids[:FAST_Q_CAP]\nprint(f\"Docs={len(doc_ids)}, Queries={len(qids)}\")\n\ndef evaluate(model_or_adapter_path: str, label: str, qmap: Dict[str,str]):\n    print(f\"\\n=== {label} ===\")\n    tok, mdl, dev = safe_load(model_or_adapter_path, BASE_MODEL)\n\n    sig = hashlib.md5((\"|\".join(doc_ids[:50])+f\"|{len(doc_ids)}\").encode()).hexdigest()[:8]\n    base_name = os.path.basename(model_or_adapter_path) if os.path.isdir(model_or_adapter_path) else model_or_adapter_path.replace(\"/\",\"_\")\n    dcache = os.path.join(CACHE_DIR, f\"docs__{base_name}__{sig}__L{MAX_LEN}.pt\")\n    if os.path.exists(dcache):\n        doc_embs = torch.load(dcache, map_location=\"cpu\")\n    else:\n        doc_embs = encode_texts(mdl, tok, doc_txt, BATCH_SIZE_DOC, MAX_LEN, dev)\n        torch.save(doc_embs, dcache)\n\n    index = build_index(doc_embs)\n\n    q_texts = [qmap[qid] for qid in qids]\n    q_embs  = encode_texts(mdl, tok, q_texts, BATCH_SIZE_Q, MAX_LEN, dev)\n\n    I, D = search(index, q_embs, max(K_LIST))\n    run = { qid: [(doc_ids[j], float(D[i][k])) for k, j in enumerate(I[i])] for i, qid in enumerate(qids) }\n\n    metrics, per_mrr = eval_multi_k_and_perquery(run, qrels_og, K_LIST)\n    print({k:(round(v,4) if isinstance(v,float) else v) for k,v in metrics.items()})\n    return metrics, per_mrr\n\nres = {}\nper = {}\n\n# Baseline (plain vs q+inst)\nres[\"baseline_plain\"], per[\"baseline_plain\"] = evaluate(BASE_MODEL, \"BASELINE :: plain vs qrels_og\", q_plain_map)\nres[\"baseline_instr\"], per[\"baseline_instr\"] = evaluate(BASE_MODEL, \"BASELINE :: q+inst vs qrels_og\", q_og_map)\n\n# Finetuned (instruction-aware LoRA) \nres[\"ft_plain\"],     per[\"ft_plain\"]     = evaluate(FINETUNED_INST_DIR, \"FINETUNED :: plain vs qrels_og\", q_plain_map)\nres[\"ft_instr\"],     per[\"ft_instr\"]     = evaluate(FINETUNED_INST_DIR, \"FINETUNED :: q+inst vs qrels_og\", q_og_map)\n\n# p-MRR@k = mean_q ( MRR_instr@k(q) - MRR_plain@k(q) )\ndef p_mrr_block(tag_plain: str, tag_instr: str) -> Dict[str, float]:\n    out = {}\n    for k in K_LIST:\n        pmrr = compute_p_mrr(per[tag_plain][k], per[tag_instr][k])\n        out[f\"pMRR@{k}\"] = round(pmrr, 6)\n    return out\n\np_baseline = p_mrr_block(\"baseline_plain\", \"baseline_instr\")\np_ft       = p_mrr_block(\"ft_plain\",       \"ft_instr\")\n\nprint(\"\\n=== SUMMARY (mFollowIR-RU, qrels_og) ===\")\nfor k,v in res.items():\n    print(f\"{k:16s} ->\", {kk:(round(vv,4) if isinstance(vv,float) else vv) for kk,vv in v.items()})\n\nprint(\"\\n=== p-MRR (instr − plain) ===\")\nprint(\"Baseline:\", p_baseline)\nprint(\"Finetuned:\", p_ft)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T01:51:40.435634Z","iopub.execute_input":"2025-09-10T01:51:40.435995Z","iopub.status.idle":"2025-09-10T02:03:29.311372Z","shell.execute_reply.started":"2025-09-10T01:51:40.435971Z","shell.execute_reply":"2025-09-10T02:03:29.310745Z"}},"outputs":[{"name":"stdout","text":"Loading mFollowIR (rus)…\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ff46815b7604d84adc27f262ce87c36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"queries-rus/queries.parquet:   0%|          | 0.00/46.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79cec19688e14de391e7e4c03aeaee4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating queries split:   0%|          | 0/40 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"592c60dcc1734f56a36a2cdcd2d7c272"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"corpus-rus/corpus.parquet:   0%|          | 0.00/100M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"070fda7058cb45cd8dc30f3d3a0baba1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating corpus split:   0%|          | 0/39326 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05c9bc0090d54d7a87ccee51a3a97eae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"qrels_og-rus/test.parquet:   0%|          | 0.00/467k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9949adb232504301a1be207607c9ac55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/12067 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"667aa1e686bb43929a107a8569152c98"}},"metadata":{}},{"name":"stdout","text":"Docs=39326, Queries=40\n\n=== BASELINE :: plain vs qrels_og ===\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 40, 'Hit@1': 0.475, 'Recall@1': 0.475, 'MRR@1': 0.475, 'nDCG@1': 0.475, 'Recall@10': 0.825, 'MRR@10': 0.597, 'nDCG@10': 0.3762, 'Recall@20': 0.875, 'MRR@20': 0.6004, 'nDCG@20': 0.3877}\n\n=== BASELINE :: q+inst vs qrels_og ===\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 40, 'Hit@1': 0.575, 'Recall@1': 0.575, 'MRR@1': 0.575, 'nDCG@1': 0.575, 'Recall@10': 0.775, 'MRR@10': 0.6308, 'nDCG@10': 0.389, 'Recall@20': 0.85, 'MRR@20': 0.6369, 'nDCG@20': 0.4076}\n\n=== FINETUNED :: plain vs qrels_og ===\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 40, 'Hit@1': 0.55, 'Recall@1': 0.55, 'MRR@1': 0.55, 'nDCG@1': 0.55, 'Recall@10': 0.8, 'MRR@10': 0.6184, 'nDCG@10': 0.4079, 'Recall@20': 0.875, 'MRR@20': 0.6232, 'nDCG@20': 0.4013}\n\n=== FINETUNED :: q+inst vs qrels_og ===\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 40, 'Hit@1': 0.575, 'Recall@1': 0.575, 'MRR@1': 0.575, 'nDCG@1': 0.575, 'Recall@10': 0.8, 'MRR@10': 0.6276, 'nDCG@10': 0.4282, 'Recall@20': 0.85, 'MRR@20': 0.6312, 'nDCG@20': 0.4364}\n\n=== SUMMARY (mFollowIR-RU, qrels_og) ===\nbaseline_plain   -> {'N': 40, 'Hit@1': 0.475, 'Recall@1': 0.475, 'MRR@1': 0.475, 'nDCG@1': 0.475, 'Recall@10': 0.825, 'MRR@10': 0.597, 'nDCG@10': 0.3762, 'Recall@20': 0.875, 'MRR@20': 0.6004, 'nDCG@20': 0.3877}\nbaseline_instr   -> {'N': 40, 'Hit@1': 0.575, 'Recall@1': 0.575, 'MRR@1': 0.575, 'nDCG@1': 0.575, 'Recall@10': 0.775, 'MRR@10': 0.6308, 'nDCG@10': 0.389, 'Recall@20': 0.85, 'MRR@20': 0.6369, 'nDCG@20': 0.4076}\nft_plain         -> {'N': 40, 'Hit@1': 0.55, 'Recall@1': 0.55, 'MRR@1': 0.55, 'nDCG@1': 0.55, 'Recall@10': 0.8, 'MRR@10': 0.6184, 'nDCG@10': 0.4079, 'Recall@20': 0.875, 'MRR@20': 0.6232, 'nDCG@20': 0.4013}\nft_instr         -> {'N': 40, 'Hit@1': 0.575, 'Recall@1': 0.575, 'MRR@1': 0.575, 'nDCG@1': 0.575, 'Recall@10': 0.8, 'MRR@10': 0.6276, 'nDCG@10': 0.4282, 'Recall@20': 0.85, 'MRR@20': 0.6312, 'nDCG@20': 0.4364}\n\n=== p-MRR (instr − plain) ===\nBaseline: {'pMRR@1': 0.1, 'pMRR@10': 0.033819, 'pMRR@20': 0.036561}\nFinetuned: {'pMRR@1': 0.025, 'pMRR@10': 0.009127, 'pMRR@20': 0.007981}\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# rus-XQuAD / rus-NFCorpus / WikiFacts-Articles — HF qrels evaluation\n\nimport os, io, gzip, glob, json, math, shutil, random, csv\nfrom typing import Dict, List, Tuple\nfrom collections import defaultdict\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModel\nfrom huggingface_hub import snapshot_download\nfrom tqdm import tqdm\n\nDEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE    = 64\nMAX_LEN       = 256\nK_LIST        = [2, 5, 10]\nSEED          = 42\nrandom.seed(SEED); np.random.seed(SEED)\n\nFINETUNED_INST_DIR  = \"/kaggle/working/mE5_lora8bit_infoNCE_v1/best\"  \nFINETUNED_QUERY_DIR = \"/kaggle/working/mE5_lora8bit_queryonly_v1/best\"               \nBASE_MODEL          = \"intfloat/multilingual-e5-base\"\n\nFAST_Q_CAP   = 300     \nDOC_CAP      = None    \nUSE_PREFIXES = False  \n\nCACHE_DIR    = \"/kaggle/working/rusbeir_eval_cache\"\nos.makedirs(CACHE_DIR, exist_ok=True)\n\nOUT_ROOT     = \"/kaggle/working/rusbeir_eval_new\"\nHF_CACHE     = \"/kaggle/working/hf_cache\"\nos.makedirs(OUT_ROOT, exist_ok=True)\nos.makedirs(HF_CACHE,  exist_ok=True)\n\nprint(\"Device:\", DEVICE)\n\nSETS = {\n    \"rus-xquad\": {\n        \"data_repo\":  \"kaengreg/rus-xquad\",\n        \"qrels_repo\": \"kaengreg/rus-xquad-qrels\",\n        \"qrels_file\": \"dev.tsv\",\n    },\n    \"rus-nfcorpus\": {\n        \"data_repo\":  \"kaengreg/rus-nfcorpus\",\n        \"qrels_repo\": \"kaengreg/rus-nfcorpus-qrels\",\n        \"qrels_file\": \"test.tsv\",\n    },\n    \"wikifacts-articles\": {\n        \"data_repo\":  \"kaengreg/wikifacts-articles\",\n        \"qrels_repo\": \"kaengreg/wikifacts-articles-qrels\",\n        \"qrels_file\": \"dev.tsv\",\n    },\n}\n\ndef _copy(src, dst):\n    os.makedirs(os.path.dirname(dst), exist_ok=True)\n    shutil.copy2(src, dst)\n\ndef _maybe_gunzip(src, dst):\n    if src.endswith(\".gz\"):\n        with gzip.open(src, \"rt\", encoding=\"utf-8\") as f_in, open(dst, \"w\", encoding=\"utf-8\") as f_out:\n            for line in f_in: f_out.write(line)\n    else:\n        _copy(src, dst)\n\ndef fetch_raw_dataset(name: str, data_repo: str) -> str:\n    out_dir = os.path.join(OUT_ROOT, name)\n    os.makedirs(out_dir, exist_ok=True)\n    root = snapshot_download(repo_id=data_repo, repo_type=\"dataset\",\n                             allow_patterns=[\"corpus.jsonl\",\"queries.jsonl\",\"*.gz\"],\n                             local_dir=HF_CACHE, local_dir_use_symlinks=False)\n    cand_corpus  = glob.glob(os.path.join(root, \"**\", \"corpus.jsonl*\"),  recursive=True)\n    cand_queries = glob.glob(os.path.join(root, \"**\", \"queries.jsonl*\"), recursive=True)\n    if not cand_corpus or not cand_queries:\n        raise FileNotFoundError(f\"Could not find corpus.jsonl/queries.jsonl in {data_repo}\")\n    _maybe_gunzip(sorted(cand_corpus)[-1],  os.path.join(out_dir, \"corpus.jsonl\"))\n    _maybe_gunzip(sorted(cand_queries)[-1], os.path.join(out_dir, \"queries.jsonl\"))\n    return out_dir\n\ndef fetch_qrels(name: str, qrels_repo: str, qrels_file: str) -> str:\n    out_dir = os.path.join(OUT_ROOT, name)\n    os.makedirs(out_dir, exist_ok=True)\n    root = snapshot_download(repo_id=qrels_repo, repo_type=\"dataset\",\n                             allow_patterns=[qrels_file],\n                             local_dir=HF_CACHE, local_dir_use_symlinks=False)\n    src = glob.glob(os.path.join(root, \"**\", qrels_file), recursive=True)\n    if not src:\n        raise FileNotFoundError(f\"{qrels_file} not found in {qrels_repo}\")\n    dst = os.path.join(out_dir, \"qrels.tsv\")\n    _copy(sorted(src)[-1], dst)\n    return dst\n\ndef smart_open(path):\n    if path.endswith(\".gz\"):\n        return io.TextIOWrapper(gzip.open(path, \"rb\"), encoding=\"utf-8\")\n    return open(path, \"r\", encoding=\"utf-8\")\n\ndef load_corpus(path: str) -> Dict[str, str]:\n    corpus = {}\n    with smart_open(path) as f:\n        for line in f:\n            j = json.loads(line)\n            cid   = str(j.get(\"_id\") or j.get(\"id\"))\n            title = (j.get(\"title\") or \"\").strip()\n            text  = (j.get(\"text\")  or \"\").strip()\n            corpus[cid] = (title + \" \" + text).strip()\n    return corpus\n\ndef load_queries(path: str) -> Dict[str, str]:\n    queries = {}\n    with smart_open(path) as f:\n        for line in f:\n            j = json.loads(line)\n            qid  = str(j.get(\"_id\") or j.get(\"id\"))\n            text = (j.get(\"text\") or \"\").strip()\n            queries[qid] = text\n    return queries\n\ndef load_qrels_tsv(path: str) -> Dict[str, Dict[str, int]]:\n    qrels = defaultdict(dict)\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        reader = csv.reader(f, delimiter=\"\\t\")\n        header = None\n        qid_idx, did_idx, rel_idx = 0, 1, 2\n        for i, row in enumerate(reader):\n            if not row or all((c.strip() == \"\" for c in row)):\n                continue\n            if i == 0:\n                lower = [c.strip().lower() for c in row]\n                if any(k in lower for k in (\"score\", \"relevance\", \"label\", \"rel\")):\n                    header = lower\n                    def _idx(names, default):\n                        for n in names:\n                            if n in header:\n                                return header.index(n)\n                        return default\n                    qid_idx = _idx((\"query-id\", \"qid\", \"query_id\"), 0)\n                    did_idx = _idx((\"corpus-id\", \"docid\", \"doc_id\", \"did\"), 1)\n                    rel_idx = _idx((\"score\", \"relevance\", \"label\", \"rel\"), 2)\n                    continue\n            try:\n                qid = row[qid_idx].strip()\n                did = row[did_idx].strip()\n                rel_str = row[rel_idx].strip()\n            except IndexError:\n                continue\n            try:\n                rel_val = int(float(rel_str))\n            except ValueError:\n                continue\n            rel = 1 if rel_val > 0 else 0\n            if rel > 0:\n                qrels[qid][did] = rel\n    return qrels\n\ndef add_prefix(s: str, kind: str, mode: str = \"plain\") -> str:\n    if not USE_PREFIXES:\n        return s\n    if kind == \"query\":\n        if mode == \"neutral_inst\":\n            return f\"query: {s} Используй точное соответствие смысла запроса и отрывка; не делай выводов.\"\n        else:\n            return f\"query: {s}\"\n    else:\n        return f\"passage: {s}\"\n\n@torch.no_grad()\ndef encode_texts(model, tok, texts: List[str], batch_size=BATCH_SIZE, max_len=MAX_LEN, device=DEVICE):\n    all_embs = []\n    for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding\", leave=False):\n        batch = texts[i:i+batch_size]\n        enc = tok(batch, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\")\n        for k in enc: enc[k] = enc[k].to(device)\n        out = model(**enc).last_hidden_state\n        mask = enc[\"attention_mask\"].unsqueeze(-1).expand_as(out).float()\n        pooled = (out * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1e-9)\n        pooled = F.normalize(pooled, p=2, dim=1)\n        all_embs.append(pooled.detach().cpu())\n    return torch.cat(all_embs, dim=0) if all_embs else torch.empty(0, model.config.hidden_size)\n\ndef build_index(embs_cpu: torch.Tensor):\n    try:\n        import faiss\n        index = faiss.IndexFlatIP(embs_cpu.shape[1])\n        index.add(embs_cpu.numpy().astype(\"float32\"))\n        return index, True\n    except Exception:\n        return embs_cpu, False\n\n@torch.no_grad()\ndef search(index, is_faiss: bool, q_embs: torch.Tensor, topk: int):\n    if is_faiss:\n        import faiss\n        D, I = index.search(q_embs.numpy().astype(\"float32\"), topk)\n        return I, D\n    q = q_embs\n    d = index\n    I_all, D_all = [], []\n    chunk = 256\n    for i in tqdm(range(0, q.shape[0], chunk), desc=\"Searching\", leave=False):\n        sims = torch.mm(q[i:i+chunk], d.t())\n        D, I = torch.topk(sims, k=min(topk, sims.size(1)), dim=1)\n        I_all.append(I.numpy()); D_all.append(D.numpy())\n    return np.vstack(I_all), np.vstack(D_all)\n\ndef eval_multi_k(run, qrels: Dict[str, Dict[str,int]], ks: List[int]):\n    ks = sorted(set(ks))\n    stats = {\"N\": 0, \"Hit@1\": 0.0}\n    for k in ks:\n        stats[f\"Recall@{k}\"] = 0.0\n        stats[f\"MRR@{k}\"]    = 0.0\n        stats[f\"nDCG@{k}\"]   = 0.0\n        stats[f\"MAP@{k}\"]    = 0.0   \n\n    n = 0\n    for qid, ranking in run.items():\n        if qid not in qrels: \n            continue\n        n += 1\n        rels = qrels[qid]  \n        pos_ranks = [r for r,(did,_) in enumerate(ranking, start=1) if did in rels and rels[did] > 0]\n        pos_ranks.sort()\n\n        if pos_ranks and pos_ranks[0] == 1:\n            stats[\"Hit@1\"] += 1.0\n\n        m_total = sum(1 for _ in rels if rels[_] > 0)  \n\n        for k in ks:\n            stats[f\"Recall@{k}\"] += float(any(r <= k for r in pos_ranks))\n            stats[f\"MRR@{k}\"] += (1.0/pos_ranks[0]) if (pos_ranks and pos_ranks[0] <= k) else 0.0\n            dcg = sum(1.0 / math.log2(r+1) for r in pos_ranks if r <= k)\n            idcg = sum(1.0 / math.log2(r+1) for r in range(1, min(m_total, k)+1))\n            stats[f\"nDCG@{k}\"] += (dcg / idcg) if idcg > 0 else 0.0\n\n            # MAP@k\n            m_k = min(m_total, k)\n            if m_k > 0:\n                hits = 0\n                ap_sum = 0.0\n                for r in pos_ranks:\n                    if r > k:\n                        break\n                    hits += 1\n                    ap_sum += hits / r\n                stats[f\"MAP@{k}\"] += ap_sum / m_k\n            else:\n                stats[f\"MAP@{k}\"] += 0.0\n\n    if n == 0:\n        return stats\n    stats[\"N\"] = n\n    stats[\"Hit@1\"] /= n\n    for k in ks:\n        stats[f\"Recall@{k}\"] /= n\n        stats[f\"MRR@{k}\"]    /= n\n        stats[f\"nDCG@{k}\"]   /= n\n        stats[f\"MAP@{k}\"]    /= n\n    return stats\n\n@torch.no_grad()\ndef evaluate(model_dir_or_name: str, tag: str, doc_texts: List[str], q_texts: List[str],\n             qids_eval: List[str], qrels: Dict[str, Dict[str,int]], ks: List[int], doc_ids: List[str],\n             cache_key: str = None):\n    print(f\"\\n=== {tag} ===\")\n    tok = AutoTokenizer.from_pretrained(model_dir_or_name, use_fast=True)\n    mdl = AutoModel.from_pretrained(model_dir_or_name).to(DEVICE).eval()\n\n    cache_path = None\n    if cache_key:\n        cache_path = os.path.join(CACHE_DIR, f\"{cache_key}__{tag.replace(' ','_')}.pt\")\n    if cache_path and os.path.exists(cache_path):\n        doc_embs = torch.load(cache_path, map_location=\"cpu\")\n    else:\n        doc_embs = encode_texts(mdl, tok, doc_texts)\n        if cache_path: torch.save(doc_embs, cache_path)\n\n    index, is_faiss = build_index(doc_embs)\n\n    q_embs = encode_texts(mdl, tok, q_texts)\n    kmax = max(ks)\n    I, D = search(index, is_faiss, q_embs, kmax)\n\n    run = {}\n    for i, qid in enumerate(qids_eval):\n        ids = I[i]; scores = D[i]\n        run[qid] = [(doc_ids[j], float(scores[k])) for k, j in enumerate(ids)]\n    m = eval_multi_k(run, qrels, ks)\n    print({k:(round(v,4) if isinstance(v,float) else v) for k,v in m.items()})\n    return m\n\ndef run_on_dataset(name: str, spec: dict):\n    base_dir = fetch_raw_dataset(name, spec[\"data_repo\"])\n    qrels_path = fetch_qrels(name, spec[\"qrels_repo\"], spec[\"qrels_file\"])\n\n    corpus  = load_corpus(os.path.join(base_dir, \"corpus.jsonl\"))\n    queries = load_queries(os.path.join(base_dir, \"queries.jsonl\"))\n    if DOC_CAP:\n        rng = random.Random(SEED); doc_ids_all = list(corpus.keys()); rng.shuffle(doc_ids_all)\n        keep = set(doc_ids_all[:DOC_CAP])\n        corpus = {k:v for k,v in corpus.items() if k in keep}\n\n    qrels = load_qrels_tsv(qrels_path)\n    qrels = {qid: {did:rel for did,rel in dd.items() if did in corpus} for qid,dd in qrels.items()}\n    qrels = {qid: dd for qid,dd in qrels.items() if dd}\n\n    qids = [qid for qid in qrels.keys() if qid in queries]\n    if FAST_Q_CAP and len(qids) > FAST_Q_CAP:\n        rng = random.Random(SEED); rng.shuffle(qids); qids = qids[:FAST_Q_CAP]\n\n    print(f\"Dataset: {name} | Eval queries: {len(qids)} | Docs: {len(corpus)}\")\n\n    doc_ids   = list(corpus.keys())\n    doc_texts = [add_prefix(corpus[d], \"passage\") for d in doc_ids]\n    q_plain   = [add_prefix(queries[q], \"query\", mode=\"plain\") for q in qids]\n    q_neutral = [add_prefix(queries[q], \"query\", mode=\"neutral_inst\") for q in qids]\n\n    results = {}\n\n    if FINETUNED_INST_DIR and os.path.exists(FINETUNED_INST_DIR):\n        results[\"finetuned_instruction\"] = evaluate(\n            FINETUNED_INST_DIR, \"Finetuned (instruction-aware)\",\n            doc_texts, q_neutral, qids, qrels, K_LIST, doc_ids,\n            cache_key=f\"{name}\"\n        )\n\n    if FINETUNED_QUERY_DIR and os.path.exists(FINETUNED_QUERY_DIR):\n        results[\"finetuned_queryonly\"] = evaluate(\n            FINETUNED_QUERY_DIR, \"Finetuned (query-only)\",\n            doc_texts, q_plain, qids, qrels, K_LIST, doc_ids,\n            cache_key=f\"{name}\"\n        )\n\n    results[\"baseline\"] = evaluate(\n        BASE_MODEL, \"Baseline (zero-shot)\",\n        doc_texts, q_plain, qids, qrels, K_LIST, doc_ids,\n        cache_key=f\"{name}\"\n    )\n\n    print(\"\\n--- Summary:\", name, \"---\")\n    for k, v in results.items():\n        print(k, \"->\", {kk:(round(vv,3) if isinstance(vv,float) else vv) for kk,vv in (v or {}).items()})\n    return results\n\nall_results = {}\nfor name, spec in SETS.items():\n    print(f\"\\n=== Fetch & Evaluate: {name} ===\")\n    all_results[name] = run_on_dataset(name, spec)\n\nprint(\"\\n=== GRAND SUMMARY ===\")\nfor ds, res in all_results.items():\n    print(\"\\n\", ds)\n    for k, v in res.items():\n        print(\" \", k, \"->\", {kk:(round(vv,3) if isinstance(vv,float) else vv) for kk,vv in (v or {}).items()})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T02:10:07.234349Z","iopub.execute_input":"2025-09-10T02:10:07.235117Z","iopub.status.idle":"2025-09-10T02:38:41.433443Z","shell.execute_reply.started":"2025-09-10T02:10:07.235093Z","shell.execute_reply":"2025-09-10T02:38:41.432736Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n\n=== Fetch & Evaluate: rus-xquad ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4adab5f8ab6f4450b5ece48ef2d3ddf9"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:982: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\nFor more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"corpus.jsonl: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b235dc3516542768c1748f4a3a4a9a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"queries.jsonl: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cdcb9a3fbfc4816aacb34b750a0a4b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cc8474cc7fd43819a2511c01f65a644"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dev.tsv: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"936ef6dc46c4483a8d21c352fc197bc4"}},"metadata":{}},{"name":"stdout","text":"Dataset: rus-xquad | Eval queries: 300 | Docs: 240\n\n=== Finetuned (instruction-aware) ===\n","output_type":"stream"},{"name":"stderr","text":"                                                       \r","output_type":"stream"},{"name":"stdout","text":"{'N': 300, 'Hit@1': 0.8767, 'Recall@2': 0.94, 'MRR@2': 0.9083, 'nDCG@2': 0.9166, 'MAP@2': 0.9083, 'Recall@5': 0.99, 'MRR@5': 0.9224, 'nDCG@5': 0.9395, 'MAP@5': 0.9224, 'Recall@10': 0.9933, 'MRR@10': 0.923, 'nDCG@10': 0.9407, 'MAP@10': 0.923}\n\n=== Finetuned (query-only) ===\n","output_type":"stream"},{"name":"stderr","text":"                                                       \r","output_type":"stream"},{"name":"stdout","text":"{'N': 300, 'Hit@1': 0.84, 'Recall@2': 0.93, 'MRR@2': 0.885, 'nDCG@2': 0.8968, 'MAP@2': 0.885, 'Recall@5': 0.9733, 'MRR@5': 0.8978, 'nDCG@5': 0.9171, 'MAP@5': 0.8978, 'Recall@10': 0.9833, 'MRR@10': 0.8991, 'nDCG@10': 0.9203, 'MAP@10': 0.8991}\n\n=== Baseline (zero-shot) ===\n","output_type":"stream"},{"name":"stderr","text":"                                                       \r","output_type":"stream"},{"name":"stdout","text":"{'N': 300, 'Hit@1': 0.8933, 'Recall@2': 0.95, 'MRR@2': 0.9217, 'nDCG@2': 0.9291, 'MAP@2': 0.9217, 'Recall@5': 0.98, 'MRR@5': 0.9302, 'nDCG@5': 0.9429, 'MAP@5': 0.9302, 'Recall@10': 0.9933, 'MRR@10': 0.932, 'nDCG@10': 0.9472, 'MAP@10': 0.932}\n\n--- Summary: rus-xquad ---\nfinetuned_instruction -> {'N': 300, 'Hit@1': 0.877, 'Recall@2': 0.94, 'MRR@2': 0.908, 'nDCG@2': 0.917, 'MAP@2': 0.908, 'Recall@5': 0.99, 'MRR@5': 0.922, 'nDCG@5': 0.939, 'MAP@5': 0.922, 'Recall@10': 0.993, 'MRR@10': 0.923, 'nDCG@10': 0.941, 'MAP@10': 0.923}\nfinetuned_queryonly -> {'N': 300, 'Hit@1': 0.84, 'Recall@2': 0.93, 'MRR@2': 0.885, 'nDCG@2': 0.897, 'MAP@2': 0.885, 'Recall@5': 0.973, 'MRR@5': 0.898, 'nDCG@5': 0.917, 'MAP@5': 0.898, 'Recall@10': 0.983, 'MRR@10': 0.899, 'nDCG@10': 0.92, 'MAP@10': 0.899}\nbaseline -> {'N': 300, 'Hit@1': 0.893, 'Recall@2': 0.95, 'MRR@2': 0.922, 'nDCG@2': 0.929, 'MAP@2': 0.922, 'Recall@5': 0.98, 'MRR@5': 0.93, 'nDCG@5': 0.943, 'MAP@5': 0.93, 'Recall@10': 0.993, 'MRR@10': 0.932, 'nDCG@10': 0.947, 'MAP@10': 0.932}\n\n=== Fetch & Evaluate: rus-nfcorpus ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b3ad720aa374338a382244dbf00a2b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"corpus.jsonl:   0%|          | 0.00/22.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dd2c4486013441c9d207e03dceded04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"queries.jsonl: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0076cec441d24f37aff6cc8ff7f00b3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c84d53f12ba4821ad3f9e7abd06be1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.tsv: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac2c02497b984441a196e30389d2fd59"}},"metadata":{}},{"name":"stdout","text":"Dataset: rus-nfcorpus | Eval queries: 300 | Docs: 3633\n\n=== Finetuned (instruction-aware) ===\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"{'N': 300, 'Hit@1': 0.37, 'Recall@2': 0.46, 'MRR@2': 0.415, 'nDCG@2': 0.3412, 'MAP@2': 0.31, 'Recall@5': 0.5667, 'MRR@5': 0.4433, 'nDCG@5': 0.2999, 'MAP@5': 0.2357, 'Recall@10': 0.63, 'MRR@10': 0.4524, 'nDCG@10': 0.2688, 'MAP@10': 0.1873}\n\n=== Finetuned (query-only) ===\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"{'N': 300, 'Hit@1': 0.3633, 'Recall@2': 0.4433, 'MRR@2': 0.4033, 'nDCG@2': 0.3216, 'MAP@2': 0.2892, 'Recall@5': 0.5467, 'MRR@5': 0.4313, 'nDCG@5': 0.2831, 'MAP@5': 0.2183, 'Recall@10': 0.6233, 'MRR@10': 0.4419, 'nDCG@10': 0.2552, 'MAP@10': 0.1736}\n\n=== Baseline (zero-shot) ===\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"{'N': 300, 'Hit@1': 0.37, 'Recall@2': 0.4833, 'MRR@2': 0.4267, 'nDCG@2': 0.3468, 'MAP@2': 0.3117, 'Recall@5': 0.5767, 'MRR@5': 0.4516, 'nDCG@5': 0.3018, 'MAP@5': 0.2343, 'Recall@10': 0.6433, 'MRR@10': 0.4607, 'nDCG@10': 0.268, 'MAP@10': 0.1841}\n\n--- Summary: rus-nfcorpus ---\nfinetuned_instruction -> {'N': 300, 'Hit@1': 0.37, 'Recall@2': 0.46, 'MRR@2': 0.415, 'nDCG@2': 0.341, 'MAP@2': 0.31, 'Recall@5': 0.567, 'MRR@5': 0.443, 'nDCG@5': 0.3, 'MAP@5': 0.236, 'Recall@10': 0.63, 'MRR@10': 0.452, 'nDCG@10': 0.269, 'MAP@10': 0.187}\nfinetuned_queryonly -> {'N': 300, 'Hit@1': 0.363, 'Recall@2': 0.443, 'MRR@2': 0.403, 'nDCG@2': 0.322, 'MAP@2': 0.289, 'Recall@5': 0.547, 'MRR@5': 0.431, 'nDCG@5': 0.283, 'MAP@5': 0.218, 'Recall@10': 0.623, 'MRR@10': 0.442, 'nDCG@10': 0.255, 'MAP@10': 0.174}\nbaseline -> {'N': 300, 'Hit@1': 0.37, 'Recall@2': 0.483, 'MRR@2': 0.427, 'nDCG@2': 0.347, 'MAP@2': 0.312, 'Recall@5': 0.577, 'MRR@5': 0.452, 'nDCG@5': 0.302, 'MAP@5': 0.234, 'Recall@10': 0.643, 'MRR@10': 0.461, 'nDCG@10': 0.268, 'MAP@10': 0.184}\n\n=== Fetch & Evaluate: wikifacts-articles ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e512e14f88c54d49a6a12b3b0b649be3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"corpus.jsonl:   0%|          | 0.00/639M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"990e03cb91d94dd3b1c936ab31382bd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"queries.jsonl: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38acb1aeacde46b799d2af3d10dfdc93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"047688530dab48ab95149eefdf2f0571"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dev.tsv: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"496c091d0df345688b91db7d85adb95f"}},"metadata":{}},{"name":"stdout","text":"Dataset: wikifacts-articles | Eval queries: 300 | Docs: 12848\n\n=== Finetuned (instruction-aware) ===\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"{'N': 300, 'Hit@1': 0.49, 'Recall@2': 0.6567, 'MRR@2': 0.5733, 'nDCG@2': 0.511, 'MAP@2': 0.4708, 'Recall@5': 0.74, 'MRR@5': 0.5975, 'nDCG@5': 0.5559, 'MAP@5': 0.4987, 'Recall@10': 0.8033, 'MRR@10': 0.6062, 'nDCG@10': 0.585, 'MAP@10': 0.5143}\n\n=== Finetuned (query-only) ===\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"{'N': 300, 'Hit@1': 0.42, 'Recall@2': 0.5633, 'MRR@2': 0.4917, 'nDCG@2': 0.4389, 'MAP@2': 0.4042, 'Recall@5': 0.69, 'MRR@5': 0.5274, 'nDCG@5': 0.4914, 'MAP@5': 0.4322, 'Recall@10': 0.73, 'MRR@10': 0.5327, 'nDCG@10': 0.5146, 'MAP@10': 0.4452}\n\n=== Baseline (zero-shot) ===\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"{'N': 300, 'Hit@1': 0.4467, 'Recall@2': 0.6067, 'MRR@2': 0.5267, 'nDCG@2': 0.4712, 'MAP@2': 0.4342, 'Recall@5': 0.6967, 'MRR@5': 0.5519, 'nDCG@5': 0.5172, 'MAP@5': 0.4624, 'Recall@10': 0.78, 'MRR@10': 0.5629, 'nDCG@10': 0.5501, 'MAP@10': 0.4787}\n\n--- Summary: wikifacts-articles ---\nfinetuned_instruction -> {'N': 300, 'Hit@1': 0.49, 'Recall@2': 0.657, 'MRR@2': 0.573, 'nDCG@2': 0.511, 'MAP@2': 0.471, 'Recall@5': 0.74, 'MRR@5': 0.598, 'nDCG@5': 0.556, 'MAP@5': 0.499, 'Recall@10': 0.803, 'MRR@10': 0.606, 'nDCG@10': 0.585, 'MAP@10': 0.514}\nfinetuned_queryonly -> {'N': 300, 'Hit@1': 0.42, 'Recall@2': 0.563, 'MRR@2': 0.492, 'nDCG@2': 0.439, 'MAP@2': 0.404, 'Recall@5': 0.69, 'MRR@5': 0.527, 'nDCG@5': 0.491, 'MAP@5': 0.432, 'Recall@10': 0.73, 'MRR@10': 0.533, 'nDCG@10': 0.515, 'MAP@10': 0.445}\nbaseline -> {'N': 300, 'Hit@1': 0.447, 'Recall@2': 0.607, 'MRR@2': 0.527, 'nDCG@2': 0.471, 'MAP@2': 0.434, 'Recall@5': 0.697, 'MRR@5': 0.552, 'nDCG@5': 0.517, 'MAP@5': 0.462, 'Recall@10': 0.78, 'MRR@10': 0.563, 'nDCG@10': 0.55, 'MAP@10': 0.479}\n\n=== GRAND SUMMARY ===\n\n rus-xquad\n  finetuned_instruction -> {'N': 300, 'Hit@1': 0.877, 'Recall@2': 0.94, 'MRR@2': 0.908, 'nDCG@2': 0.917, 'MAP@2': 0.908, 'Recall@5': 0.99, 'MRR@5': 0.922, 'nDCG@5': 0.939, 'MAP@5': 0.922, 'Recall@10': 0.993, 'MRR@10': 0.923, 'nDCG@10': 0.941, 'MAP@10': 0.923}\n  finetuned_queryonly -> {'N': 300, 'Hit@1': 0.84, 'Recall@2': 0.93, 'MRR@2': 0.885, 'nDCG@2': 0.897, 'MAP@2': 0.885, 'Recall@5': 0.973, 'MRR@5': 0.898, 'nDCG@5': 0.917, 'MAP@5': 0.898, 'Recall@10': 0.983, 'MRR@10': 0.899, 'nDCG@10': 0.92, 'MAP@10': 0.899}\n  baseline -> {'N': 300, 'Hit@1': 0.893, 'Recall@2': 0.95, 'MRR@2': 0.922, 'nDCG@2': 0.929, 'MAP@2': 0.922, 'Recall@5': 0.98, 'MRR@5': 0.93, 'nDCG@5': 0.943, 'MAP@5': 0.93, 'Recall@10': 0.993, 'MRR@10': 0.932, 'nDCG@10': 0.947, 'MAP@10': 0.932}\n\n rus-nfcorpus\n  finetuned_instruction -> {'N': 300, 'Hit@1': 0.37, 'Recall@2': 0.46, 'MRR@2': 0.415, 'nDCG@2': 0.341, 'MAP@2': 0.31, 'Recall@5': 0.567, 'MRR@5': 0.443, 'nDCG@5': 0.3, 'MAP@5': 0.236, 'Recall@10': 0.63, 'MRR@10': 0.452, 'nDCG@10': 0.269, 'MAP@10': 0.187}\n  finetuned_queryonly -> {'N': 300, 'Hit@1': 0.363, 'Recall@2': 0.443, 'MRR@2': 0.403, 'nDCG@2': 0.322, 'MAP@2': 0.289, 'Recall@5': 0.547, 'MRR@5': 0.431, 'nDCG@5': 0.283, 'MAP@5': 0.218, 'Recall@10': 0.623, 'MRR@10': 0.442, 'nDCG@10': 0.255, 'MAP@10': 0.174}\n  baseline -> {'N': 300, 'Hit@1': 0.37, 'Recall@2': 0.483, 'MRR@2': 0.427, 'nDCG@2': 0.347, 'MAP@2': 0.312, 'Recall@5': 0.577, 'MRR@5': 0.452, 'nDCG@5': 0.302, 'MAP@5': 0.234, 'Recall@10': 0.643, 'MRR@10': 0.461, 'nDCG@10': 0.268, 'MAP@10': 0.184}\n\n wikifacts-articles\n  finetuned_instruction -> {'N': 300, 'Hit@1': 0.49, 'Recall@2': 0.657, 'MRR@2': 0.573, 'nDCG@2': 0.511, 'MAP@2': 0.471, 'Recall@5': 0.74, 'MRR@5': 0.598, 'nDCG@5': 0.556, 'MAP@5': 0.499, 'Recall@10': 0.803, 'MRR@10': 0.606, 'nDCG@10': 0.585, 'MAP@10': 0.514}\n  finetuned_queryonly -> {'N': 300, 'Hit@1': 0.42, 'Recall@2': 0.563, 'MRR@2': 0.492, 'nDCG@2': 0.439, 'MAP@2': 0.404, 'Recall@5': 0.69, 'MRR@5': 0.527, 'nDCG@5': 0.491, 'MAP@5': 0.432, 'Recall@10': 0.73, 'MRR@10': 0.533, 'nDCG@10': 0.515, 'MAP@10': 0.445}\n  baseline -> {'N': 300, 'Hit@1': 0.447, 'Recall@2': 0.607, 'MRR@2': 0.527, 'nDCG@2': 0.471, 'MAP@2': 0.434, 'Recall@5': 0.697, 'MRR@5': 0.552, 'nDCG@5': 0.517, 'MAP@5': 0.462, 'Recall@10': 0.78, 'MRR@10': 0.563, 'nDCG@10': 0.55, 'MAP@10': 0.479}\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# Evaluate: rus-arguana & rus-scifact (RU) with LoRA adapters\n# - Baseline E5-base, LoRA instruction-aware, LoRA query-only\n# - Rebuild qrels from BEIR via ir_datasets, filtered to RU IDs\n# - Caches doc embeddings per (dataset, model)\n\nimport os, io, gzip, glob, json, math, random, sys, hashlib\nfrom collections import defaultdict\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModel\nfrom huggingface_hub import snapshot_download\nfrom tqdm import tqdm\n\nimport ir_datasets as irds\n\ntry:\n    import faiss\n    FAISS_OK = True\nexcept Exception:\n    FAISS_OK = False\n\nos.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n\nDEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE    = 64           \nMAX_LEN       = 256\nK_LIST        = [1, 5, 10]\nSEED          = 42\nFAST_Q_CAP    = 300           \nDOC_CAP       = None         \n\nFINETUNED_INST_DIR  = \"/kaggle/working/mE5_lora8bit_infoNCE_v1/best\"    \nFINETUNED_QUERY_DIR = \"/kaggle/working/mE5_lora8bit_queryonly_v1/best\"   \nBASE_MODEL          = \"intfloat/multilingual-e5-base\"                    \n\nUSE_PREFIXES       = True\nUSE_NEUTRAL_INST   = True  \nNEUTRAL_RU         = \"Используй точное соответствие смысла запроса и отрывка; не делай выводов.\"\n\nCACHE_DIR   = \"/kaggle/working/rusbeir_eval_cache\"\nOUT_ROOT    = \"/kaggle/working/rusbeir_eval_runs\"\nHF_CACHE    = \"/kaggle/working/hf_cache\"\nos.makedirs(CACHE_DIR, exist_ok=True)\nos.makedirs(OUT_ROOT,  exist_ok=True)\nos.makedirs(HF_CACHE,  exist_ok=True)\n\nprint(f\"Device: {DEVICE} | BATCH={BATCH_SIZE} | MAX_LEN={MAX_LEN} | FAST_Q_CAP={FAST_Q_CAP} | DOC_CAP={DOC_CAP}\")\n\nSETS = {\n    \"rus-arguana\": {\n        \"data_repo\": \"kaengreg/rus-arguana\",\n        \"beir_ids\":  [\"beir/arguana\", \"beir-v1.0.0/arguana\", \"arguana\"],   # try in order\n    },\n    \"rus-scifact\": {\n        \"data_repo\": \"kaengreg/rus-scifact\",\n        \"beir_ids\":  [\"beir/scifact/test\", \"beir/scifact\"],                # try in order\n    },\n}\n\ndef _copy(src, dst):\n    os.makedirs(os.path.dirname(dst), exist_ok=True)\n    if src == dst:\n        return\n    with open(src, \"rb\") as f_in, open(dst, \"wb\") as f_out:\n        f_out.write(f_in.read())\n\ndef _maybe_gunzip(src, dst):\n    if src.endswith(\".gz\"):\n        with gzip.open(src, \"rt\", encoding=\"utf-8\") as f_in, open(dst, \"w\", encoding=\"utf-8\") as f_out:\n            for line in f_in: f_out.write(line)\n    else:\n        _copy(src, dst)\n\ndef fetch_raw_dataset(name: str, data_repo: str) -> str:\n    out_dir = os.path.join(OUT_ROOT, name)\n    os.makedirs(out_dir, exist_ok=True)\n    root = snapshot_download(repo_id=data_repo, repo_type=\"dataset\",\n                             allow_patterns=[\"corpus.jsonl\", \"queries.jsonl\", \"*.gz\"],\n                             local_dir=HF_CACHE, local_dir_use_symlinks=False)\n    cand_corpus  = glob.glob(os.path.join(root, \"**\", \"corpus.jsonl*\"),  recursive=True)\n    cand_queries = glob.glob(os.path.join(root, \"**\", \"queries.jsonl*\"), recursive=True)\n    if not cand_corpus or not cand_queries:\n        raise FileNotFoundError(f\"Could not find corpus.jsonl/queries.jsonl in {data_repo}\")\n    _maybe_gunzip(sorted(cand_corpus)[-1],  os.path.join(out_dir, \"corpus.jsonl\"))\n    _maybe_gunzip(sorted(cand_queries)[-1], os.path.join(out_dir, \"queries.jsonl\"))\n    return out_dir\n\ndef smart_open(path):\n    if path.endswith(\".gz\"):\n        return io.TextIOWrapper(gzip.open(path, \"rb\"), encoding=\"utf-8\")\n    return open(path, \"r\", encoding=\"utf-8\")\n\ndef load_corpus(path: str) -> Dict[str, str]:\n    corpus = {}\n    with smart_open(path) as f:\n        for line in f:\n            j = json.loads(line)\n            cid   = str(j.get(\"_id\") or j.get(\"id\"))\n            title = (j.get(\"title\") or \"\").strip()\n            text  = (j.get(\"text\")  or \"\").strip()\n            corpus[cid] = (title + \" \" + text).strip()\n    return corpus\n\ndef load_queries(path: str) -> Dict[str, str]:\n    queries = {}\n    with smart_open(path) as f:\n        for line in f:\n            j = json.loads(line)\n            qid  = str(j.get(\"_id\") or j.get(\"id\"))\n            text = (j.get(\"text\") or \"\").strip()\n            queries[qid] = text\n    return queries\n\ndef build_qrels_from_beir(beir_ids: List[str], hf_qids: set, hf_docs: set):\n    ds = None; chosen = None\n    tried = []\n    for dsid in beir_ids:\n        tried.append(dsid)\n        try:\n            ds = irds.load(dsid)\n            chosen = dsid\n            break\n        except KeyError:\n            continue\n    if ds is None:\n        raise KeyError(f\"None of these IRDS ids exist: {tried}\")\n\n    qrels = defaultdict(dict)\n    kept = skipped = 0\n    for q in ds.qrels_iter():\n        qid = str(q.query_id)\n        did = str(q.doc_id)\n        rel = int(getattr(q, \"relevance\", 1))\n        if (qid in hf_qids) and (did in hf_docs) and rel > 0:\n            qrels[qid][did] = 1\n            kept += 1\n        else:\n            skipped += 1\n    print(f\"  qrels from {chosen} -> kept {kept:,}, skipped {skipped:,}\")\n    return dict(qrels)\n\ndef add_prefix_query(s: str, for_inst_model: bool) -> str:\n    if not USE_PREFIXES:\n        return s\n    q = f\"query: {s}\".strip()\n    if for_inst_model and USE_NEUTRAL_INST and NEUTRAL_RU:\n        q = f\"{q} {NEUTRAL_RU}\".strip()\n    return q\n\ndef add_prefix_passage(s: str) -> str:\n    if not USE_PREFIXES:\n        return s\n    return f\"passage: {s}\".strip()\n\ndef corpus_signature(ids: List[str]) -> str:\n    key = \"||\".join(ids[:50]) + \"||\" + \"||\".join(ids[-50:]) + f\"||{len(ids)}\"\n    return hashlib.md5(key.encode()).hexdigest()[:8]\n\n@torch.no_grad()\ndef encode_texts(model, tok, texts: List[str], batch_size=BATCH_SIZE, max_len=MAX_LEN, device=DEVICE):\n    all_embs = []\n    for i in tqdm(range(0, len(texts), batch_size), desc=f\"Encoding@{device}\", leave=False):\n        batch = texts[i:i+batch_size]\n        enc = tok(batch, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\")\n        for k in enc: enc[k] = enc[k].to(device)\n        out = model(**enc).last_hidden_state   # [B, L, H]\n        mask = enc[\"attention_mask\"].unsqueeze(-1).expand_as(out).float()\n        pooled = (out * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1e-9)  # mean pool\n        pooled = F.normalize(pooled, p=2, dim=1)\n        all_embs.append(pooled.detach().cpu())\n    return torch.cat(all_embs, dim=0) if all_embs else torch.empty(0, model.config.hidden_size)\n\ndef build_index(embs_cpu: torch.Tensor):\n    if FAISS_OK:\n        index = faiss.IndexFlatIP(embs_cpu.shape[1])\n        index.add(embs_cpu.numpy().astype(\"float32\"))\n        return index, True\n    return embs_cpu, False\n\n@torch.no_grad()\ndef search(index, is_faiss: bool, q_embs: torch.Tensor, topk: int):\n    if is_faiss:\n        D, I = index.search(q_embs.numpy().astype(\"float32\"), topk)\n        return I, D\n    # torch fallback\n    q = q_embs; d = index\n    I_all, D_all = [], []\n    chunk = 256\n    for i in tqdm(range(0, q.shape[0], chunk), desc=\"Searching\", leave=False):\n        sims = torch.mm(q[i:i+chunk], d.t())\n        D, I = torch.topk(sims, k=min(topk, sims.size(1)), dim=1)\n        I_all.append(I.numpy()); D_all.append(D.numpy())\n    return np.vstack(I_all), np.vstack(D_all)\n\ndef eval_multi_k(run, qrels: Dict[str, Dict[str,int]], ks: List[int]):\n    ks = sorted(set(ks))\n    stats = {\"N\": 0, \"Hit@1\": 0.0}\n    for k in ks:\n        stats[f\"Recall@{k}\"] = 0.0\n        stats[f\"MRR@{k}\"]    = 0.0\n        stats[f\"nDCG@{k}\"]   = 0.0\n        stats[f\"MAP@{k}\"]    = 0.0   \n\n    n = 0\n    for qid, ranking in run.items():\n        if qid not in qrels: \n            continue\n        n += 1\n        rels = qrels[qid]\n        pos_ranks = [r for r,(did,_) in enumerate(ranking, start=1) if did in rels and rels[did] > 0]\n        pos_ranks.sort()\n\n        if pos_ranks and pos_ranks[0] == 1:\n            stats[\"Hit@1\"] += 1.0\n\n        m_total = sum(1 for _ in rels if rels[_] > 0)  \n\n        for k in ks:\n            stats[f\"Recall@{k}\"] += float(any(r <= k for r in pos_ranks))\n            stats[f\"MRR@{k}\"]    += (1.0/pos_ranks[0]) if (pos_ranks and pos_ranks[0] <= k) else 0.0\n            dcg  = sum(1.0/math.log2(r+1) for r in pos_ranks if r <= k)\n            idcg = sum(1.0/math.log2(r+1) for r in range(1, min(m_total, k)+1))\n            stats[f\"nDCG@{k}\"] += (dcg/idcg) if idcg > 0 else 0.0\n            m_k = min(m_total, k)\n            if m_k > 0:\n                hits = 0\n                ap_sum = 0.0\n                for r in pos_ranks:\n                    if r > k:\n                        break\n                    hits += 1\n                    ap_sum += hits / r\n                stats[f\"MAP@{k}\"] += ap_sum / m_k\n            else:\n                stats[f\"MAP@{k}\"] += 0.0\n\n    if n == 0:\n        return stats\n    stats[\"N\"] = n\n    stats[\"Hit@1\"] /= n\n    for k in ks:\n        stats[f\"Recall@{k}\"] /= n\n        stats[f\"MRR@{k}\"]    /= n\n        stats[f\"nDCG@{k}\"]   /= n\n        stats[f\"MAP@{k}\"]    /= n\n    return stats\n\ndef fmt(d):  \n    return {k:(round(v,4) if isinstance(v,float) else v) for k,v in d.items()}\n\ndef safe_load_peft_for_inference(base_model_name: str, adapter_dir: str, device: str):\n\n    from peft import PeftModel\n    torch.cuda.empty_cache()\n    dtype = torch.float16 if (device==\"cuda\") else None\n    try:\n        base = AutoModel.from_pretrained(base_model_name, torch_dtype=dtype, low_cpu_mem_usage=True)\n        base = base.to(device).eval()\n    except RuntimeError as e:\n        print(f\"[warn] base on GPU failed ({type(e).__name__}): using CPU fp32\")\n        base = AutoModel.from_pretrained(base_model_name, low_cpu_mem_usage=True).to(\"cpu\").eval()\n        device = \"cpu\"\n    try:\n        tok = AutoTokenizer.from_pretrained(adapter_dir, use_fast=True)\n    except Exception:\n        tok = AutoTokenizer.from_pretrained(base_model_name, use_fast=True)\n    model = PeftModel.from_pretrained(base, adapter_dir).eval()\n    return tok, model, device\n\ndef safe_load_baseline(model_name: str, device: str):\n    dtype = torch.float16 if (device==\"cuda\") else None\n    try:\n        tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n        mdl = AutoModel.from_pretrained(model_name, torch_dtype=dtype, low_cpu_mem_usage=True).to(device).eval()\n        return tok, mdl, device\n    except RuntimeError as e:\n        print(f\"[warn] baseline on GPU failed ({type(e).__name__}): using CPU fp32\")\n        tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n        mdl = AutoModel.from_pretrained(model_name, low_cpu_mem_usage=True).to(\"cpu\").eval()\n        return tok, mdl, \"cpu\"\n\ndef evaluate_model(tag: str, tok, mdl, dev: str,\n                   doc_ids: List[str], doc_texts: List[str],\n                   qids_eval: List[str], q_texts: List[str],\n                   qrels: Dict[str, Dict[str,int]],\n                   cache_key: str):\n    print(f\"\\n=== {tag} ===\")\n\n    cache_path = os.path.join(CACHE_DIR, f\"docs__{cache_key}.pt\")\n    if os.path.exists(cache_path):\n        doc_embs = torch.load(cache_path, map_location=\"cpu\")\n        print(f\"[cache] loaded docs {doc_embs.shape} from {cache_path}\")\n    else:\n        doc_embs = encode_texts(mdl, tok, doc_texts, batch_size=BATCH_SIZE, max_len=MAX_LEN, device=dev)\n        torch.save(doc_embs, cache_path)\n        print(f\"[cache] saved docs {doc_embs.shape} -> {cache_path}\")\n\n    index, is_faiss = build_index(doc_embs)\n\n    q_embs = encode_texts(mdl, tok, q_texts, batch_size=BATCH_SIZE, max_len=MAX_LEN, device=dev)\n\n    I, D = search(index, is_faiss, q_embs, topk=max(K_LIST))\n\n    run = {}\n    for i, qid in enumerate(qids_eval):\n        ids = I[i]; scores = D[i]\n        run[qid] = [(doc_ids[j], float(scores[k])) for k, j in enumerate(ids)]\n\n    m = eval_multi_k(run, qrels, K_LIST)\n    print(fmt(m))\n    return m\n\ndef run_on_dataset(name: str, spec: dict):\n    base_dir = fetch_raw_dataset(name, spec[\"data_repo\"])\n    corpus   = load_corpus(os.path.join(base_dir, \"corpus.jsonl\"))\n    queries  = load_queries(os.path.join(base_dir, \"queries.jsonl\"))\n\n    if DOC_CAP:\n        rng = random.Random(SEED)\n        keys = list(corpus.keys()); rng.shuffle(keys)\n        keep = set(keys[:DOC_CAP])\n        corpus = {k:v for k,v in corpus.items() if k in keep}\n\n    hf_qids = set(queries.keys())\n    hf_docs = set(corpus.keys())\n    qrels   = build_qrels_from_beir(spec[\"beir_ids\"], hf_qids, hf_docs)\n\n    qids = [qid for qid in queries if qid in qrels]\n    if FAST_Q_CAP and len(qids) > FAST_Q_CAP:\n        rng = random.Random(SEED)\n        rng.shuffle(qids)\n        qids = qids[:FAST_Q_CAP]\n\n    print(f\"\\nDataset: {name} | Eval queries: {len(qids)} | Docs: {len(corpus)}\")\n\n    doc_ids   = list(corpus.keys())\n    doc_texts = [add_prefix_passage(corpus[d]) for d in doc_ids]\n\n    q_texts_base   = [add_prefix_query(queries[q], for_inst_model=False) for q in qids]\n    q_texts_inst   = [add_prefix_query(queries[q], for_inst_model=True ) for q in qids]\n    q_texts_qonly  = [add_prefix_query(queries[q], for_inst_model=False) for q in qids]  \n\n    results = {}\n\n    tok_b, mdl_b, dev_b = safe_load_baseline(BASE_MODEL, DEVICE)\n    cache_key_b = f\"{name}__baseline__{corpus_signature(doc_ids)}__L{MAX_LEN}\"\n    results[\"baseline\"] = evaluate_model(\"Baseline (mE5-base)\", tok_b, mdl_b, dev_b,\n                                         doc_ids, doc_texts, qids, q_texts_base, qrels, cache_key_b)\n    del mdl_b; torch.cuda.empty_cache()\n\n    if FINETUNED_INST_DIR and os.path.exists(FINETUNED_INST_DIR):\n        tok_i, mdl_i, dev_i = safe_load_peft_for_inference(BASE_MODEL, FINETUNED_INST_DIR, DEVICE)\n        cache_key_i = f\"{name}__inst__{corpus_signature(doc_ids)}__L{MAX_LEN}\"\n        results[\"finetuned_instruction\"] = evaluate_model(\"LoRA (instruction-aware)\", tok_i, mdl_i, dev_i,\n                                                          doc_ids, doc_texts, qids, q_texts_inst, qrels, cache_key_i)\n        del mdl_i; torch.cuda.empty_cache()\n    else:\n        print(\"(!) Skipping finetuned_instruction — dir not found.\")\n\n    if FINETUNED_QUERY_DIR and os.path.exists(FINETUNED_QUERY_DIR):\n        tok_q, mdl_q, dev_q = safe_load_peft_for_inference(BASE_MODEL, FINETUNED_QUERY_DIR, DEVICE)\n        cache_key_q = f\"{name}__queryonly__{corpus_signature(doc_ids)}__L{MAX_LEN}\"\n        results[\"finetuned_queryonly\"] = evaluate_model(\"LoRA (query-only)\", tok_q, mdl_q, dev_q,\n                                                        doc_ids, doc_texts, qids, q_texts_qonly, qrels, cache_key_q)\n        del mdl_q; torch.cuda.empty_cache()\n    else:\n        print(\"(!) Skipping finetuned_queryonly — dir not found.\")\n\n    print(\"\\n--- Summary:\", name, \"---\")\n    for k, v in results.items():\n        print(k, \"->\", fmt(v))\n    return results\n\nall_results = {}\nfor name, spec in SETS.items():\n    all_results[name] = run_on_dataset(name, spec)\n\nprint(\"\\n=== GRAND SUMMARY ===\")\nfor ds, res in all_results.items():\n    print(\"\\n\", ds)\n    for k, v in res.items():\n        print(\" \", k, \"->\", fmt(v))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T02:39:27.676068Z","iopub.execute_input":"2025-09-10T02:39:27.676335Z","iopub.status.idle":"2025-09-10T02:44:43.736122Z","shell.execute_reply.started":"2025-09-10T02:39:27.676317Z","shell.execute_reply":"2025-09-10T02:44:43.735350Z"}},"outputs":[{"name":"stdout","text":"Device: cuda | BATCH=64 | MAX_LEN=256 | FAST_Q_CAP=300 | DOC_CAP=None\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5490c40ccf34afd9c83bb5077774ba8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"queries.jsonl: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"013bc6a038974b0385ece254e4a17f10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"corpus.jsonl:   0%|          | 0.00/33.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fd1e110a37e401fabebeefaf46e93a1"}},"metadata":{}},{"name":"stderr","text":"[INFO] [starting] opening zip file\n[INFO] [starting] https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/arguana.zip\n[INFO] [finished] https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/arguana.zip: [00:00] [3.77MB] [4.83MB/s]\n[INFO] [finished] opening zip file [5.25s]                                                                 \n","output_type":"stream"},{"name":"stdout","text":"  qrels from beir/arguana -> kept 1,401, skipped 5\n\nDataset: rus-arguana | Eval queries: 300 | Docs: 8674\n\n=== Baseline (mE5-base) ===\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[cache] saved docs torch.Size([8674, 768]) -> /kaggle/working/rusbeir_eval_cache/docs__rus-arguana__baseline__9369b9db__L256.pt\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 300, 'Hit@1': 0.0, 'Recall@1': 0.0, 'MRR@1': 0.0, 'nDCG@1': 0.0, 'MAP@1': 0.0, 'Recall@5': 0.49, 'MRR@5': 0.1803, 'nDCG@5': 0.2572, 'MAP@5': 0.1803, 'Recall@10': 0.6733, 'MRR@10': 0.2045, 'nDCG@10': 0.3163, 'MAP@10': 0.2045}\n\n=== LoRA (instruction-aware) ===\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[cache] saved docs torch.Size([8674, 768]) -> /kaggle/working/rusbeir_eval_cache/docs__rus-arguana__inst__9369b9db__L256.pt\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 300, 'Hit@1': 0.0, 'Recall@1': 0.0, 'MRR@1': 0.0, 'nDCG@1': 0.0, 'MAP@1': 0.0, 'Recall@5': 0.6067, 'MRR@5': 0.2337, 'nDCG@5': 0.3272, 'MAP@5': 0.2337, 'Recall@10': 0.7667, 'MRR@10': 0.2554, 'nDCG@10': 0.3793, 'MAP@10': 0.2554}\n\n=== LoRA (query-only) ===\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[cache] saved docs torch.Size([8674, 768]) -> /kaggle/working/rusbeir_eval_cache/docs__rus-arguana__queryonly__9369b9db__L256.pt\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 300, 'Hit@1': 0.0, 'Recall@1': 0.0, 'MRR@1': 0.0, 'nDCG@1': 0.0, 'MAP@1': 0.0, 'Recall@5': 0.55, 'MRR@5': 0.2108, 'nDCG@5': 0.2957, 'MAP@5': 0.2108, 'Recall@10': 0.73, 'MRR@10': 0.236, 'nDCG@10': 0.3551, 'MAP@10': 0.236}\n\n--- Summary: rus-arguana ---\nbaseline -> {'N': 300, 'Hit@1': 0.0, 'Recall@1': 0.0, 'MRR@1': 0.0, 'nDCG@1': 0.0, 'MAP@1': 0.0, 'Recall@5': 0.49, 'MRR@5': 0.1803, 'nDCG@5': 0.2572, 'MAP@5': 0.1803, 'Recall@10': 0.6733, 'MRR@10': 0.2045, 'nDCG@10': 0.3163, 'MAP@10': 0.2045}\nfinetuned_instruction -> {'N': 300, 'Hit@1': 0.0, 'Recall@1': 0.0, 'MRR@1': 0.0, 'nDCG@1': 0.0, 'MAP@1': 0.0, 'Recall@5': 0.6067, 'MRR@5': 0.2337, 'nDCG@5': 0.3272, 'MAP@5': 0.2337, 'Recall@10': 0.7667, 'MRR@10': 0.2554, 'nDCG@10': 0.3793, 'MAP@10': 0.2554}\nfinetuned_queryonly -> {'N': 300, 'Hit@1': 0.0, 'Recall@1': 0.0, 'MRR@1': 0.0, 'nDCG@1': 0.0, 'MAP@1': 0.0, 'Recall@5': 0.55, 'MRR@5': 0.2108, 'nDCG@5': 0.2957, 'MAP@5': 0.2108, 'Recall@10': 0.73, 'MRR@10': 0.236, 'nDCG@10': 0.3551, 'MAP@10': 0.236}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"685c4859134a47228129d1f199241f64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"corpus.jsonl:   0%|          | 0.00/28.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57bfa06f67824db9ba26c9c27c0c118e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"queries.jsonl: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8318460da32b4132958973858d4add5c"}},"metadata":{}},{"name":"stderr","text":"[INFO] [starting] opening zip file\n[INFO] [starting] https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/scifact.zip\n[INFO] [finished] https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/scifact.zip: [00:00] [2.82MB] [4.17MB/s]\n[INFO] [finished] opening zip file [1.70s]                                                                 \n","output_type":"stream"},{"name":"stdout","text":"  qrels from beir/scifact/test -> kept 339, skipped 0\n\nDataset: rus-scifact | Eval queries: 300 | Docs: 5183\n\n=== Baseline (mE5-base) ===\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"[cache] saved docs torch.Size([5183, 768]) -> /kaggle/working/rusbeir_eval_cache/docs__rus-scifact__baseline__fe705ae6__L256.pt\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 300, 'Hit@1': 0.49, 'Recall@1': 0.49, 'MRR@1': 0.49, 'nDCG@1': 0.49, 'MAP@1': 0.49, 'Recall@5': 0.69, 'MRR@5': 0.5646, 'nDCG@5': 0.5845, 'MAP@5': 0.5496, 'Recall@10': 0.75, 'MRR@10': 0.5729, 'nDCG@10': 0.6073, 'MAP@10': 0.5608}\n\n=== LoRA (instruction-aware) ===\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"[cache] saved docs torch.Size([5183, 768]) -> /kaggle/working/rusbeir_eval_cache/docs__rus-scifact__inst__fe705ae6__L256.pt\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 300, 'Hit@1': 0.45, 'Recall@1': 0.45, 'MRR@1': 0.45, 'nDCG@1': 0.45, 'MAP@1': 0.45, 'Recall@5': 0.6667, 'MRR@5': 0.5336, 'nDCG@5': 0.5567, 'MAP@5': 0.5198, 'Recall@10': 0.7267, 'MRR@10': 0.542, 'nDCG@10': 0.58, 'MAP@10': 0.5315}\n\n=== LoRA (query-only) ===\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"[cache] saved docs torch.Size([5183, 768]) -> /kaggle/working/rusbeir_eval_cache/docs__rus-scifact__queryonly__fe705ae6__L256.pt\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 300, 'Hit@1': 0.4133, 'Recall@1': 0.4133, 'MRR@1': 0.4133, 'nDCG@1': 0.4133, 'MAP@1': 0.4133, 'Recall@5': 0.63, 'MRR@5': 0.494, 'nDCG@5': 0.5155, 'MAP@5': 0.4782, 'Recall@10': 0.6933, 'MRR@10': 0.5029, 'nDCG@10': 0.54, 'MAP@10': 0.4905}\n\n--- Summary: rus-scifact ---\nbaseline -> {'N': 300, 'Hit@1': 0.49, 'Recall@1': 0.49, 'MRR@1': 0.49, 'nDCG@1': 0.49, 'MAP@1': 0.49, 'Recall@5': 0.69, 'MRR@5': 0.5646, 'nDCG@5': 0.5845, 'MAP@5': 0.5496, 'Recall@10': 0.75, 'MRR@10': 0.5729, 'nDCG@10': 0.6073, 'MAP@10': 0.5608}\nfinetuned_instruction -> {'N': 300, 'Hit@1': 0.45, 'Recall@1': 0.45, 'MRR@1': 0.45, 'nDCG@1': 0.45, 'MAP@1': 0.45, 'Recall@5': 0.6667, 'MRR@5': 0.5336, 'nDCG@5': 0.5567, 'MAP@5': 0.5198, 'Recall@10': 0.7267, 'MRR@10': 0.542, 'nDCG@10': 0.58, 'MAP@10': 0.5315}\nfinetuned_queryonly -> {'N': 300, 'Hit@1': 0.4133, 'Recall@1': 0.4133, 'MRR@1': 0.4133, 'nDCG@1': 0.4133, 'MAP@1': 0.4133, 'Recall@5': 0.63, 'MRR@5': 0.494, 'nDCG@5': 0.5155, 'MAP@5': 0.4782, 'Recall@10': 0.6933, 'MRR@10': 0.5029, 'nDCG@10': 0.54, 'MAP@10': 0.4905}\n\n=== GRAND SUMMARY ===\n\n rus-arguana\n  baseline -> {'N': 300, 'Hit@1': 0.0, 'Recall@1': 0.0, 'MRR@1': 0.0, 'nDCG@1': 0.0, 'MAP@1': 0.0, 'Recall@5': 0.49, 'MRR@5': 0.1803, 'nDCG@5': 0.2572, 'MAP@5': 0.1803, 'Recall@10': 0.6733, 'MRR@10': 0.2045, 'nDCG@10': 0.3163, 'MAP@10': 0.2045}\n  finetuned_instruction -> {'N': 300, 'Hit@1': 0.0, 'Recall@1': 0.0, 'MRR@1': 0.0, 'nDCG@1': 0.0, 'MAP@1': 0.0, 'Recall@5': 0.6067, 'MRR@5': 0.2337, 'nDCG@5': 0.3272, 'MAP@5': 0.2337, 'Recall@10': 0.7667, 'MRR@10': 0.2554, 'nDCG@10': 0.3793, 'MAP@10': 0.2554}\n  finetuned_queryonly -> {'N': 300, 'Hit@1': 0.0, 'Recall@1': 0.0, 'MRR@1': 0.0, 'nDCG@1': 0.0, 'MAP@1': 0.0, 'Recall@5': 0.55, 'MRR@5': 0.2108, 'nDCG@5': 0.2957, 'MAP@5': 0.2108, 'Recall@10': 0.73, 'MRR@10': 0.236, 'nDCG@10': 0.3551, 'MAP@10': 0.236}\n\n rus-scifact\n  baseline -> {'N': 300, 'Hit@1': 0.49, 'Recall@1': 0.49, 'MRR@1': 0.49, 'nDCG@1': 0.49, 'MAP@1': 0.49, 'Recall@5': 0.69, 'MRR@5': 0.5646, 'nDCG@5': 0.5845, 'MAP@5': 0.5496, 'Recall@10': 0.75, 'MRR@10': 0.5729, 'nDCG@10': 0.6073, 'MAP@10': 0.5608}\n  finetuned_instruction -> {'N': 300, 'Hit@1': 0.45, 'Recall@1': 0.45, 'MRR@1': 0.45, 'nDCG@1': 0.45, 'MAP@1': 0.45, 'Recall@5': 0.6667, 'MRR@5': 0.5336, 'nDCG@5': 0.5567, 'MAP@5': 0.5198, 'Recall@10': 0.7267, 'MRR@10': 0.542, 'nDCG@10': 0.58, 'MAP@10': 0.5315}\n  finetuned_queryonly -> {'N': 300, 'Hit@1': 0.4133, 'Recall@1': 0.4133, 'MRR@1': 0.4133, 'nDCG@1': 0.4133, 'MAP@1': 0.4133, 'Recall@5': 0.63, 'MRR@5': 0.494, 'nDCG@5': 0.5155, 'MAP@5': 0.4782, 'Recall@10': 0.6933, 'MRR@10': 0.5029, 'nDCG@10': 0.54, 'MAP@10': 0.4905}\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# Unified Robustness / Prompt-Transfer Eval \n#  - Datasets: Mr.TyDi-RU (prepared), mFollowIR-RU, LAReQA XQuAD-R (RU↔EN)\n#  - Models:   mE5-base, optional LoRA inst-aware, LoRA query-only\n#  - Metrics:  Hit@1, Recall@K, MRR@K, MAP@K, nDCG@K, p-MRR@K\n#  - Robustness: Prompt ablation, Paraphrase variance (5x), Light noise stress\n\nimport os, io, re, json, math, time, random, hashlib, gzip, glob\nfrom collections import defaultdict\nfrom typing import Dict, List, Tuple\n\nos.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\nos.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\ntry:\n    import faiss\n    FAISS_OK = True\nexcept Exception:\n    FAISS_OK = False\n\nDEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE_D  = 64\nBATCH_SIZE_Q  = 64\nMAX_LEN       = 256\nK_LIST        = [1, 5, 10]\nSEED          = 42\n\nDO_PROMPT_ABLATION   = True    \nDO_PARAPHRASE_ROBUST = True    \nDO_NOISE_STRESS      = True    \n\nBASE_MODEL          = \"intfloat/multilingual-e5-base\"\nFINETUNED_INST_DIR  = \"/kaggle/working/mE5_lora8bit_infoNCE_v1/best\"     \nFINETUNED_QUERY_DIR = \"/kaggle/working/mE5_lora8bit_queryonly_v1/best\"   \n\nUSE_PREFIXES = True  \n\nROOT_WORK         = \"/kaggle/working\"\nCACHE_DIR_EMB     = os.path.join(ROOT_WORK, \"robust_eval_cache\"); os.makedirs(CACHE_DIR_EMB, exist_ok=True)\n\nMRTYDI_DIR        = os.path.join(ROOT_WORK, \"mrtydi_ru_prepared\")\nLAREQA_DIR        = os.path.join(ROOT_WORK, \"lareqa_xquad_r\"); os.makedirs(LAREQA_DIR, exist_ok=True)\n\nMRTYDI_DOC_CAP    = 20000\nMRTYDI_FAST_Q     = None\nMFOLLOWIR_DOC_CAP = None\nMFOLLOWIR_FAST_Q  = None\nLAREQA_DOC_CAP    = None\nLAREQA_FAST_Q     = None\n\nrandom.seed(SEED); np.random.seed(SEED)\nprint(f\"Device: {DEVICE} | K={K_LIST} | MAX_LEN={MAX_LEN}\")\n\nPROMPTS_RU = {\n    \"plain\": \"\",\n    \"minimal_ru\": \"Верни релевантные документы.\",\n    \"qa_ru\":      \"Найди предложение, отвечающее на вопрос.\",\n    \"search_ru\":  \"Найди веб-страницы, связанные с запросом.\",\n    \"mfoll_ru\":   \"Ответь кратко и по делу. Найди наиболее релевантные документы.\"\n}\nPROMPTS_EN = {\n    \"plain\": \"\",\n    \"minimal_en\": \"Return relevant documents.\",\n    \"qa_en\":      \"Find the sentence that answers the question.\",\n    \"search_en\":  \"Find web pages related to the query.\",\n    \"mfoll_en\":   \"Answer briefly and to the point. Retrieve the most relevant documents.\"\n}\n\nPARAPHRASES_RU = [\n    \"Дай наиболее релевантные документы.\",\n    \"Найди лучшие соответствующие тексты.\",\n    \"Верни документы, максимально подходящие запросу.\",\n    \"Подбери самые уместные тексты по запросу.\",\n    \"Найди тексты, которые лучше всего отвечают запросу.\",\n]\nPARAPHRASES_EN = [\n    \"Return the most relevant documents.\",\n    \"Find the best matching texts.\",\n    \"Provide documents that best fit the query.\",\n    \"Select the most suitable texts for the query.\",\n    \"Retrieve texts that best answer the query.\",\n]\n\n# Instruction BANK (still used for p-MRR and transfer)\nBANK = [\n  \"Ты — поисковая система. Найди наиболее релевантные документы, помогающие ответить на запрос. Возвращай документы, а не готовый ответ.\",\n  \"Найди пассажи, содержащие факты для ответа на вопрос. Отдавай приоритет точности и контекстной релевантности.\",\n  \"Выполни поиск по корпусу и верни документы, где максимально вероятно встретится ответ. Избегай нерелевантных совпадений.\"\n]\ndef bank_for_qid(qid: str) -> str:\n    return BANK[int(hashlib.md5(qid.encode()).hexdigest(), 16) % len(BANK)]\n\ndef add_prefix(s: str, kind: str) -> str:\n    if not USE_PREFIXES: return s\n    return ((\"query: \" if kind == \"query\" else \"passage: \") + s).strip()\n\ndef fmt(d): return {k:(round(v,4) if isinstance(v,float) else v) for k,v in d.items()}\n\ndef corpus_signature(ids: List[str]) -> str:\n    key = \"||\".join(ids[:50]) + \"||\" + \"||\".join(ids[-50:]) + f\"||{len(ids)}\"\n    return hashlib.md5(key.encode()).hexdigest()[:8]\n\ndef noise_variants(q: str) -> Dict[str, str]:\n    base = re.sub(r\"[^\\w\\s]\", \" \", q)\n    base = re.sub(r\"\\s+\", \" \", base).strip()\n    toks = base.split()\n    swapped = base\n    if len(toks) >= 2:\n        t = toks[:]\n        i = random.randrange(0, len(t)-1)\n        t[i], t[i+1] = t[i+1], t[i]\n        swapped = \" \".join(t)\n    return {\"clean\": q, \"nopunct\": base, \"swap\": swapped, \"lower\": q.lower()}\n\ndef mean_pool(last_hidden_state, attention_mask):\n    m = attention_mask.unsqueeze(-1).expand_as(last_hidden_state).float()\n    return (last_hidden_state * m).sum(dim=1) / m.sum(dim=1).clamp(min=1e-9)\n\n@torch.no_grad()\ndef encode_texts(model, tok, texts: List[str], batch_size: int, max_len: int, device: str):\n    outs=[]\n    for i in tqdm(range(0, len(texts), batch_size), desc=f\"Encode@{device}\", leave=False):\n        batch = texts[i:i+batch_size]\n        enc = tok(batch, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\")\n        for k in enc: enc[k] = enc[k].to(device)\n        last = model(**enc).last_hidden_state\n        pooled = mean_pool(last, enc[\"attention_mask\"])\n        outs.append(F.normalize(pooled, p=2, dim=1).cpu())\n    return torch.cat(outs, 0) if outs else torch.empty(0, model.config.hidden_size)\n\ndef build_index(doc_embs_cpu: torch.Tensor):\n    if FAISS_OK:\n        index = faiss.IndexFlatIP(doc_embs_cpu.shape[1])\n        index.add(doc_embs_cpu.numpy().astype(\"float32\"))\n        return index, True\n    return doc_embs_cpu, False\n\n@torch.no_grad()\ndef search_index(index, is_faiss: bool, q_embs: torch.Tensor, topk: int):\n    if is_faiss:\n        D, I = index.search(q_embs.numpy().astype(\"float32\"), topk)\n        return I, D\n    sims = torch.mm(q_embs, index.t())\n    D, I = torch.topk(sims, k=min(topk, sims.size(1)), dim=1)\n    return I.numpy(), D.numpy()\n\ndef eval_multi_k_and_perquery(run, qrels: Dict[str, Dict[str,int]], ks: List[int]):\n    ks = sorted(set(ks))\n    stats = {\"N\": 0, \"Hit@1\": 0.0}\n    for k in ks:\n        stats.update({f\"Recall@{k}\":0.0, f\"MRR@{k}\":0.0, f\"MAP@{k}\":0.0, f\"nDCG@{k}\":0.0})\n    per_mrr = {k:{} for k in ks}\n    per_ap  = {k:{} for k in ks}\n    n=0\n    for qid, ranking in run.items():\n        if qid not in qrels: continue\n        n += 1\n        rels = qrels[qid]\n        pos = [r for r,(did,_) in enumerate(ranking, start=1) if did in rels]\n        pos.sort()\n        if pos and pos[0]==1: stats[\"Hit@1\"] += 1.0\n        m_rel = len(rels)\n        for k in ks:\n            stats[f\"Recall@{k}\"] += float(any(r<=k for r in pos))\n            mrr_q = (1.0/pos[0]) if (pos and pos[0]<=k) else 0.0\n            stats[f\"MRR@{k}\"] += mrr_q\n            per_mrr[k][qid] = mrr_q\n            dcg  = sum(1.0/math.log2(r+1) for r in pos if r<=k)\n            idcg = sum(1.0/math.log2(r+1) for r in range(1, min(m_rel,k)+1))\n            stats[f\"nDCG@{k}\"] += (dcg/idcg) if idcg>0 else 0.0\n            hits=0; ap_sum=0.0\n            for r,(did,_) in enumerate(ranking[:k], start=1):\n                if did in rels:\n                    hits += 1; ap_sum += hits / r\n            denom = float(min(m_rel, k)) if m_rel>0 else 1.0\n            ap_q = (ap_sum/denom) if denom>0 else 0.0\n            stats[f\"MAP@{k}\"] += ap_q\n            per_ap[k][qid] = ap_q\n    if n==0: return stats, per_mrr, per_ap\n    stats[\"N\"]=n; stats[\"Hit@1\"] /= n\n    for k in ks:\n        stats[f\"Recall@{k}\"] /= n; stats[f\"MRR@{k}\"]/=n; stats[f\"MAP@{k}\"]/=n; stats[f\"nDCG@{k}\"]/=n\n    return stats, per_mrr, per_ap\n\ndef paired_diff_mean(per_a: Dict[str,float], per_b: Dict[str,float]) -> float:\n    qids = sorted(set(per_a.keys()) & set(per_b.keys()))\n    if not qids: return 0.0\n    return float(np.mean([per_b[q]-per_a[q] for q in qids]))\n\nfrom transformers import AutoTokenizer, AutoModel\ndef safe_load_baseline(model_name: str, device: str):\n    dtype = torch.float16 if (device==\"cuda\") else None\n    try:\n        tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n        mdl = AutoModel.from_pretrained(model_name, torch_dtype=dtype, low_cpu_mem_usage=True).to(device).eval()\n    except RuntimeError:\n        tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n        mdl = AutoModel.from_pretrained(model_name, low_cpu_mem_usage=True).to(\"cpu\").eval()\n        device = \"cpu\"\n    return tok, mdl, device\n\ndef safe_load_peft_for_inference(base_model_name: str, adapter_dir: str, device: str):\n    try:\n        import peft.import_utils as _iu\n        _iu.is_bnb_available = lambda: False\n    except Exception:\n        pass\n    from peft import PeftModel\n    dtype = torch.float16 if (device==\"cuda\") else None\n    try:\n        base = AutoModel.from_pretrained(base_model_name, torch_dtype=dtype, low_cpu_mem_usage=True).to(device).eval()\n    except RuntimeError:\n        base = AutoModel.from_pretrained(base_model_name, low_cpu_mem_usage=True).to(\"cpu\").eval()\n        device = \"cpu\"\n    try:\n        tok = AutoTokenizer.from_pretrained(adapter_dir, use_fast=True)\n    except Exception:\n        tok = AutoTokenizer.from_pretrained(base_model_name, use_fast=True)\n    model = PeftModel.from_pretrained(base, adapter_dir).eval()\n    return tok, model, device\n\ndef evaluate_model(tag: str, tok, mdl, dev: str,\n                   doc_ids: List[str], doc_texts_pref: List[str],\n                   qids: List[str], q_texts_pref: List[str],\n                   qrels: Dict[str, Dict[str,int]],\n                   cache_key: str):\n    print(f\"\\n=== {tag} ===\")\n    dcache = os.path.join(CACHE_DIR_EMB, f\"docs__{cache_key}.pt\")\n    if os.path.exists(dcache):\n        doc_embs = torch.load(dcache, map_location=\"cpu\"); print(f\"[cache] docs {doc_embs.shape} loaded\")\n    else:\n        doc_embs = encode_texts(mdl, tok, doc_texts_pref, BATCH_SIZE_D, MAX_LEN, dev)\n        torch.save(doc_embs, dcache); print(f\"[cache] docs {doc_embs.shape} saved -> {dcache}\")\n    index, is_faiss = build_index(doc_embs)\n    q_embs = encode_texts(mdl, tok, q_texts_pref, BATCH_SIZE_Q, MAX_LEN, dev)\n    I, D = search_index(index, is_faiss, q_embs, topk=max(K_LIST))\n    run = { qid: [(doc_ids[j], float(D[i][k])) for k, j in enumerate(I[i])] for i, qid in enumerate(qids) }\n    stats, per_mrr, per_ap = eval_multi_k_and_perquery(run, qrels, K_LIST)\n    print(fmt(stats))\n    return stats, per_mrr, per_ap\n\ndef build_prompted_queries(qids: List[str], qmap: Dict[str,str], prompt_text: str):\n    if prompt_text.strip()==\"\":\n        return [add_prefix(qmap[q], \"query\") for q in qids]\n    return [add_prefix((qmap[q] + \" \" + prompt_text).strip(), \"query\") for q in qids]\n\ndef paraphrase_suite(qids: List[str], qmap: Dict[str,str], paraphrases: List[str]):\n    return {f\"para{i+1}\": [add_prefix((qmap[q] + \" \" + p).strip(), \"query\") for q in qids]\n            for i,p in enumerate(paraphrases)}\n\ndef noise_suite(qids: List[str], qmap: Dict[str,str]):\n    suite = {\"clean\": [add_prefix(qmap[q], \"query\") for q in qids]}\n    # compute per-q variants and then stitch column-wise\n    per = {q: noise_variants(qmap[q]) for q in qids}\n    for key in [\"nopunct\",\"swap\",\"lower\"]:\n        suite[key] = [add_prefix(per[q][key], \"query\") for q in qids]\n    return suite\n\ndef agg_mean_std(stats_list: List[Dict[str,float]]):\n    if not stats_list: return {}\n    keys = stats_list[0].keys()\n    out = {}\n    for k in keys:\n        if isinstance(stats_list[0][k], float):\n            vals = [s[k] for s in stats_list]\n            out[f\"{k}_mean\"] = float(np.mean(vals))\n            out[f\"{k}_std\"]  = float(np.std(vals))\n    return out\n\n# A) Mr.TyDi-RU (prepared)\n\ndef prepare_mrtydi_ru(out_dir: str):\n    if all(os.path.exists(os.path.join(out_dir, fn)) for fn in [\"corpus.jsonl\",\"queries.jsonl\",\"qrels.tsv\"]):\n        print(\"[MrTyDi] prepared files already present.\"); return\n    print(\"[MrTyDi] preparing via ir_datasets …\")\n    import ir_datasets as irds\n    os.makedirs(out_dir, exist_ok=True)\n    ds = irds.load(\"mr-tydi/ru/test\")\n    with open(os.path.join(out_dir, \"corpus.jsonl\"), \"w\", encoding=\"utf-8\") as fc:\n        for d in ds.docs_iter():\n            j = {\"_id\": str(d.doc_id), \"title\": getattr(d, \"title\", \"\") or \"\", \"text\": d.text or \"\"}\n            fc.write(json.dumps(j, ensure_ascii=False) + \"\\n\")\n    with open(os.path.join(out_dir, \"queries.jsonl\"), \"w\", encoding=\"utf-8\") as fq:\n        for q in ds.queries_iter():\n            j = {\"_id\": str(q.query_id), \"text\": q.text}\n            fq.write(json.dumps(j, ensure_ascii=False) + \"\\n\")\n    with open(os.path.join(out_dir, \"qrels.tsv\"), \"w\", encoding=\"utf-8\") as fr:\n        for r in ds.qrels_iter():\n            rel = int(getattr(r, \"relevance\", 1) > 0)\n            fr.write(f\"{r.query_id}\\t{r.doc_id}\\t{rel}\\n\")\n    print(\"[MrTyDi] wrote corpus.jsonl, queries.jsonl, qrels.tsv\")\n\ndef load_jsonl_map(path: str, key=\"_id\", text=\"text\"):\n    m={}\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            j = json.loads(line); i = str(j.get(key) or j.get(\"id\")); t=(j.get(text) or \"\").strip()\n            if i and t: m[i]=t\n    return m\n\ndef load_qrels_tsv(path: str):\n    qrels=defaultdict(dict)\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for row in f:\n            parts=row.strip().split(\"\\t\")\n            if len(parts)<3: continue\n            qid,did,rel=parts[0],parts[1],parts[2]\n            try: r = int(float(rel))\n            except: continue\n            if r>0: qrels[qid][did]=1\n    return dict(qrels)\n\ndef run_mrtydi():\n    print(\"\\n========== Mr.TyDi-RU ==========\")\n    prepare_mrtydi_ru(MRTYDI_DIR)\n    # load\n    corpus_raw = {}\n    with open(os.path.join(MRTYDI_DIR,\"corpus.jsonl\"), \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            j=json.loads(line); cid=str(j.get(\"_id\") or j.get(\"id\"))\n            title=(j.get(\"title\") or \"\").strip(); text=(j.get(\"text\") or \"\").strip()\n            corpus_raw[cid]=(title+\" \"+text).strip()\n    queries = load_jsonl_map(os.path.join(MRTYDI_DIR,\"queries.jsonl\"))\n    qrels   = load_qrels_tsv(os.path.join(MRTYDI_DIR,\"qrels.tsv\"))\n    # subset: keep positives + fill negatives\n    rng=random.Random(SEED)\n    qids_all=[qid for qid in qrels if qid in queries]\n    all_pos=set()\n    for qid in qids_all: all_pos.update(qrels[qid].keys())\n    pos_doc_ids=[d for d in all_pos if d in corpus_raw]\n    if MRTYDI_DOC_CAP is None:\n        doc_ids=list(corpus_raw.keys()); neg_added=\"N/A\"\n    else:\n        if len(pos_doc_ids)>=MRTYDI_DOC_CAP:\n            rng.shuffle(pos_doc_ids); doc_ids=pos_doc_ids[:MRTYDI_DOC_CAP]; neg_added=0\n        else:\n            need=MRTYDI_DOC_CAP-len(pos_doc_ids)\n            neg_pool=[d for d in corpus_raw if d not in all_pos]; rng.shuffle(neg_pool)\n            neg_added=min(need,len(neg_pool)); doc_ids=pos_doc_ids+neg_pool[:neg_added]\n    corpus={d:corpus_raw[d] for d in doc_ids}\n    doc_texts=[add_prefix(corpus[d],\"passage\") for d in doc_ids]\n    qids=[qid for qid in qids_all if any((did in corpus) for did in qrels[qid])]\n    if MRTYDI_FAST_Q and len(qids)>MRTYDI_FAST_Q:\n        rng.shuffle(qids); qids=qids[:MRTYDI_FAST_Q]\n    print(f\"Docs={len(doc_ids):,} | Queries={len(qids):,} | PosDocs={len(pos_doc_ids):,} | NegAdded={neg_added}\")\n\n    loaders=[]\n    tok_b, mdl_b, dev_b = safe_load_baseline(BASE_MODEL, DEVICE)\n    loaders.append((\"BASE\", tok_b, mdl_b, dev_b, f\"mrtydi__base__{corpus_signature(doc_ids)}__L{MAX_LEN}\"))\n    if FINETUNED_INST_DIR and os.path.isdir(FINETUNED_INST_DIR):\n        tok_i, mdl_i, dev_i = safe_load_peft_for_inference(BASE_MODEL, FINETUNED_INST_DIR, DEVICE)\n        loaders.append((\"FT_INST\", tok_i, mdl_i, dev_i, f\"mrtydi__inst__{corpus_signature(doc_ids)}__L{MAX_LEN}\"))\n    if FINETUNED_QUERY_DIR and os.path.isdir(FINETUNED_QUERY_DIR):\n        tok_q, mdl_q, dev_q = safe_load_peft_for_inference(BASE_MODEL, FINETUNED_QUERY_DIR, DEVICE)\n        loaders.append((\"FT_QUERY\", tok_q, mdl_q, dev_q, f\"mrtydi__qonly__{corpus_signature(doc_ids)}__L{MAX_LEN}\"))\n\n    # base query maps\n    qmap = {qid: queries[qid] for qid in qids}\n    results = {}\n\n    for tag, tok, mdl, dev, sigbase in loaders:\n        # plain\n        q_plain = [add_prefix(qmap[q],\"query\") for q in qids]\n        stats_plain, per_plain, _ = evaluate_model(f\"MrTyDi | {tag} | plain\", tok, mdl, dev, doc_ids, doc_texts, qids, q_plain, qrels, sigbase)\n\n        # BANK (for p-MRR)\n        q_bank  = [add_prefix((qmap[q]+\" \"+bank_for_qid(q)).strip(),\"query\") for q in qids]\n        stats_bank, per_bank, _ = evaluate_model(f\"MrTyDi | {tag} | q+BANK\", tok, mdl, dev, doc_ids, doc_texts, qids, q_bank, qrels, sigbase)\n\n        print(f\"\\n--- MrTyDi p-MRR (BANK − plain) :: {tag} ---\")\n        print({f\"pMRR@{k}\": round(paired_diff_mean(per_plain[k], per_bank[k]), 6) for k in K_LIST})\n\n        # Prompt ablation\n        if DO_PROMPT_ABLATION:\n            print(f\"\\n--- Prompt ablation :: {tag} ---\")\n            for pname, prompt in PROMPTS_RU.items():\n                qv = build_prompted_queries(qids, qmap, prompt)\n                s,_,_ = evaluate_model(f\"MrTyDi | {tag} | prompt:{pname}\", tok, mdl, dev, doc_ids, doc_texts, qids, qv, qrels, sigbase)\n\n        # Paraphrase robustness (mean/std)\n        if DO_PARAPHRASE_ROBUST:\n            print(f\"\\n--- Paraphrase robustness (5x) :: {tag} ---\")\n            suite = paraphrase_suite(qids, qmap, PARAPHRASES_RU)\n            stats_list=[]\n            for key, qv in suite.items():\n                s,_,_ = evaluate_model(f\"MrTyDi | {tag} | {key}\", tok, mdl, dev, doc_ids, doc_texts, qids, qv, qrels, sigbase)\n                stats_list.append(s)\n            agg = agg_mean_std(stats_list)\n            print(\"Paraphrase mean/std:\", fmt(agg))\n\n        # Noise stress\n        if DO_NOISE_STRESS:\n            print(f\"\\n--- Noise stress :: {tag} ---\")\n            suite = noise_suite(qids, qmap)\n            base_stats=None\n            for key, qv in suite.items():\n                s,_,_ = evaluate_model(f\"MrTyDi | {tag} | noise:{key}\", tok, mdl, dev, doc_ids, doc_texts, qids, qv, qrels, sigbase)\n                if key==\"clean\": base_stats=s\n                else:\n                    # report MRR@k drop vs clean\n                    drops = {f\"ΔMRR@{k}\": round(s.get(f\"MRR@{k}\",0)-base_stats.get(f\"MRR@{k}\",0), 6) for k in K_LIST}\n                    print(f\"  drop vs clean ({key}):\", drops)\n\n        results[tag]=stats_plain  # keep at least plain for summary\n\n        del mdl; torch.cuda.empty_cache()\n\n    print(\"\\n=== Mr.TyDi SUMMARY (plain only) ===\")\n    for k,v in results.items():\n        print(k, \"->\", fmt(v))\n    return results\n\n# B) mFollowIR-RU\n\ndef run_mfollowir():\n    print(\"\\n========== mFollowIR-RU ==========\")\n    from datasets import load_dataset\n    ds_q = load_dataset(\"jhu-clsp/mFollowIR-parquet\",\"queries-rus\")[\"queries\"]\n    ds_d = load_dataset(\"jhu-clsp/mFollowIR-parquet\",\"corpus-rus\")[\"corpus\"]\n    ds_og= load_dataset(\"jhu-clsp/mFollowIR-parquet\",\"qrels_og-rus\")[\"test\"]\n\n    doc_ids=[r[\"_id\"] for r in ds_d]\n    doc_txt=[add_prefix(((r.get(\"title\") or \"\")+\" \"+(r.get(\"text\") or \"\")).strip(),\"passage\") for r in ds_d]\n    if MFOLLOWIR_DOC_CAP:\n        rng=random.Random(SEED); idx=list(range(len(doc_ids))); rng.shuffle(idx); keep=sorted(idx[:MFOLLOWIR_DOC_CAP])\n        doc_ids=[doc_ids[i] for i in keep]; doc_txt=[doc_txt[i] for i in keep]\n    doc_set=set(doc_ids)\n    qrels=defaultdict(dict)\n    for r in ds_og:\n        if r[\"corpus-id\"] in doc_set and float(r[\"score\"])>0:\n            qrels[r[\"query-id\"]][r[\"corpus-id\"]]=1\n    qrels={k:v for k,v in qrels.items() if v}\n\n    qmap = {r[\"_id\"]: r[\"text\"].strip() for r in ds_q}\n    qids = [qid for qid in qmap if qid in qrels]\n    if MFOLLOWIR_FAST_Q and len(qids)>MFOLLOWIR_FAST_Q:\n        rng=random.Random(SEED); rng.shuffle(qids); qids=qids[:MFOLLOWIR_FAST_Q]\n\n    loaders=[]\n    tok_b, mdl_b, dev_b = safe_load_baseline(BASE_MODEL, DEVICE)\n    loaders.append((\"BASE\", tok_b, mdl_b, dev_b, f\"mfollowir__base__{corpus_signature(doc_ids)}__L{MAX_LEN}\"))\n    if FINETUNED_INST_DIR and os.path.isdir(FINETUNED_INST_DIR):\n        tok_i, mdl_i, dev_i = safe_load_peft_for_inference(BASE_MODEL, FINETUNED_INST_DIR, DEVICE)\n        loaders.append((\"FT_INST\", tok_i, mdl_i, dev_i, f\"mfollowir__inst__{corpus_signature(doc_ids)}__L{MAX_LEN}\"))\n    if FINETUNED_QUERY_DIR and os.path.isdir(FINETUNED_QUERY_DIR):\n        tok_q, mdl_q, dev_q = safe_load_peft_for_inference(BASE_MODEL, FINETUNED_QUERY_DIR, DEVICE)\n        loaders.append((\"FT_QUERY\", tok_q, mdl_q, dev_q, f\"mfollowir__qonly__{corpus_signature(doc_ids)}__L{MAX_LEN}\"))\n\n    for tag, tok, mdl, dev, sigbase in loaders:\n        # plain\n        q_plain=[add_prefix(qmap[q],\"query\") for q in qids]\n        stats_plain, per_plain, _ = evaluate_model(f\"mFollowIR | {tag} | plain\", tok, mdl, dev, doc_ids, doc_txt, qids, q_plain, qrels, sigbase)\n        # BANK\n        q_bank=[add_prefix((qmap[q]+\" \"+bank_for_qid(q)).strip(),\"query\") for q in qids]\n        stats_bank, per_bank, _ = evaluate_model(f\"mFollowIR | {tag} | q+BANK\", tok, mdl, dev, doc_ids, doc_txt, qids, q_bank, qrels, sigbase)\n\n        print(f\"\\n--- mFollowIR p-MRR (BANK − plain) :: {tag} ---\")\n        print({f\"pMRR@{k}\": round(paired_diff_mean(per_plain[k], per_bank[k]),6) for k in K_LIST})\n\n        if DO_PROMPT_ABLATION:\n            print(f\"\\n--- Prompt ablation :: {tag} ---\")\n            for pname, prompt in PROMPTS_RU.items():\n                qv = build_prompted_queries(qids, qmap, prompt)\n                evaluate_model(f\"mFollowIR | {tag} | prompt:{pname}\", tok, mdl, dev, doc_ids, doc_txt, qids, qv, qrels, sigbase)\n        if DO_PARAPHRASE_ROBUST:\n            print(f\"\\n--- Paraphrase robustness (5x) :: {tag} ---\")\n            suite=paraphrase_suite(qids, qmap, PARAPHRASES_RU)\n            stats_list=[]\n            for key,qv in suite.items():\n                s,_,_ = evaluate_model(f\"mFollowIR | {tag} | {key}\", tok, mdl, dev, doc_ids, doc_txt, qids, qv, qrels, sigbase)\n                stats_list.append(s)\n            agg=agg_mean_std(stats_list); print(\"Paraphrase mean/std:\", fmt(agg))\n        if DO_NOISE_STRESS:\n            print(f\"\\n--- Noise stress :: {tag} ---\")\n            suite=noise_suite(qids, qmap)\n            base_stats=None\n            for key,qv in suite.items():\n                s,_,_ = evaluate_model(f\"mFollowIR | {tag} | noise:{key}\", tok, mdl, dev, doc_ids, doc_txt, qids, qv, qrels, sigbase)\n                if key==\"clean\": base_stats=s\n                else:\n                    drops={f\"ΔMRR@{k}\": round(s.get(f\"MRR@{k}\",0)-base_stats.get(f\"MRR@{k}\",0),6) for k in K_LIST}\n                    print(f\"  drop vs clean ({key}):\", drops)\n\n        del mdl; torch.cuda.empty_cache()\n\n    return {\"done\": True}\n\n# C) LAReQA XQuAD-R RU↔EN\n\nimport requests\ndef ensure_lang_file(lang: str, out_dir: str) -> str:\n    fp=os.path.join(out_dir, f\"{lang}.json\")\n    if os.path.exists(fp): return fp\n    url=f\"https://raw.githubusercontent.com/google-research-datasets/lareqa/master/xquad-r/{lang}.json\"\n    print(f\"[fetch] {lang}.json …\"); r=requests.get(url,timeout=30); r.raise_for_status()\n    with open(fp,\"wb\") as f: f.write(r.content); return fp\n\ndef split_sentences(context: str, sent_field):\n    if isinstance(sent_field, list) and sent_field and isinstance(sent_field[0], list):\n        sents=[]; \n        for s,e in sent_field:\n            try:\n                t=context[s:e].strip()\n                if t: sents.append(t)\n            except: pass\n        return sents\n    if isinstance(sent_field, list) and sent_field and isinstance(sent_field[0], str):\n        return [s.strip() for s in sent_field if str(s).strip()]\n    return re.split(r'\\s*(?<=\\.|\\?|!)\\s+', context.strip())\n\ndef load_xquadr(path: str):\n    with open(path,\"r\",encoding=\"utf-8\") as f: data=json.load(f)\n    qa_map={}; sents=[]\n    for art in data.get(\"data\",[]):\n        for par in art.get(\"paragraphs\",[]):\n            context=par.get(\"context\",\"\") or \"\"\n            ss=split_sentences(context, par.get(\"sentences\")); base=len(sents); sents.extend(ss)\n            for qa in par.get(\"qas\",[]):\n                qid=qa.get(\"id\") or qa.get(\"qid\") or \"\"\n                qtext=qa.get(\"question\",\"\").strip()\n                answers=[a.get(\"text\",\"\").strip() for a in qa.get(\"answers\",[]) if str(a.get(\"text\",\"\")).strip()]\n                qa_map[qid]={\"question\":qtext,\"answers\":answers}\n    return sents, qa_map\n\ndef normalize(s): return re.sub(r\"\\s+\",\" \", s.strip().lower())\n\ndef build_xling_ir(qas_src, answers_tgt, cand_sents, cand_lang: str, doc_cap: int, fast_q: int):\n    doc_ids=[f\"{cand_lang}-s{ix}\" for ix in range(len(cand_sents))]\n    if doc_cap and doc_cap<len(doc_ids):\n        rng=random.Random(SEED); idx=list(range(len(doc_ids))); rng.shuffle(idx); keep=sorted(idx[:doc_cap])\n        cand_sents=[cand_sents[i] for i in keep]; doc_ids=[doc_ids[i] for i in keep]\n    qids_all=sorted(set(qas_src.keys()) & set(answers_tgt.keys()))\n    if fast_q and fast_q<len(qids_all):\n        rng=random.Random(SEED); rng.shuffle(qids_all); qids_all=sorted(qids_all[:fast_q])\n    q_texts={qid: qas_src[qid][\"question\"] for qid in qids_all}\n    cand_norm=[normalize(x) for x in cand_sents]\n    qrels=defaultdict(dict)\n    for qid in qids_all:\n        golds=[normalize(a) for a in answers_tgt[qid].get(\"answers\",[]) if a.strip()]\n        if not golds: continue\n        for i,s in enumerate(cand_norm):\n            if any(g and g in s for g in golds):\n                qrels[qid][doc_ids[i]]=1\n    qids=[qid for qid in qids_all if qrels.get(qid)]\n    qrels={qid:qrels[qid] for qid in qids}\n    return doc_ids, cand_sents, qids, q_texts, qrels\n\ndef run_lareqa():\n    print(\"\\n========== LAReQA XQuAD-R ==========\")\n    ru_path=ensure_lang_file(\"ru\", LAREQA_DIR); en_path=ensure_lang_file(\"en\", LAREQA_DIR)\n    ru_sents, ru_qas = load_xquadr(ru_path); en_sents, en_qas = load_xquadr(en_path)\n\n    def run_dir(tag, qas_src, ans_tgt, cand_sents, cand_lang, prompts, paraphrases):\n        doc_ids, docs, qids, qmap, qrels = build_xling_ir(qas_src, ans_tgt, cand_sents, cand_lang, LAREQA_DOC_CAP, LAREQA_FAST_Q)\n        print(f\"\\n--- {tag} --- Docs={len(doc_ids):,} | Queries={len(qids):,} | QrelsQ={len(qrels):,}\")\n        doc_texts=[add_prefix(t,\"passage\") for t in docs]\n        loaders=[]\n        tok_b, mdl_b, dev_b = safe_load_baseline(BASE_MODEL, DEVICE)\n        loaders.append((\"BASE\", tok_b, mdl_b, dev_b, f\"lareqa_{tag}__base__{corpus_signature(doc_ids)}__L{MAX_LEN}\"))\n        if FINETUNED_INST_DIR and os.path.isdir(FINETUNED_INST_DIR):\n            tok_i, mdl_i, dev_i = safe_load_peft_for_inference(BASE_MODEL, FINETUNED_INST_DIR, DEVICE)\n            loaders.append((\"FT_INST\", tok_i, mdl_i, dev_i, f\"lareqa_{tag}__inst__{corpus_signature(doc_ids)}__L{MAX_LEN}\"))\n        if FINETUNED_QUERY_DIR and os.path.isdir(FINETUNED_QUERY_DIR):\n            tok_q, mdl_q, dev_q = safe_load_peft_for_inference(BASE_MODEL, FINETUNED_QUERY_DIR, DEVICE)\n            loaders.append((\"FT_QUERY\", tok_q, mdl_q, dev_q, f\"lareqa_{tag}__qonly__{corpus_signature(doc_ids)}__L{MAX_LEN}\"))\n\n        for mtag, tok, mdl, dev, sigbase in loaders:\n            # plain\n            q_plain=[add_prefix(qmap[q],\"query\") for q in qids]\n            stats_plain, per_plain, _ = evaluate_model(f\"LAReQA {tag} | {mtag} | plain\", tok, mdl, dev, doc_ids, doc_texts, qids, q_plain, qrels, sigbase)\n            # BANK\n            q_bank=[add_prefix((qmap[q]+\" \"+bank_for_qid(q)).strip(),\"query\") for q in qids]\n            stats_bank, per_bank, _ = evaluate_model(f\"LAReQA {tag} | {mtag} | q+BANK\", tok, mdl, dev, doc_ids, doc_texts, qids, q_bank, qrels, sigbase)\n            print(f\"\\n--- LAReQA p-MRR (BANK − plain) :: {tag} :: {mtag} ---\")\n            print({f\"pMRR@{k}\": round(paired_diff_mean(per_plain[k], per_bank[k]),6) for k in K_LIST})\n\n            if DO_PROMPT_ABLATION:\n                print(f\"\\n--- Prompt ablation :: {tag} :: {mtag} ---\")\n                for pname, prompt in prompts.items():\n                    qv = build_prompted_queries(qids, qmap, prompt)\n                    evaluate_model(f\"LAReQA {tag} | {mtag} | prompt:{pname}\", tok, mdl, dev, doc_ids, doc_texts, qids, qv, qrels, sigbase)\n            if DO_PARAPHRASE_ROBUST:\n                print(f\"\\n--- Paraphrase robustness (5x) :: {tag} :: {mtag} ---\")\n                suite=paraphrase_suite(qids, qmap, paraphrases)\n                stats_list=[]\n                for key,qv in suite.items():\n                    s,_,_ = evaluate_model(f\"LAReQA {tag} | {mtag} | {key}\", tok, mdl, dev, doc_ids, doc_texts, qids, qv, qrels, sigbase)\n                    stats_list.append(s)\n                agg=agg_mean_std(stats_list); print(\"Paraphrase mean/std:\", fmt(agg))\n            if DO_NOISE_STRESS:\n                print(f\"\\n--- Noise stress :: {tag} :: {mtag} ---\")\n                suite=noise_suite(qids, qmap)\n                base_stats=None\n                for key,qv in suite.items():\n                    s,_,_ = evaluate_model(f\"LAReQA {tag} | {mtag} | noise:{key}\", tok, mdl, dev, doc_ids, doc_texts, qids, qv, qrels, sigbase)\n                    if key==\"clean\": base_stats=s\n                    else:\n                        drops={f\"ΔMRR@{k}\": round(s.get(f\"MRR@{k}\",0)-base_stats.get(f\"MRR@{k}\",0),6) for k in K_LIST}\n                        print(f\"  drop vs clean ({key}):\", drops)\n            del mdl; torch.cuda.empty_cache()\n\n    # RU→EN: RU queries -> use RU prompts/paraphrases\n    run_dir(\"RU→EN\", ru_qas, en_qas, en_sents, \"en\", PROMPTS_RU, PARAPHRASES_RU)\n    # EN→RU: EN queries -> use EN prompts/paraphrases\n    run_dir(\"EN→RU\", en_qas, ru_qas, ru_sents, \"ru\", PROMPTS_EN, PARAPHRASES_EN)\n    return {\"done\": True}\n\n\nall_results = {}\nall_results[\"MrTyDi-RU\"]  = run_mrtydi()\nall_results[\"mFollowIR\"]  = run_mfollowir()\nall_results[\"LAReQA\"]     = run_lareqa()\n\nprint(\"\\n=============== DONE ===============\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T03:12:38.842849Z","iopub.execute_input":"2025-09-10T03:12:38.843141Z","iopub.status.idle":"2025-09-10T03:41:51.235170Z","shell.execute_reply.started":"2025-09-10T03:12:38.843120Z","shell.execute_reply":"2025-09-10T03:41:51.234390Z"}},"outputs":[{"name":"stdout","text":"Device: cuda | K=[1, 5, 10] | MAX_LEN=256\n\n========== Mr.TyDi-RU ==========\n[MrTyDi] prepared files already present.\nDocs=20,000 | Queries=995 | PosDocs=1,100 | NegAdded=18900\n\n=== MrTyDi | BASE | plain ===\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"[cache] docs torch.Size([20000, 768]) saved -> /kaggle/working/robust_eval_cache/docs__mrtydi__base__363db7cc__L256.pt\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.808, 'Recall@1': 0.808, 'MRR@1': 0.808, 'MAP@1': 0.808, 'nDCG@1': 0.808, 'Recall@5': 0.9357, 'MRR@5': 0.862, 'MAP@5': 0.8446, 'nDCG@5': 0.8686, 'Recall@10': 0.9588, 'MRR@10': 0.8653, 'MAP@10': 0.8494, 'nDCG@10': 0.8782}\n\n=== MrTyDi | BASE | q+BANK ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.7859, 'Recall@1': 0.7859, 'MRR@1': 0.7859, 'MAP@1': 0.7859, 'nDCG@1': 0.7859, 'Recall@5': 0.9095, 'MRR@5': 0.8357, 'MAP@5': 0.8151, 'nDCG@5': 0.8394, 'Recall@10': 0.9347, 'MRR@10': 0.8391, 'MAP@10': 0.8203, 'nDCG@10': 0.85}\n\n--- MrTyDi p-MRR (BANK − plain) :: BASE ---\n{'pMRR@1': -0.022111, 'pMRR@5': -0.026382, 'pMRR@10': -0.026228}\n\n--- Prompt ablation :: BASE ---\n\n=== MrTyDi | BASE | prompt:plain ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.808, 'Recall@1': 0.808, 'MRR@1': 0.808, 'MAP@1': 0.808, 'nDCG@1': 0.808, 'Recall@5': 0.9357, 'MRR@5': 0.862, 'MAP@5': 0.8446, 'nDCG@5': 0.8686, 'Recall@10': 0.9588, 'MRR@10': 0.8653, 'MAP@10': 0.8494, 'nDCG@10': 0.8782}\n\n=== MrTyDi | BASE | prompt:minimal_ru ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.8, 'Recall@1': 0.8, 'MRR@1': 0.8, 'MAP@1': 0.8, 'nDCG@1': 0.8, 'Recall@5': 0.9347, 'MRR@5': 0.8548, 'MAP@5': 0.8378, 'nDCG@5': 0.8629, 'Recall@10': 0.9548, 'MRR@10': 0.8575, 'MAP@10': 0.842, 'nDCG@10': 0.8715}\n\n=== MrTyDi | BASE | prompt:qa_ru ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.803, 'Recall@1': 0.803, 'MRR@1': 0.803, 'MAP@1': 0.803, 'nDCG@1': 0.803, 'Recall@5': 0.9256, 'MRR@5': 0.8526, 'MAP@5': 0.8352, 'nDCG@5': 0.8589, 'Recall@10': 0.9437, 'MRR@10': 0.8552, 'MAP@10': 0.8389, 'nDCG@10': 0.8665}\n\n=== MrTyDi | BASE | prompt:search_ru ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.794, 'Recall@1': 0.794, 'MRR@1': 0.794, 'MAP@1': 0.794, 'nDCG@1': 0.794, 'Recall@5': 0.9266, 'MRR@5': 0.8486, 'MAP@5': 0.8313, 'nDCG@5': 0.8558, 'Recall@10': 0.9457, 'MRR@10': 0.8512, 'MAP@10': 0.8355, 'nDCG@10': 0.8641}\n\n=== MrTyDi | BASE | prompt:mfoll_ru ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.796, 'Recall@1': 0.796, 'MRR@1': 0.796, 'MAP@1': 0.796, 'nDCG@1': 0.796, 'Recall@5': 0.9226, 'MRR@5': 0.8483, 'MAP@5': 0.8293, 'nDCG@5': 0.8538, 'Recall@10': 0.9467, 'MRR@10': 0.8516, 'MAP@10': 0.8343, 'nDCG@10': 0.8638}\n\n--- Paraphrase robustness (5x) :: BASE ---\n\n=== MrTyDi | BASE | para1 ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.8, 'Recall@1': 0.8, 'MRR@1': 0.8, 'MAP@1': 0.8, 'nDCG@1': 0.8, 'Recall@5': 0.9307, 'MRR@5': 0.8535, 'MAP@5': 0.8361, 'nDCG@5': 0.8608, 'Recall@10': 0.9518, 'MRR@10': 0.8563, 'MAP@10': 0.8401, 'nDCG@10': 0.8693}\n\n=== MrTyDi | BASE | para2 ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.8, 'Recall@1': 0.8, 'MRR@1': 0.8, 'MAP@1': 0.8, 'nDCG@1': 0.8, 'Recall@5': 0.9337, 'MRR@5': 0.8548, 'MAP@5': 0.836, 'nDCG@5': 0.8615, 'Recall@10': 0.9487, 'MRR@10': 0.8567, 'MAP@10': 0.8394, 'nDCG@10': 0.8684}\n\n=== MrTyDi | BASE | para3 ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.8, 'Recall@1': 0.8, 'MRR@1': 0.8, 'MAP@1': 0.8, 'nDCG@1': 0.8, 'Recall@5': 0.9236, 'MRR@5': 0.8509, 'MAP@5': 0.8326, 'nDCG@5': 0.8565, 'Recall@10': 0.9457, 'MRR@10': 0.854, 'MAP@10': 0.8368, 'nDCG@10': 0.8652}\n\n=== MrTyDi | BASE | para4 ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.794, 'Recall@1': 0.794, 'MRR@1': 0.794, 'MAP@1': 0.794, 'nDCG@1': 0.794, 'Recall@5': 0.9236, 'MRR@5': 0.8478, 'MAP@5': 0.8296, 'nDCG@5': 0.8542, 'Recall@10': 0.9427, 'MRR@10': 0.8503, 'MAP@10': 0.8332, 'nDCG@10': 0.8618}\n\n=== MrTyDi | BASE | para5 ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.7849, 'Recall@1': 0.7849, 'MRR@1': 0.7849, 'MAP@1': 0.7849, 'nDCG@1': 0.7849, 'Recall@5': 0.9206, 'MRR@5': 0.84, 'MAP@5': 0.8201, 'nDCG@5': 0.8462, 'Recall@10': 0.9387, 'MRR@10': 0.8425, 'MAP@10': 0.8241, 'nDCG@10': 0.8541}\nParaphrase mean/std: {'Hit@1_mean': 0.7958, 'Hit@1_std': 0.0059, 'Recall@1_mean': 0.7958, 'Recall@1_std': 0.0059, 'MRR@1_mean': 0.7958, 'MRR@1_std': 0.0059, 'MAP@1_mean': 0.7958, 'MAP@1_std': 0.0059, 'nDCG@1_mean': 0.7958, 'nDCG@1_std': 0.0059, 'Recall@5_mean': 0.9264, 'Recall@5_std': 0.0049, 'MRR@5_mean': 0.8494, 'MRR@5_std': 0.0052, 'MAP@5_mean': 0.8309, 'MAP@5_std': 0.0059, 'nDCG@5_mean': 0.8559, 'nDCG@5_std': 0.0055, 'Recall@10_mean': 0.9455, 'Recall@10_std': 0.0046, 'MRR@10_mean': 0.852, 'MRR@10_std': 0.0052, 'MAP@10_mean': 0.8347, 'MAP@10_std': 0.0058, 'nDCG@10_mean': 0.8638, 'nDCG@10_std': 0.0055}\n\n--- Noise stress :: BASE ---\n\n=== MrTyDi | BASE | noise:clean ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.808, 'Recall@1': 0.808, 'MRR@1': 0.808, 'MAP@1': 0.808, 'nDCG@1': 0.808, 'Recall@5': 0.9357, 'MRR@5': 0.862, 'MAP@5': 0.8446, 'nDCG@5': 0.8686, 'Recall@10': 0.9588, 'MRR@10': 0.8653, 'MAP@10': 0.8494, 'nDCG@10': 0.8782}\n\n=== MrTyDi | BASE | noise:nopunct ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.7799, 'Recall@1': 0.7799, 'MRR@1': 0.7799, 'MAP@1': 0.7799, 'nDCG@1': 0.7799, 'Recall@5': 0.9126, 'MRR@5': 0.8343, 'MAP@5': 0.8167, 'nDCG@5': 0.8414, 'Recall@10': 0.9337, 'MRR@10': 0.8372, 'MAP@10': 0.8209, 'nDCG@10': 0.8502}\n  drop vs clean (nopunct): {'ΔMRR@1': -0.028141, 'ΔMRR@5': -0.027772, 'ΔMRR@10': -0.028132}\n\n=== MrTyDi | BASE | noise:swap ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.7678, 'Recall@1': 0.7678, 'MRR@1': 0.7678, 'MAP@1': 0.7678, 'nDCG@1': 0.7678, 'Recall@5': 0.8985, 'MRR@5': 0.8225, 'MAP@5': 0.8027, 'nDCG@5': 0.8278, 'Recall@10': 0.9226, 'MRR@10': 0.8258, 'MAP@10': 0.8078, 'nDCG@10': 0.8381}\n  drop vs clean (swap): {'ΔMRR@1': -0.040201, 'ΔMRR@5': -0.039497, 'ΔMRR@10': -0.039501}\n\n=== MrTyDi | BASE | noise:lower ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.792, 'Recall@1': 0.792, 'MRR@1': 0.792, 'MAP@1': 0.792, 'nDCG@1': 0.792, 'Recall@5': 0.9166, 'MRR@5': 0.8454, 'MAP@5': 0.8266, 'nDCG@5': 0.8505, 'Recall@10': 0.9407, 'MRR@10': 0.8487, 'MAP@10': 0.8317, 'nDCG@10': 0.8609}\n  drop vs clean (lower): {'ΔMRR@1': -0.01608, 'ΔMRR@5': -0.016667, 'ΔMRR@10': -0.016637}\n\n=== MrTyDi | FT_INST | plain ===\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"[cache] docs torch.Size([20000, 768]) saved -> /kaggle/working/robust_eval_cache/docs__mrtydi__inst__363db7cc__L256.pt\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.7357, 'Recall@1': 0.7357, 'MRR@1': 0.7357, 'MAP@1': 0.7357, 'nDCG@1': 0.7357, 'Recall@5': 0.8975, 'MRR@5': 0.803, 'MAP@5': 0.7828, 'nDCG@5': 0.8124, 'Recall@10': 0.9276, 'MRR@10': 0.8071, 'MAP@10': 0.7877, 'nDCG@10': 0.8233}\n\n=== MrTyDi | FT_INST | q+BANK ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.7417, 'Recall@1': 0.7417, 'MRR@1': 0.7417, 'MAP@1': 0.7417, 'nDCG@1': 0.7417, 'Recall@5': 0.8884, 'MRR@5': 0.8044, 'MAP@5': 0.7846, 'nDCG@5': 0.8112, 'Recall@10': 0.9236, 'MRR@10': 0.8091, 'MAP@10': 0.7907, 'nDCG@10': 0.8246}\n\n--- MrTyDi p-MRR (BANK − plain) :: FT_INST ---\n{'pMRR@1': 0.00603, 'pMRR@5': 0.001323, 'pMRR@10': 0.002024}\n\n--- Prompt ablation :: FT_INST ---\n\n=== MrTyDi | FT_INST | prompt:plain ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.7357, 'Recall@1': 0.7357, 'MRR@1': 0.7357, 'MAP@1': 0.7357, 'nDCG@1': 0.7357, 'Recall@5': 0.8975, 'MRR@5': 0.803, 'MAP@5': 0.7828, 'nDCG@5': 0.8124, 'Recall@10': 0.9276, 'MRR@10': 0.8071, 'MAP@10': 0.7877, 'nDCG@10': 0.8233}\n\n=== MrTyDi | FT_INST | prompt:minimal_ru ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.7407, 'Recall@1': 0.7407, 'MRR@1': 0.7407, 'MAP@1': 0.7407, 'nDCG@1': 0.7407, 'Recall@5': 0.8945, 'MRR@5': 0.8073, 'MAP@5': 0.7865, 'nDCG@5': 0.8146, 'Recall@10': 0.9256, 'MRR@10': 0.8116, 'MAP@10': 0.7921, 'nDCG@10': 0.8266}\n\n=== MrTyDi | FT_INST | prompt:qa_ru ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.7397, 'Recall@1': 0.7397, 'MRR@1': 0.7397, 'MAP@1': 0.7397, 'nDCG@1': 0.7397, 'Recall@5': 0.8955, 'MRR@5': 0.8068, 'MAP@5': 0.7864, 'nDCG@5': 0.8145, 'Recall@10': 0.9246, 'MRR@10': 0.8108, 'MAP@10': 0.7917, 'nDCG@10': 0.826}\n\n=== MrTyDi | FT_INST | prompt:search_ru ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.7427, 'Recall@1': 0.7427, 'MRR@1': 0.7427, 'MAP@1': 0.7427, 'nDCG@1': 0.7427, 'Recall@5': 0.8955, 'MRR@5': 0.8086, 'MAP@5': 0.7885, 'nDCG@5': 0.8163, 'Recall@10': 0.9246, 'MRR@10': 0.8124, 'MAP@10': 0.7935, 'nDCG@10': 0.8274}\n\n=== MrTyDi | FT_INST | prompt:mfoll_ru ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.7497, 'Recall@1': 0.7497, 'MRR@1': 0.7497, 'MAP@1': 0.7497, 'nDCG@1': 0.7497, 'Recall@5': 0.8945, 'MRR@5': 0.8129, 'MAP@5': 0.7921, 'nDCG@5': 0.8188, 'Recall@10': 0.9256, 'MRR@10': 0.8168, 'MAP@10': 0.7977, 'nDCG@10': 0.8308}\n\n--- Paraphrase robustness (5x) :: FT_INST ---\n\n=== MrTyDi | FT_INST | para1 ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.7347, 'Recall@1': 0.7347, 'MRR@1': 0.7347, 'MAP@1': 0.7347, 'nDCG@1': 0.7347, 'Recall@5': 0.8894, 'MRR@5': 0.802, 'MAP@5': 0.7818, 'nDCG@5': 0.81, 'Recall@10': 0.9236, 'MRR@10': 0.8065, 'MAP@10': 0.7871, 'nDCG@10': 0.8221}\n\n=== MrTyDi | FT_INST | para2 ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.7377, 'Recall@1': 0.7377, 'MRR@1': 0.7377, 'MAP@1': 0.7377, 'nDCG@1': 0.7377, 'Recall@5': 0.8985, 'MRR@5': 0.8059, 'MAP@5': 0.7864, 'nDCG@5': 0.8152, 'Recall@10': 0.9246, 'MRR@10': 0.8095, 'MAP@10': 0.7911, 'nDCG@10': 0.8253}\n\n=== MrTyDi | FT_INST | para3 ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.7357, 'Recall@1': 0.7357, 'MRR@1': 0.7357, 'MAP@1': 0.7357, 'nDCG@1': 0.7357, 'Recall@5': 0.8935, 'MRR@5': 0.8033, 'MAP@5': 0.7827, 'nDCG@5': 0.8114, 'Recall@10': 0.9256, 'MRR@10': 0.8077, 'MAP@10': 0.7882, 'nDCG@10': 0.8235}\n\n=== MrTyDi | FT_INST | para4 ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.7417, 'Recall@1': 0.7417, 'MRR@1': 0.7417, 'MAP@1': 0.7417, 'nDCG@1': 0.7417, 'Recall@5': 0.8955, 'MRR@5': 0.8083, 'MAP@5': 0.7886, 'nDCG@5': 0.8163, 'Recall@10': 0.9296, 'MRR@10': 0.8129, 'MAP@10': 0.7944, 'nDCG@10': 0.8291}\n\n=== MrTyDi | FT_INST | para5 ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.7447, 'Recall@1': 0.7447, 'MRR@1': 0.7447, 'MAP@1': 0.7447, 'nDCG@1': 0.7447, 'Recall@5': 0.8985, 'MRR@5': 0.8099, 'MAP@5': 0.79, 'nDCG@5': 0.8181, 'Recall@10': 0.9226, 'MRR@10': 0.8132, 'MAP@10': 0.7945, 'nDCG@10': 0.8276}\nParaphrase mean/std: {'Hit@1_mean': 0.7389, 'Hit@1_std': 0.0038, 'Recall@1_mean': 0.7389, 'Recall@1_std': 0.0038, 'MRR@1_mean': 0.7389, 'MRR@1_std': 0.0038, 'MAP@1_mean': 0.7389, 'MAP@1_std': 0.0038, 'nDCG@1_mean': 0.7389, 'nDCG@1_std': 0.0038, 'Recall@5_mean': 0.8951, 'Recall@5_std': 0.0034, 'MRR@5_mean': 0.8059, 'MRR@5_std': 0.003, 'MAP@5_mean': 0.7859, 'MAP@5_std': 0.0032, 'nDCG@5_mean': 0.8142, 'nDCG@5_std': 0.003, 'Recall@10_mean': 0.9252, 'Recall@10_std': 0.0024, 'MRR@10_mean': 0.8099, 'MRR@10_std': 0.0027, 'MAP@10_mean': 0.7911, 'MAP@10_std': 0.0031, 'nDCG@10_mean': 0.8255, 'nDCG@10_std': 0.0025}\n\n--- Noise stress :: FT_INST ---\n\n=== MrTyDi | FT_INST | noise:clean ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.7357, 'Recall@1': 0.7357, 'MRR@1': 0.7357, 'MAP@1': 0.7357, 'nDCG@1': 0.7357, 'Recall@5': 0.8975, 'MRR@5': 0.803, 'MAP@5': 0.7828, 'nDCG@5': 0.8124, 'Recall@10': 0.9276, 'MRR@10': 0.8071, 'MAP@10': 0.7877, 'nDCG@10': 0.8233}\n\n=== MrTyDi | FT_INST | noise:nopunct ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.7106, 'Recall@1': 0.7106, 'MRR@1': 0.7106, 'MAP@1': 0.7106, 'nDCG@1': 0.7106, 'Recall@5': 0.8663, 'MRR@5': 0.7727, 'MAP@5': 0.7521, 'nDCG@5': 0.782, 'Recall@10': 0.9015, 'MRR@10': 0.7775, 'MAP@10': 0.7579, 'nDCG@10': 0.7947}\n  drop vs clean (nopunct): {'ΔMRR@1': -0.025126, 'ΔMRR@5': -0.030369, 'ΔMRR@10': -0.02958}\n\n=== MrTyDi | FT_INST | noise:swap ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.6915, 'Recall@1': 0.6915, 'MRR@1': 0.6915, 'MAP@1': 0.6915, 'nDCG@1': 0.6915, 'Recall@5': 0.8613, 'MRR@5': 0.7583, 'MAP@5': 0.7382, 'nDCG@5': 0.77, 'Recall@10': 0.8915, 'MRR@10': 0.7625, 'MAP@10': 0.7437, 'nDCG@10': 0.7818}\n  drop vs clean (swap): {'ΔMRR@1': -0.044221, 'ΔMRR@5': -0.044791, 'ΔMRR@10': -0.044536}\n\n=== MrTyDi | FT_INST | noise:lower ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.6935, 'Recall@1': 0.6935, 'MRR@1': 0.6935, 'MAP@1': 0.6935, 'nDCG@1': 0.6935, 'Recall@5': 0.8603, 'MRR@5': 0.7622, 'MAP@5': 0.7426, 'nDCG@5': 0.7732, 'Recall@10': 0.9015, 'MRR@10': 0.7678, 'MAP@10': 0.7491, 'nDCG@10': 0.7877}\n  drop vs clean (lower): {'ΔMRR@1': -0.042211, 'ΔMRR@5': -0.040888, 'ΔMRR@10': -0.039269}\n\n=== MrTyDi | FT_QUERY | plain ===\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"[cache] docs torch.Size([20000, 768]) saved -> /kaggle/working/robust_eval_cache/docs__mrtydi__qonly__363db7cc__L256.pt\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.7869, 'Recall@1': 0.7869, 'MRR@1': 0.7869, 'MAP@1': 0.7869, 'nDCG@1': 0.7869, 'Recall@5': 0.9075, 'MRR@5': 0.8376, 'MAP@5': 0.82, 'nDCG@5': 0.8429, 'Recall@10': 0.9367, 'MRR@10': 0.8417, 'MAP@10': 0.8245, 'nDCG@10': 0.853}\n\n=== MrTyDi | FT_QUERY | q+BANK ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.7789, 'Recall@1': 0.7789, 'MRR@1': 0.7789, 'MAP@1': 0.7789, 'nDCG@1': 0.7789, 'Recall@5': 0.9055, 'MRR@5': 0.8318, 'MAP@5': 0.8129, 'nDCG@5': 0.8374, 'Recall@10': 0.9317, 'MRR@10': 0.8356, 'MAP@10': 0.8173, 'nDCG@10': 0.847}\n\n--- MrTyDi p-MRR (BANK − plain) :: FT_QUERY ---\n{'pMRR@1': -0.00804, 'pMRR@5': -0.005829, 'pMRR@10': -0.006134}\n\n--- Prompt ablation :: FT_QUERY ---\n\n=== MrTyDi | FT_QUERY | prompt:plain ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.7869, 'Recall@1': 0.7869, 'MRR@1': 0.7869, 'MAP@1': 0.7869, 'nDCG@1': 0.7869, 'Recall@5': 0.9075, 'MRR@5': 0.8376, 'MAP@5': 0.82, 'nDCG@5': 0.8429, 'Recall@10': 0.9367, 'MRR@10': 0.8417, 'MAP@10': 0.8245, 'nDCG@10': 0.853}\n\n=== MrTyDi | FT_QUERY | prompt:minimal_ru ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.7779, 'Recall@1': 0.7779, 'MRR@1': 0.7779, 'MAP@1': 0.7779, 'nDCG@1': 0.7779, 'Recall@5': 0.9085, 'MRR@5': 0.8329, 'MAP@5': 0.8158, 'nDCG@5': 0.8402, 'Recall@10': 0.9347, 'MRR@10': 0.8366, 'MAP@10': 0.82, 'nDCG@10': 0.8493}\n\n=== MrTyDi | FT_QUERY | prompt:qa_ru ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.7799, 'Recall@1': 0.7799, 'MRR@1': 0.7799, 'MAP@1': 0.7799, 'nDCG@1': 0.7799, 'Recall@5': 0.9106, 'MRR@5': 0.8345, 'MAP@5': 0.8157, 'nDCG@5': 0.8407, 'Recall@10': 0.9337, 'MRR@10': 0.8377, 'MAP@10': 0.8195, 'nDCG@10': 0.849}\n\n=== MrTyDi | FT_QUERY | prompt:search_ru ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.7809, 'Recall@1': 0.7809, 'MRR@1': 0.7809, 'MAP@1': 0.7809, 'nDCG@1': 0.7809, 'Recall@5': 0.9116, 'MRR@5': 0.8348, 'MAP@5': 0.8168, 'nDCG@5': 0.8418, 'Recall@10': 0.9337, 'MRR@10': 0.838, 'MAP@10': 0.8207, 'nDCG@10': 0.8501}\n\n=== MrTyDi | FT_QUERY | prompt:mfoll_ru ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.7819, 'Recall@1': 0.7819, 'MRR@1': 0.7819, 'MAP@1': 0.7819, 'nDCG@1': 0.7819, 'Recall@5': 0.9075, 'MRR@5': 0.8352, 'MAP@5': 0.8169, 'nDCG@5': 0.8409, 'Recall@10': 0.9327, 'MRR@10': 0.8387, 'MAP@10': 0.8211, 'nDCG@10': 0.85}\n\n--- Paraphrase robustness (5x) :: FT_QUERY ---\n\n=== MrTyDi | FT_QUERY | para1 ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.7779, 'Recall@1': 0.7779, 'MRR@1': 0.7779, 'MAP@1': 0.7779, 'nDCG@1': 0.7779, 'Recall@5': 0.9085, 'MRR@5': 0.8328, 'MAP@5': 0.8157, 'nDCG@5': 0.8399, 'Recall@10': 0.9367, 'MRR@10': 0.8367, 'MAP@10': 0.8203, 'nDCG@10': 0.8501}\n\n=== MrTyDi | FT_QUERY | para2 ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.7849, 'Recall@1': 0.7849, 'MRR@1': 0.7849, 'MAP@1': 0.7849, 'nDCG@1': 0.7849, 'Recall@5': 0.9126, 'MRR@5': 0.8381, 'MAP@5': 0.8201, 'nDCG@5': 0.8443, 'Recall@10': 0.9357, 'MRR@10': 0.8412, 'MAP@10': 0.824, 'nDCG@10': 0.8529}\n\n=== MrTyDi | FT_QUERY | para3 ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.7799, 'Recall@1': 0.7799, 'MRR@1': 0.7799, 'MAP@1': 0.7799, 'nDCG@1': 0.7799, 'Recall@5': 0.9126, 'MRR@5': 0.835, 'MAP@5': 0.8172, 'nDCG@5': 0.8423, 'Recall@10': 0.9367, 'MRR@10': 0.8384, 'MAP@10': 0.8211, 'nDCG@10': 0.8508}\n\n=== MrTyDi | FT_QUERY | para4 ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.7819, 'Recall@1': 0.7819, 'MRR@1': 0.7819, 'MAP@1': 0.7819, 'nDCG@1': 0.7819, 'Recall@5': 0.9085, 'MRR@5': 0.8347, 'MAP@5': 0.817, 'nDCG@5': 0.8413, 'Recall@10': 0.9357, 'MRR@10': 0.8386, 'MAP@10': 0.8211, 'nDCG@10': 0.8505}\n\n=== MrTyDi | FT_QUERY | para5 ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.7749, 'Recall@1': 0.7749, 'MRR@1': 0.7749, 'MAP@1': 0.7749, 'nDCG@1': 0.7749, 'Recall@5': 0.9106, 'MRR@5': 0.8309, 'MAP@5': 0.813, 'nDCG@5': 0.8385, 'Recall@10': 0.9337, 'MRR@10': 0.8343, 'MAP@10': 0.817, 'nDCG@10': 0.8471}\nParaphrase mean/std: {'Hit@1_mean': 0.7799, 'Hit@1_std': 0.0034, 'Recall@1_mean': 0.7799, 'Recall@1_std': 0.0034, 'MRR@1_mean': 0.7799, 'MRR@1_std': 0.0034, 'MAP@1_mean': 0.7799, 'MAP@1_std': 0.0034, 'nDCG@1_mean': 0.7799, 'nDCG@1_std': 0.0034, 'Recall@5_mean': 0.9106, 'Recall@5_std': 0.0018, 'MRR@5_mean': 0.8343, 'MRR@5_std': 0.0024, 'MAP@5_mean': 0.8166, 'MAP@5_std': 0.0023, 'nDCG@5_mean': 0.8413, 'nDCG@5_std': 0.002, 'Recall@10_mean': 0.9357, 'Recall@10_std': 0.0011, 'MRR@10_mean': 0.8378, 'MRR@10_std': 0.0023, 'MAP@10_mean': 0.8207, 'MAP@10_std': 0.0022, 'nDCG@10_mean': 0.8503, 'nDCG@10_std': 0.0019}\n\n--- Noise stress :: FT_QUERY ---\n\n=== MrTyDi | FT_QUERY | noise:clean ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.7869, 'Recall@1': 0.7869, 'MRR@1': 0.7869, 'MAP@1': 0.7869, 'nDCG@1': 0.7869, 'Recall@5': 0.9075, 'MRR@5': 0.8376, 'MAP@5': 0.82, 'nDCG@5': 0.8429, 'Recall@10': 0.9367, 'MRR@10': 0.8417, 'MAP@10': 0.8245, 'nDCG@10': 0.853}\n\n=== MrTyDi | FT_QUERY | noise:nopunct ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.7588, 'Recall@1': 0.7588, 'MRR@1': 0.7588, 'MAP@1': 0.7588, 'nDCG@1': 0.7588, 'Recall@5': 0.8894, 'MRR@5': 0.8137, 'MAP@5': 0.7957, 'nDCG@5': 0.8202, 'Recall@10': 0.9186, 'MRR@10': 0.8177, 'MAP@10': 0.8006, 'nDCG@10': 0.8308}\n  drop vs clean (nopunct): {'ΔMRR@1': -0.028141, 'ΔMRR@5': -0.02397, 'ΔMRR@10': -0.024057}\n\n=== MrTyDi | FT_QUERY | noise:swap ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.7538, 'Recall@1': 0.7538, 'MRR@1': 0.7538, 'MAP@1': 0.7538, 'nDCG@1': 0.7538, 'Recall@5': 0.8864, 'MRR@5': 0.8098, 'MAP@5': 0.7929, 'nDCG@5': 0.8175, 'Recall@10': 0.9146, 'MRR@10': 0.8136, 'MAP@10': 0.7973, 'nDCG@10': 0.8276}\n  drop vs clean (swap): {'ΔMRR@1': -0.033166, 'ΔMRR@5': -0.027772, 'ΔMRR@10': -0.028117}\n\n=== MrTyDi | FT_QUERY | noise:lower ===\n[cache] docs torch.Size([20000, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 995, 'Hit@1': 0.7648, 'Recall@1': 0.7648, 'MRR@1': 0.7648, 'MAP@1': 0.7648, 'nDCG@1': 0.7648, 'Recall@5': 0.8935, 'MRR@5': 0.8178, 'MAP@5': 0.7986, 'nDCG@5': 0.8233, 'Recall@10': 0.9146, 'MRR@10': 0.8206, 'MAP@10': 0.8023, 'nDCG@10': 0.8315}\n  drop vs clean (lower): {'ΔMRR@1': -0.022111, 'ΔMRR@5': -0.019816, 'ΔMRR@10': -0.021089}\n\n=== Mr.TyDi SUMMARY (plain only) ===\nBASE -> {'N': 995, 'Hit@1': 0.808, 'Recall@1': 0.808, 'MRR@1': 0.808, 'MAP@1': 0.808, 'nDCG@1': 0.808, 'Recall@5': 0.9357, 'MRR@5': 0.862, 'MAP@5': 0.8446, 'nDCG@5': 0.8686, 'Recall@10': 0.9588, 'MRR@10': 0.8653, 'MAP@10': 0.8494, 'nDCG@10': 0.8782}\nFT_INST -> {'N': 995, 'Hit@1': 0.7357, 'Recall@1': 0.7357, 'MRR@1': 0.7357, 'MAP@1': 0.7357, 'nDCG@1': 0.7357, 'Recall@5': 0.8975, 'MRR@5': 0.803, 'MAP@5': 0.7828, 'nDCG@5': 0.8124, 'Recall@10': 0.9276, 'MRR@10': 0.8071, 'MAP@10': 0.7877, 'nDCG@10': 0.8233}\nFT_QUERY -> {'N': 995, 'Hit@1': 0.7869, 'Recall@1': 0.7869, 'MRR@1': 0.7869, 'MAP@1': 0.7869, 'nDCG@1': 0.7869, 'Recall@5': 0.9075, 'MRR@5': 0.8376, 'MAP@5': 0.82, 'nDCG@5': 0.8429, 'Recall@10': 0.9367, 'MRR@10': 0.8417, 'MAP@10': 0.8245, 'nDCG@10': 0.853}\n\n========== mFollowIR-RU ==========\n\n=== mFollowIR | BASE | plain ===\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"[cache] docs torch.Size([39326, 768]) saved -> /kaggle/working/robust_eval_cache/docs__mfollowir__base__d3fc34cd__L256.pt\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.4872, 'Recall@1': 0.4872, 'MRR@1': 0.4872, 'MAP@1': 0.4872, 'nDCG@1': 0.4872, 'Recall@5': 0.7692, 'MRR@5': 0.6034, 'MAP@5': 0.3404, 'nDCG@5': 0.4301, 'Recall@10': 0.8462, 'MRR@10': 0.6123, 'MAP@10': 0.2679, 'nDCG@10': 0.3859}\n\n=== mFollowIR | BASE | q+BANK ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.4359, 'Recall@1': 0.4359, 'MRR@1': 0.4359, 'MAP@1': 0.4359, 'nDCG@1': 0.4359, 'Recall@5': 0.6667, 'MRR@5': 0.5286, 'MAP@5': 0.3332, 'nDCG@5': 0.4091, 'Recall@10': 0.7949, 'MRR@10': 0.5458, 'MAP@10': 0.2616, 'nDCG@10': 0.368}\n\n--- mFollowIR p-MRR (BANK − plain) :: BASE ---\n{'pMRR@1': -0.051282, 'pMRR@5': -0.074786, 'pMRR@10': -0.066494}\n\n--- Prompt ablation :: BASE ---\n\n=== mFollowIR | BASE | prompt:plain ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.4872, 'Recall@1': 0.4872, 'MRR@1': 0.4872, 'MAP@1': 0.4872, 'nDCG@1': 0.4872, 'Recall@5': 0.7692, 'MRR@5': 0.6034, 'MAP@5': 0.3404, 'nDCG@5': 0.4301, 'Recall@10': 0.8462, 'MRR@10': 0.6123, 'MAP@10': 0.2679, 'nDCG@10': 0.3859}\n\n=== mFollowIR | BASE | prompt:minimal_ru ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.4872, 'Recall@1': 0.4872, 'MRR@1': 0.4872, 'MAP@1': 0.4872, 'nDCG@1': 0.4872, 'Recall@5': 0.7179, 'MRR@5': 0.5726, 'MAP@5': 0.3323, 'nDCG@5': 0.4189, 'Recall@10': 0.7692, 'MRR@10': 0.5784, 'MAP@10': 0.2645, 'nDCG@10': 0.373}\n\n=== mFollowIR | BASE | prompt:qa_ru ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.5128, 'Recall@1': 0.5128, 'MRR@1': 0.5128, 'MAP@1': 0.5128, 'nDCG@1': 0.5128, 'Recall@5': 0.7436, 'MRR@5': 0.6034, 'MAP@5': 0.3258, 'nDCG@5': 0.4148, 'Recall@10': 0.7949, 'MRR@10': 0.6105, 'MAP@10': 0.2552, 'nDCG@10': 0.3701}\n\n=== mFollowIR | BASE | prompt:search_ru ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.4103, 'Recall@1': 0.4103, 'MRR@1': 0.4103, 'MAP@1': 0.4103, 'nDCG@1': 0.4103, 'Recall@5': 0.6923, 'MRR@5': 0.5081, 'MAP@5': 0.2916, 'nDCG@5': 0.3739, 'Recall@10': 0.7949, 'MRR@10': 0.521, 'MAP@10': 0.2265, 'nDCG@10': 0.3377}\n\n=== mFollowIR | BASE | prompt:mfoll_ru ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.5128, 'Recall@1': 0.5128, 'MRR@1': 0.5128, 'MAP@1': 0.5128, 'nDCG@1': 0.5128, 'Recall@5': 0.7436, 'MRR@5': 0.5962, 'MAP@5': 0.3502, 'nDCG@5': 0.435, 'Recall@10': 0.7692, 'MRR@10': 0.6004, 'MAP@10': 0.2745, 'nDCG@10': 0.3854}\n\n--- Paraphrase robustness (5x) :: BASE ---\n\n=== mFollowIR | BASE | para1 ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.4615, 'Recall@1': 0.4615, 'MRR@1': 0.4615, 'MAP@1': 0.4615, 'nDCG@1': 0.4615, 'Recall@5': 0.7436, 'MRR@5': 0.5744, 'MAP@5': 0.3324, 'nDCG@5': 0.4227, 'Recall@10': 0.7692, 'MRR@10': 0.5769, 'MAP@10': 0.2622, 'nDCG@10': 0.3731}\n\n=== mFollowIR | BASE | para2 ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.4359, 'Recall@1': 0.4359, 'MRR@1': 0.4359, 'MAP@1': 0.4359, 'nDCG@1': 0.4359, 'Recall@5': 0.7179, 'MRR@5': 0.5436, 'MAP@5': 0.3116, 'nDCG@5': 0.3976, 'Recall@10': 0.7949, 'MRR@10': 0.5541, 'MAP@10': 0.2509, 'nDCG@10': 0.3649}\n\n=== mFollowIR | BASE | para3 ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.4359, 'Recall@1': 0.4359, 'MRR@1': 0.4359, 'MAP@1': 0.4359, 'nDCG@1': 0.4359, 'Recall@5': 0.6923, 'MRR@5': 0.5329, 'MAP@5': 0.2988, 'nDCG@5': 0.3825, 'Recall@10': 0.7436, 'MRR@10': 0.5408, 'MAP@10': 0.2375, 'nDCG@10': 0.3517}\n\n=== mFollowIR | BASE | para4 ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.4359, 'Recall@1': 0.4359, 'MRR@1': 0.4359, 'MAP@1': 0.4359, 'nDCG@1': 0.4359, 'Recall@5': 0.6667, 'MRR@5': 0.5188, 'MAP@5': 0.2813, 'nDCG@5': 0.3638, 'Recall@10': 0.7436, 'MRR@10': 0.5285, 'MAP@10': 0.2274, 'nDCG@10': 0.3347}\n\n=== mFollowIR | BASE | para5 ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.4615, 'Recall@1': 0.4615, 'MRR@1': 0.4615, 'MAP@1': 0.4615, 'nDCG@1': 0.4615, 'Recall@5': 0.6667, 'MRR@5': 0.5513, 'MAP@5': 0.3064, 'nDCG@5': 0.3895, 'Recall@10': 0.7949, 'MRR@10': 0.5694, 'MAP@10': 0.2459, 'nDCG@10': 0.3587}\nParaphrase mean/std: {'Hit@1_mean': 0.4462, 'Hit@1_std': 0.0126, 'Recall@1_mean': 0.4462, 'Recall@1_std': 0.0126, 'MRR@1_mean': 0.4462, 'MRR@1_std': 0.0126, 'MAP@1_mean': 0.4462, 'MAP@1_std': 0.0126, 'nDCG@1_mean': 0.4462, 'nDCG@1_std': 0.0126, 'Recall@5_mean': 0.6974, 'Recall@5_std': 0.0299, 'MRR@5_mean': 0.5442, 'MRR@5_std': 0.0186, 'MAP@5_mean': 0.3061, 'MAP@5_std': 0.0167, 'nDCG@5_mean': 0.3912, 'nDCG@5_std': 0.0193, 'Recall@10_mean': 0.7692, 'Recall@10_std': 0.0229, 'MRR@10_mean': 0.554, 'MRR@10_std': 0.0178, 'MAP@10_mean': 0.2448, 'MAP@10_std': 0.0118, 'nDCG@10_mean': 0.3566, 'nDCG@10_std': 0.013}\n\n--- Noise stress :: BASE ---\n\n=== mFollowIR | BASE | noise:clean ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.4872, 'Recall@1': 0.4872, 'MRR@1': 0.4872, 'MAP@1': 0.4872, 'nDCG@1': 0.4872, 'Recall@5': 0.7692, 'MRR@5': 0.6034, 'MAP@5': 0.3404, 'nDCG@5': 0.4301, 'Recall@10': 0.8462, 'MRR@10': 0.6123, 'MAP@10': 0.2679, 'nDCG@10': 0.3859}\n\n=== mFollowIR | BASE | noise:nopunct ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.5385, 'Recall@1': 0.5385, 'MRR@1': 0.5385, 'MAP@1': 0.5385, 'nDCG@1': 0.5385, 'Recall@5': 0.7692, 'MRR@5': 0.6303, 'MAP@5': 0.3649, 'nDCG@5': 0.453, 'Recall@10': 0.8462, 'MRR@10': 0.6403, 'MAP@10': 0.2892, 'nDCG@10': 0.412}\n  drop vs clean (nopunct): {'ΔMRR@1': 0.051282, 'ΔMRR@5': 0.026923, 'ΔMRR@10': 0.027991}\n\n=== mFollowIR | BASE | noise:swap ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.5128, 'Recall@1': 0.5128, 'MRR@1': 0.5128, 'MAP@1': 0.5128, 'nDCG@1': 0.5128, 'Recall@5': 0.7692, 'MRR@5': 0.6197, 'MAP@5': 0.3567, 'nDCG@5': 0.446, 'Recall@10': 0.8462, 'MRR@10': 0.6303, 'MAP@10': 0.2895, 'nDCG@10': 0.4104}\n  drop vs clean (swap): {'ΔMRR@1': 0.025641, 'ΔMRR@5': 0.016239, 'ΔMRR@10': 0.01802}\n\n=== mFollowIR | BASE | noise:lower ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.4359, 'Recall@1': 0.4359, 'MRR@1': 0.4359, 'MAP@1': 0.4359, 'nDCG@1': 0.4359, 'Recall@5': 0.6923, 'MRR@5': 0.5329, 'MAP@5': 0.2993, 'nDCG@5': 0.3803, 'Recall@10': 0.7692, 'MRR@10': 0.5451, 'MAP@10': 0.2433, 'nDCG@10': 0.3514}\n  drop vs clean (lower): {'ΔMRR@1': -0.051282, 'ΔMRR@5': -0.070513, 'ΔMRR@10': -0.067206}\n\n=== mFollowIR | FT_INST | plain ===\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"[cache] docs torch.Size([39326, 768]) saved -> /kaggle/working/robust_eval_cache/docs__mfollowir__inst__d3fc34cd__L256.pt\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.5641, 'Recall@1': 0.5641, 'MRR@1': 0.5641, 'MAP@1': 0.5641, 'nDCG@1': 0.5641, 'Recall@5': 0.7436, 'MRR@5': 0.6214, 'MAP@5': 0.3814, 'nDCG@5': 0.4574, 'Recall@10': 0.8205, 'MRR@10': 0.6322, 'MAP@10': 0.3048, 'nDCG@10': 0.416}\n\n=== mFollowIR | FT_INST | q+BANK ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.5128, 'Recall@1': 0.5128, 'MRR@1': 0.5128, 'MAP@1': 0.5128, 'nDCG@1': 0.5128, 'Recall@5': 0.7179, 'MRR@5': 0.5885, 'MAP@5': 0.3631, 'nDCG@5': 0.4381, 'Recall@10': 0.7436, 'MRR@10': 0.5927, 'MAP@10': 0.2949, 'nDCG@10': 0.3997}\n\n--- mFollowIR p-MRR (BANK − plain) :: FT_INST ---\n{'pMRR@1': -0.051282, 'pMRR@5': -0.032906, 'pMRR@10': -0.039418}\n\n--- Prompt ablation :: FT_INST ---\n\n=== mFollowIR | FT_INST | prompt:plain ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.5641, 'Recall@1': 0.5641, 'MRR@1': 0.5641, 'MAP@1': 0.5641, 'nDCG@1': 0.5641, 'Recall@5': 0.7436, 'MRR@5': 0.6214, 'MAP@5': 0.3814, 'nDCG@5': 0.4574, 'Recall@10': 0.8205, 'MRR@10': 0.6322, 'MAP@10': 0.3048, 'nDCG@10': 0.416}\n\n=== mFollowIR | FT_INST | prompt:minimal_ru ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.5385, 'Recall@1': 0.5385, 'MRR@1': 0.5385, 'MAP@1': 0.5385, 'nDCG@1': 0.5385, 'Recall@5': 0.7692, 'MRR@5': 0.6179, 'MAP@5': 0.3643, 'nDCG@5': 0.4461, 'Recall@10': 0.8205, 'MRR@10': 0.6242, 'MAP@10': 0.3044, 'nDCG@10': 0.4164}\n\n=== mFollowIR | FT_INST | prompt:qa_ru ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.5385, 'Recall@1': 0.5385, 'MRR@1': 0.5385, 'MAP@1': 0.5385, 'nDCG@1': 0.5385, 'Recall@5': 0.7436, 'MRR@5': 0.6077, 'MAP@5': 0.3564, 'nDCG@5': 0.4373, 'Recall@10': 0.7949, 'MRR@10': 0.6142, 'MAP@10': 0.3005, 'nDCG@10': 0.4116}\n\n=== mFollowIR | FT_INST | prompt:search_ru ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.5128, 'Recall@1': 0.5128, 'MRR@1': 0.5128, 'MAP@1': 0.5128, 'nDCG@1': 0.5128, 'Recall@5': 0.7692, 'MRR@5': 0.5979, 'MAP@5': 0.3678, 'nDCG@5': 0.4459, 'Recall@10': 0.8205, 'MRR@10': 0.6039, 'MAP@10': 0.2974, 'nDCG@10': 0.4073}\n\n=== mFollowIR | FT_INST | prompt:mfoll_ru ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.5128, 'Recall@1': 0.5128, 'MRR@1': 0.5128, 'MAP@1': 0.5128, 'nDCG@1': 0.5128, 'Recall@5': 0.7692, 'MRR@5': 0.6107, 'MAP@5': 0.3766, 'nDCG@5': 0.4574, 'Recall@10': 0.7949, 'MRR@10': 0.6132, 'MAP@10': 0.3053, 'nDCG@10': 0.4159}\n\n--- Paraphrase robustness (5x) :: FT_INST ---\n\n=== mFollowIR | FT_INST | para1 ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.5385, 'Recall@1': 0.5385, 'MRR@1': 0.5385, 'MAP@1': 0.5385, 'nDCG@1': 0.5385, 'Recall@5': 0.7436, 'MRR@5': 0.6128, 'MAP@5': 0.3694, 'nDCG@5': 0.4482, 'Recall@10': 0.8205, 'MRR@10': 0.6225, 'MAP@10': 0.3025, 'nDCG@10': 0.4129}\n\n=== mFollowIR | FT_INST | para2 ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.5385, 'Recall@1': 0.5385, 'MRR@1': 0.5385, 'MAP@1': 0.5385, 'nDCG@1': 0.5385, 'Recall@5': 0.7436, 'MRR@5': 0.6141, 'MAP@5': 0.3629, 'nDCG@5': 0.4427, 'Recall@10': 0.7949, 'MRR@10': 0.6202, 'MAP@10': 0.3038, 'nDCG@10': 0.416}\n\n=== mFollowIR | FT_INST | para3 ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.4872, 'Recall@1': 0.4872, 'MRR@1': 0.4872, 'MAP@1': 0.4872, 'nDCG@1': 0.4872, 'Recall@5': 0.7436, 'MRR@5': 0.5829, 'MAP@5': 0.3402, 'nDCG@5': 0.4205, 'Recall@10': 0.8205, 'MRR@10': 0.594, 'MAP@10': 0.2837, 'nDCG@10': 0.3943}\n\n=== mFollowIR | FT_INST | para4 ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.5128, 'Recall@1': 0.5128, 'MRR@1': 0.5128, 'MAP@1': 0.5128, 'nDCG@1': 0.5128, 'Recall@5': 0.7436, 'MRR@5': 0.6, 'MAP@5': 0.3632, 'nDCG@5': 0.4428, 'Recall@10': 0.7949, 'MRR@10': 0.6061, 'MAP@10': 0.3016, 'nDCG@10': 0.4097}\n\n=== mFollowIR | FT_INST | para5 ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.5385, 'Recall@1': 0.5385, 'MRR@1': 0.5385, 'MAP@1': 0.5385, 'nDCG@1': 0.5385, 'Recall@5': 0.7436, 'MRR@5': 0.6132, 'MAP@5': 0.3695, 'nDCG@5': 0.4493, 'Recall@10': 0.7949, 'MRR@10': 0.6198, 'MAP@10': 0.3031, 'nDCG@10': 0.4138}\nParaphrase mean/std: {'Hit@1_mean': 0.5231, 'Hit@1_std': 0.0205, 'Recall@1_mean': 0.5231, 'Recall@1_std': 0.0205, 'MRR@1_mean': 0.5231, 'MRR@1_std': 0.0205, 'MAP@1_mean': 0.5231, 'MAP@1_std': 0.0205, 'nDCG@1_mean': 0.5231, 'nDCG@1_std': 0.0205, 'Recall@5_mean': 0.7436, 'Recall@5_std': 0.0, 'MRR@5_mean': 0.6046, 'MRR@5_std': 0.012, 'MAP@5_mean': 0.3611, 'MAP@5_std': 0.0108, 'nDCG@5_mean': 0.4407, 'nDCG@5_std': 0.0105, 'Recall@10_mean': 0.8051, 'Recall@10_std': 0.0126, 'MRR@10_mean': 0.6125, 'MRR@10_std': 0.0109, 'MAP@10_mean': 0.2989, 'MAP@10_std': 0.0077, 'nDCG@10_mean': 0.4093, 'nDCG@10_std': 0.0078}\n\n--- Noise stress :: FT_INST ---\n\n=== mFollowIR | FT_INST | noise:clean ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.5641, 'Recall@1': 0.5641, 'MRR@1': 0.5641, 'MAP@1': 0.5641, 'nDCG@1': 0.5641, 'Recall@5': 0.7436, 'MRR@5': 0.6214, 'MAP@5': 0.3814, 'nDCG@5': 0.4574, 'Recall@10': 0.8205, 'MRR@10': 0.6322, 'MAP@10': 0.3048, 'nDCG@10': 0.416}\n\n=== mFollowIR | FT_INST | noise:nopunct ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.5385, 'Recall@1': 0.5385, 'MRR@1': 0.5385, 'MAP@1': 0.5385, 'nDCG@1': 0.5385, 'Recall@5': 0.7436, 'MRR@5': 0.6171, 'MAP@5': 0.3759, 'nDCG@5': 0.4528, 'Recall@10': 0.7949, 'MRR@10': 0.6236, 'MAP@10': 0.3044, 'nDCG@10': 0.4147}\n  drop vs clean (nopunct): {'ΔMRR@1': -0.025641, 'ΔMRR@5': -0.004274, 'ΔMRR@10': -0.008547}\n\n=== mFollowIR | FT_INST | noise:swap ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.5385, 'Recall@1': 0.5385, 'MRR@1': 0.5385, 'MAP@1': 0.5385, 'nDCG@1': 0.5385, 'Recall@5': 0.7179, 'MRR@5': 0.6043, 'MAP@5': 0.3722, 'nDCG@5': 0.4462, 'Recall@10': 0.7692, 'MRR@10': 0.6111, 'MAP@10': 0.2984, 'nDCG@10': 0.4067}\n  drop vs clean (swap): {'ΔMRR@1': -0.025641, 'ΔMRR@5': -0.017094, 'ΔMRR@10': -0.021042}\n\n=== mFollowIR | FT_INST | noise:lower ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.5128, 'Recall@1': 0.5128, 'MRR@1': 0.5128, 'MAP@1': 0.5128, 'nDCG@1': 0.5128, 'Recall@5': 0.7179, 'MRR@5': 0.5885, 'MAP@5': 0.3486, 'nDCG@5': 0.4171, 'Recall@10': 0.7949, 'MRR@10': 0.6007, 'MAP@10': 0.2791, 'nDCG@10': 0.3848}\n  drop vs clean (lower): {'ΔMRR@1': -0.051282, 'ΔMRR@5': -0.032906, 'ΔMRR@10': -0.031481}\n\n=== mFollowIR | FT_QUERY | plain ===\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"[cache] docs torch.Size([39326, 768]) saved -> /kaggle/working/robust_eval_cache/docs__mfollowir__qonly__d3fc34cd__L256.pt\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.4103, 'Recall@1': 0.4103, 'MRR@1': 0.4103, 'MAP@1': 0.4103, 'nDCG@1': 0.4103, 'Recall@5': 0.7692, 'MRR@5': 0.5628, 'MAP@5': 0.3193, 'nDCG@5': 0.4102, 'Recall@10': 0.8462, 'MRR@10': 0.574, 'MAP@10': 0.2548, 'nDCG@10': 0.3703}\n\n=== mFollowIR | FT_QUERY | q+BANK ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.5128, 'Recall@1': 0.5128, 'MRR@1': 0.5128, 'MAP@1': 0.5128, 'nDCG@1': 0.5128, 'Recall@5': 0.6923, 'MRR@5': 0.5812, 'MAP@5': 0.3415, 'nDCG@5': 0.4195, 'Recall@10': 0.7436, 'MRR@10': 0.5881, 'MAP@10': 0.265, 'nDCG@10': 0.3676}\n\n--- mFollowIR p-MRR (BANK − plain) :: FT_QUERY ---\n{'pMRR@1': 0.102564, 'pMRR@5': 0.018376, 'pMRR@10': 0.014103}\n\n--- Prompt ablation :: FT_QUERY ---\n\n=== mFollowIR | FT_QUERY | prompt:plain ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.4103, 'Recall@1': 0.4103, 'MRR@1': 0.4103, 'MAP@1': 0.4103, 'nDCG@1': 0.4103, 'Recall@5': 0.7692, 'MRR@5': 0.5628, 'MAP@5': 0.3193, 'nDCG@5': 0.4102, 'Recall@10': 0.8462, 'MRR@10': 0.574, 'MAP@10': 0.2548, 'nDCG@10': 0.3703}\n\n=== mFollowIR | FT_QUERY | prompt:minimal_ru ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.4615, 'Recall@1': 0.4615, 'MRR@1': 0.4615, 'MAP@1': 0.4615, 'nDCG@1': 0.4615, 'Recall@5': 0.7436, 'MRR@5': 0.5671, 'MAP@5': 0.3202, 'nDCG@5': 0.4053, 'Recall@10': 0.7949, 'MRR@10': 0.5736, 'MAP@10': 0.2521, 'nDCG@10': 0.3614}\n\n=== mFollowIR | FT_QUERY | prompt:qa_ru ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.4615, 'Recall@1': 0.4615, 'MRR@1': 0.4615, 'MAP@1': 0.4615, 'nDCG@1': 0.4615, 'Recall@5': 0.7179, 'MRR@5': 0.562, 'MAP@5': 0.3182, 'nDCG@5': 0.4027, 'Recall@10': 0.7692, 'MRR@10': 0.5694, 'MAP@10': 0.2567, 'nDCG@10': 0.3645}\n\n=== mFollowIR | FT_QUERY | prompt:search_ru ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.4615, 'Recall@1': 0.4615, 'MRR@1': 0.4615, 'MAP@1': 0.4615, 'nDCG@1': 0.4615, 'Recall@5': 0.7436, 'MRR@5': 0.5564, 'MAP@5': 0.3018, 'nDCG@5': 0.3894, 'Recall@10': 0.7949, 'MRR@10': 0.5622, 'MAP@10': 0.2407, 'nDCG@10': 0.3503}\n\n=== mFollowIR | FT_QUERY | prompt:mfoll_ru ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.4615, 'Recall@1': 0.4615, 'MRR@1': 0.4615, 'MAP@1': 0.4615, 'nDCG@1': 0.4615, 'Recall@5': 0.7692, 'MRR@5': 0.5808, 'MAP@5': 0.3425, 'nDCG@5': 0.4265, 'Recall@10': 0.8462, 'MRR@10': 0.5896, 'MAP@10': 0.2752, 'nDCG@10': 0.3878}\n\n--- Paraphrase robustness (5x) :: FT_QUERY ---\n\n=== mFollowIR | FT_QUERY | para1 ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.4872, 'Recall@1': 0.4872, 'MRR@1': 0.4872, 'MAP@1': 0.4872, 'nDCG@1': 0.4872, 'Recall@5': 0.7436, 'MRR@5': 0.5876, 'MAP@5': 0.3393, 'nDCG@5': 0.4225, 'Recall@10': 0.8205, 'MRR@10': 0.597, 'MAP@10': 0.2684, 'nDCG@10': 0.3787}\n\n=== mFollowIR | FT_QUERY | para2 ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.4872, 'Recall@1': 0.4872, 'MRR@1': 0.4872, 'MAP@1': 0.4872, 'nDCG@1': 0.4872, 'Recall@5': 0.7692, 'MRR@5': 0.5872, 'MAP@5': 0.3179, 'nDCG@5': 0.4093, 'Recall@10': 0.8205, 'MRR@10': 0.5932, 'MAP@10': 0.2596, 'nDCG@10': 0.3762}\n\n=== mFollowIR | FT_QUERY | para3 ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.5128, 'Recall@1': 0.5128, 'MRR@1': 0.5128, 'MAP@1': 0.5128, 'nDCG@1': 0.5128, 'Recall@5': 0.6923, 'MRR@5': 0.5714, 'MAP@5': 0.2979, 'nDCG@5': 0.3859, 'Recall@10': 0.7949, 'MRR@10': 0.5855, 'MAP@10': 0.2365, 'nDCG@10': 0.3493}\n\n=== mFollowIR | FT_QUERY | para4 ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.4615, 'Recall@1': 0.4615, 'MRR@1': 0.4615, 'MAP@1': 0.4615, 'nDCG@1': 0.4615, 'Recall@5': 0.7179, 'MRR@5': 0.5641, 'MAP@5': 0.3063, 'nDCG@5': 0.3907, 'Recall@10': 0.7949, 'MRR@10': 0.5741, 'MAP@10': 0.2479, 'nDCG@10': 0.3598}\n\n=== mFollowIR | FT_QUERY | para5 ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.4872, 'Recall@1': 0.4872, 'MRR@1': 0.4872, 'MAP@1': 0.4872, 'nDCG@1': 0.4872, 'Recall@5': 0.7179, 'MRR@5': 0.5833, 'MAP@5': 0.3272, 'nDCG@5': 0.4137, 'Recall@10': 0.8205, 'MRR@10': 0.5967, 'MAP@10': 0.2643, 'nDCG@10': 0.3787}\nParaphrase mean/std: {'Hit@1_mean': 0.4872, 'Hit@1_std': 0.0162, 'Recall@1_mean': 0.4872, 'Recall@1_std': 0.0162, 'MRR@1_mean': 0.4872, 'MRR@1_std': 0.0162, 'MAP@1_mean': 0.4872, 'MAP@1_std': 0.0162, 'nDCG@1_mean': 0.4872, 'nDCG@1_std': 0.0162, 'Recall@5_mean': 0.7282, 'Recall@5_std': 0.0261, 'MRR@5_mean': 0.5787, 'MRR@5_std': 0.0094, 'MAP@5_mean': 0.3177, 'MAP@5_std': 0.0147, 'nDCG@5_mean': 0.4044, 'nDCG@5_std': 0.0139, 'Recall@10_mean': 0.8103, 'Recall@10_std': 0.0126, 'MRR@10_mean': 0.5893, 'MRR@10_std': 0.0086, 'MAP@10_mean': 0.2554, 'MAP@10_std': 0.0116, 'nDCG@10_mean': 0.3685, 'nDCG@10_std': 0.0119}\n\n--- Noise stress :: FT_QUERY ---\n\n=== mFollowIR | FT_QUERY | noise:clean ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.4103, 'Recall@1': 0.4103, 'MRR@1': 0.4103, 'MAP@1': 0.4103, 'nDCG@1': 0.4103, 'Recall@5': 0.7692, 'MRR@5': 0.5628, 'MAP@5': 0.3193, 'nDCG@5': 0.4102, 'Recall@10': 0.8462, 'MRR@10': 0.574, 'MAP@10': 0.2548, 'nDCG@10': 0.3703}\n\n=== mFollowIR | FT_QUERY | noise:nopunct ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.4615, 'Recall@1': 0.4615, 'MRR@1': 0.4615, 'MAP@1': 0.4615, 'nDCG@1': 0.4615, 'Recall@5': 0.7949, 'MRR@5': 0.6056, 'MAP@5': 0.3505, 'nDCG@5': 0.4358, 'Recall@10': 0.8205, 'MRR@10': 0.6098, 'MAP@10': 0.2759, 'nDCG@10': 0.3906}\n  drop vs clean (nopunct): {'ΔMRR@1': 0.051282, 'ΔMRR@5': 0.042735, 'ΔMRR@10': 0.035867}\n\n=== mFollowIR | FT_QUERY | noise:swap ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.4615, 'Recall@1': 0.4615, 'MRR@1': 0.4615, 'MAP@1': 0.4615, 'nDCG@1': 0.4615, 'Recall@5': 0.7949, 'MRR@5': 0.5957, 'MAP@5': 0.343, 'nDCG@5': 0.4283, 'Recall@10': 0.8462, 'MRR@10': 0.6032, 'MAP@10': 0.2645, 'nDCG@10': 0.3825}\n  drop vs clean (swap): {'ΔMRR@1': 0.051282, 'ΔMRR@5': 0.032906, 'ΔMRR@10': 0.029243}\n\n=== mFollowIR | FT_QUERY | noise:lower ===\n[cache] docs torch.Size([39326, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"{'N': 39, 'Hit@1': 0.4103, 'Recall@1': 0.4103, 'MRR@1': 0.4103, 'MAP@1': 0.4103, 'nDCG@1': 0.4103, 'Recall@5': 0.6667, 'MRR@5': 0.5235, 'MAP@5': 0.3026, 'nDCG@5': 0.3775, 'Recall@10': 0.7949, 'MRR@10': 0.5428, 'MAP@10': 0.2344, 'nDCG@10': 0.3396}\n  drop vs clean (lower): {'ΔMRR@1': 0.0, 'ΔMRR@5': -0.039316, 'ΔMRR@10': -0.031125}\n\n========== LAReQA XQuAD-R ==========\n\n--- RU→EN --- Docs=1,180 | Queries=1,187 | QrelsQ=1,187\n\n=== LAReQA RU→EN | BASE | plain ===\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"[cache] docs torch.Size([1180, 768]) saved -> /kaggle/working/robust_eval_cache/docs__lareqa_RU→EN__base__be7f5f38__L256.pt\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7666, 'Recall@1': 0.7666, 'MRR@1': 0.7666, 'MAP@1': 0.7666, 'nDCG@1': 0.7666, 'Recall@5': 0.9334, 'MRR@5': 0.8353, 'MAP@5': 0.6889, 'nDCG@5': 0.7423, 'Recall@10': 0.9646, 'MRR@10': 0.8396, 'MAP@10': 0.6877, 'nDCG@10': 0.7488}\n\n=== LAReQA RU→EN | BASE | q+BANK ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.6596, 'Recall@1': 0.6596, 'MRR@1': 0.6596, 'MAP@1': 0.6596, 'nDCG@1': 0.6596, 'Recall@5': 0.8997, 'MRR@5': 0.7598, 'MAP@5': 0.626, 'nDCG@5': 0.684, 'Recall@10': 0.9377, 'MRR@10': 0.7651, 'MAP@10': 0.6268, 'nDCG@10': 0.6929}\n\n--- LAReQA p-MRR (BANK − plain) :: RU→EN :: BASE ---\n{'pMRR@1': -0.106992, 'pMRR@5': -0.075512, 'pMRR@10': -0.07453}\n\n--- Prompt ablation :: RU→EN :: BASE ---\n\n=== LAReQA RU→EN | BASE | prompt:plain ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7666, 'Recall@1': 0.7666, 'MRR@1': 0.7666, 'MAP@1': 0.7666, 'nDCG@1': 0.7666, 'Recall@5': 0.9334, 'MRR@5': 0.8353, 'MAP@5': 0.6889, 'nDCG@5': 0.7423, 'Recall@10': 0.9646, 'MRR@10': 0.8396, 'MAP@10': 0.6877, 'nDCG@10': 0.7488}\n\n=== LAReQA RU→EN | BASE | prompt:minimal_ru ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7388, 'Recall@1': 0.7388, 'MRR@1': 0.7388, 'MAP@1': 0.7388, 'nDCG@1': 0.7388, 'Recall@5': 0.9183, 'MRR@5': 0.8133, 'MAP@5': 0.6719, 'nDCG@5': 0.7246, 'Recall@10': 0.9511, 'MRR@10': 0.8177, 'MAP@10': 0.6706, 'nDCG@10': 0.7307}\n\n=== LAReQA RU→EN | BASE | prompt:qa_ru ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.695, 'Recall@1': 0.695, 'MRR@1': 0.695, 'MAP@1': 0.695, 'nDCG@1': 0.695, 'Recall@5': 0.9166, 'MRR@5': 0.7879, 'MAP@5': 0.6511, 'nDCG@5': 0.7078, 'Recall@10': 0.9553, 'MRR@10': 0.7932, 'MAP@10': 0.6511, 'nDCG@10': 0.7161}\n\n=== LAReQA RU→EN | BASE | prompt:search_ru ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7254, 'Recall@1': 0.7254, 'MRR@1': 0.7254, 'MAP@1': 0.7254, 'nDCG@1': 0.7254, 'Recall@5': 0.9174, 'MRR@5': 0.8048, 'MAP@5': 0.6654, 'nDCG@5': 0.7193, 'Recall@10': 0.9478, 'MRR@10': 0.8088, 'MAP@10': 0.6639, 'nDCG@10': 0.7247}\n\n=== LAReQA RU→EN | BASE | prompt:mfoll_ru ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.6984, 'Recall@1': 0.6984, 'MRR@1': 0.6984, 'MAP@1': 0.6984, 'nDCG@1': 0.6984, 'Recall@5': 0.9073, 'MRR@5': 0.7851, 'MAP@5': 0.6479, 'nDCG@5': 0.7027, 'Recall@10': 0.9469, 'MRR@10': 0.7902, 'MAP@10': 0.6475, 'nDCG@10': 0.7107}\n\n--- Paraphrase robustness (5x) :: RU→EN :: BASE ---\n\n=== LAReQA RU→EN | BASE | para1 ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7136, 'Recall@1': 0.7136, 'MRR@1': 0.7136, 'MAP@1': 0.7136, 'nDCG@1': 0.7136, 'Recall@5': 0.9115, 'MRR@5': 0.7937, 'MAP@5': 0.6542, 'nDCG@5': 0.709, 'Recall@10': 0.9469, 'MRR@10': 0.7984, 'MAP@10': 0.6533, 'nDCG@10': 0.7158}\n\n=== LAReQA RU→EN | BASE | para2 ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7254, 'Recall@1': 0.7254, 'MRR@1': 0.7254, 'MAP@1': 0.7254, 'nDCG@1': 0.7254, 'Recall@5': 0.9158, 'MRR@5': 0.8055, 'MAP@5': 0.6652, 'nDCG@5': 0.7189, 'Recall@10': 0.9503, 'MRR@10': 0.81, 'MAP@10': 0.6637, 'nDCG@10': 0.7248}\n\n=== LAReQA RU→EN | BASE | para3 ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.711, 'Recall@1': 0.711, 'MRR@1': 0.711, 'MAP@1': 0.711, 'nDCG@1': 0.711, 'Recall@5': 0.9115, 'MRR@5': 0.7927, 'MAP@5': 0.6554, 'nDCG@5': 0.7103, 'Recall@10': 0.9461, 'MRR@10': 0.7971, 'MAP@10': 0.6537, 'nDCG@10': 0.7154}\n\n=== LAReQA RU→EN | BASE | para4 ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.6992, 'Recall@1': 0.6992, 'MRR@1': 0.6992, 'MAP@1': 0.6992, 'nDCG@1': 0.6992, 'Recall@5': 0.909, 'MRR@5': 0.7865, 'MAP@5': 0.6498, 'nDCG@5': 0.7054, 'Recall@10': 0.9461, 'MRR@10': 0.7916, 'MAP@10': 0.6491, 'nDCG@10': 0.7121}\n\n=== LAReQA RU→EN | BASE | para5 ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.6841, 'Recall@1': 0.6841, 'MRR@1': 0.6841, 'MAP@1': 0.6841, 'nDCG@1': 0.6841, 'Recall@5': 0.9048, 'MRR@5': 0.7745, 'MAP@5': 0.6399, 'nDCG@5': 0.6966, 'Recall@10': 0.9452, 'MRR@10': 0.7801, 'MAP@10': 0.64, 'nDCG@10': 0.7045}\nParaphrase mean/std: {'Hit@1_mean': 0.7067, 'Hit@1_std': 0.014, 'Recall@1_mean': 0.7067, 'Recall@1_std': 0.014, 'MRR@1_mean': 0.7067, 'MRR@1_std': 0.014, 'MAP@1_mean': 0.7067, 'MAP@1_std': 0.014, 'nDCG@1_mean': 0.7067, 'nDCG@1_std': 0.014, 'Recall@5_mean': 0.9105, 'Recall@5_std': 0.0036, 'MRR@5_mean': 0.7906, 'MRR@5_std': 0.0101, 'MAP@5_mean': 0.6529, 'MAP@5_std': 0.0082, 'nDCG@5_mean': 0.708, 'nDCG@5_std': 0.0073, 'Recall@10_mean': 0.9469, 'Recall@10_std': 0.0018, 'MRR@10_mean': 0.7954, 'MRR@10_std': 0.0097, 'MAP@10_mean': 0.6519, 'MAP@10_std': 0.0077, 'nDCG@10_mean': 0.7145, 'nDCG@10_std': 0.0066}\n\n--- Noise stress :: RU→EN :: BASE ---\n\n=== LAReQA RU→EN | BASE | noise:clean ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7666, 'Recall@1': 0.7666, 'MRR@1': 0.7666, 'MAP@1': 0.7666, 'nDCG@1': 0.7666, 'Recall@5': 0.9334, 'MRR@5': 0.8353, 'MAP@5': 0.6889, 'nDCG@5': 0.7423, 'Recall@10': 0.9646, 'MRR@10': 0.8396, 'MAP@10': 0.6877, 'nDCG@10': 0.7488}\n\n=== LAReQA RU→EN | BASE | noise:nopunct ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.77, 'Recall@1': 0.77, 'MRR@1': 0.77, 'MAP@1': 0.77, 'nDCG@1': 0.77, 'Recall@5': 0.9318, 'MRR@5': 0.8359, 'MAP@5': 0.6905, 'nDCG@5': 0.7431, 'Recall@10': 0.9612, 'MRR@10': 0.8399, 'MAP@10': 0.6888, 'nDCG@10': 0.7488}\n  drop vs clean (nopunct): {'ΔMRR@1': 0.00337, 'ΔMRR@5': 0.000534, 'ΔMRR@10': 0.000331}\n\n=== LAReQA RU→EN | BASE | noise:swap ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7515, 'Recall@1': 0.7515, 'MRR@1': 0.7515, 'MAP@1': 0.7515, 'nDCG@1': 0.7515, 'Recall@5': 0.925, 'MRR@5': 0.8249, 'MAP@5': 0.6797, 'nDCG@5': 0.7332, 'Recall@10': 0.9579, 'MRR@10': 0.8295, 'MAP@10': 0.6785, 'nDCG@10': 0.7401}\n  drop vs clean (swap): {'ΔMRR@1': -0.015164, 'ΔMRR@5': -0.010447, 'ΔMRR@10': -0.010083}\n\n=== LAReQA RU→EN | BASE | noise:lower ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7582, 'Recall@1': 0.7582, 'MRR@1': 0.7582, 'MAP@1': 0.7582, 'nDCG@1': 0.7582, 'Recall@5': 0.9309, 'MRR@5': 0.8296, 'MAP@5': 0.6847, 'nDCG@5': 0.7386, 'Recall@10': 0.9629, 'MRR@10': 0.834, 'MAP@10': 0.6837, 'nDCG@10': 0.7451}\n  drop vs clean (lower): {'ΔMRR@1': -0.008425, 'ΔMRR@5': -0.005715, 'ΔMRR@10': -0.005621}\n\n=== LAReQA RU→EN | FT_INST | plain ===\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"[cache] docs torch.Size([1180, 768]) saved -> /kaggle/working/robust_eval_cache/docs__lareqa_RU→EN__inst__be7f5f38__L256.pt\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7195, 'Recall@1': 0.7195, 'MRR@1': 0.7195, 'MAP@1': 0.7195, 'nDCG@1': 0.7195, 'Recall@5': 0.9208, 'MRR@5': 0.8029, 'MAP@5': 0.6617, 'nDCG@5': 0.7181, 'Recall@10': 0.9596, 'MRR@10': 0.8081, 'MAP@10': 0.6614, 'nDCG@10': 0.7266}\n\n=== LAReQA RU→EN | FT_INST | q+BANK ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7178, 'Recall@1': 0.7178, 'MRR@1': 0.7178, 'MAP@1': 0.7178, 'nDCG@1': 0.7178, 'Recall@5': 0.9149, 'MRR@5': 0.7986, 'MAP@5': 0.6599, 'nDCG@5': 0.7152, 'Recall@10': 0.9511, 'MRR@10': 0.8035, 'MAP@10': 0.6596, 'nDCG@10': 0.723}\n\n--- LAReQA p-MRR (BANK − plain) :: RU→EN :: FT_INST ---\n{'pMRR@1': -0.001685, 'pMRR@5': -0.004353, 'pMRR@10': -0.004693}\n\n--- Prompt ablation :: RU→EN :: FT_INST ---\n\n=== LAReQA RU→EN | FT_INST | prompt:plain ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7195, 'Recall@1': 0.7195, 'MRR@1': 0.7195, 'MAP@1': 0.7195, 'nDCG@1': 0.7195, 'Recall@5': 0.9208, 'MRR@5': 0.8029, 'MAP@5': 0.6617, 'nDCG@5': 0.7181, 'Recall@10': 0.9596, 'MRR@10': 0.8081, 'MAP@10': 0.6614, 'nDCG@10': 0.7266}\n\n=== LAReQA RU→EN | FT_INST | prompt:minimal_ru ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7203, 'Recall@1': 0.7203, 'MRR@1': 0.7203, 'MAP@1': 0.7203, 'nDCG@1': 0.7203, 'Recall@5': 0.9208, 'MRR@5': 0.8027, 'MAP@5': 0.6626, 'nDCG@5': 0.7185, 'Recall@10': 0.9562, 'MRR@10': 0.8075, 'MAP@10': 0.6621, 'nDCG@10': 0.7265}\n\n=== LAReQA RU→EN | FT_INST | prompt:qa_ru ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7262, 'Recall@1': 0.7262, 'MRR@1': 0.7262, 'MAP@1': 0.7262, 'nDCG@1': 0.7262, 'Recall@5': 0.92, 'MRR@5': 0.8068, 'MAP@5': 0.6659, 'nDCG@5': 0.7215, 'Recall@10': 0.9621, 'MRR@10': 0.8126, 'MAP@10': 0.6663, 'nDCG@10': 0.7312}\n\n=== LAReQA RU→EN | FT_INST | prompt:search_ru ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7195, 'Recall@1': 0.7195, 'MRR@1': 0.7195, 'MAP@1': 0.7195, 'nDCG@1': 0.7195, 'Recall@5': 0.9166, 'MRR@5': 0.8006, 'MAP@5': 0.6604, 'nDCG@5': 0.716, 'Recall@10': 0.9545, 'MRR@10': 0.8057, 'MAP@10': 0.6605, 'nDCG@10': 0.7248}\n\n=== LAReQA RU→EN | FT_INST | prompt:mfoll_ru ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.727, 'Recall@1': 0.727, 'MRR@1': 0.727, 'MAP@1': 0.727, 'nDCG@1': 0.727, 'Recall@5': 0.9166, 'MRR@5': 0.805, 'MAP@5': 0.6645, 'nDCG@5': 0.7195, 'Recall@10': 0.9562, 'MRR@10': 0.8104, 'MAP@10': 0.6647, 'nDCG@10': 0.7286}\n\n--- Paraphrase robustness (5x) :: RU→EN :: FT_INST ---\n\n=== LAReQA RU→EN | FT_INST | para1 ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7144, 'Recall@1': 0.7144, 'MRR@1': 0.7144, 'MAP@1': 0.7144, 'nDCG@1': 0.7144, 'Recall@5': 0.9158, 'MRR@5': 0.7964, 'MAP@5': 0.6566, 'nDCG@5': 0.7121, 'Recall@10': 0.9537, 'MRR@10': 0.8014, 'MAP@10': 0.657, 'nDCG@10': 0.7212}\n\n=== LAReQA RU→EN | FT_INST | para2 ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7211, 'Recall@1': 0.7211, 'MRR@1': 0.7211, 'MAP@1': 0.7211, 'nDCG@1': 0.7211, 'Recall@5': 0.9132, 'MRR@5': 0.7996, 'MAP@5': 0.6613, 'nDCG@5': 0.7158, 'Recall@10': 0.9553, 'MRR@10': 0.8053, 'MAP@10': 0.6614, 'nDCG@10': 0.7252}\n\n=== LAReQA RU→EN | FT_INST | para3 ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7245, 'Recall@1': 0.7245, 'MRR@1': 0.7245, 'MAP@1': 0.7245, 'nDCG@1': 0.7245, 'Recall@5': 0.9183, 'MRR@5': 0.8038, 'MAP@5': 0.6617, 'nDCG@5': 0.7171, 'Recall@10': 0.9562, 'MRR@10': 0.8089, 'MAP@10': 0.6622, 'nDCG@10': 0.7266}\n\n=== LAReQA RU→EN | FT_INST | para4 ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7254, 'Recall@1': 0.7254, 'MRR@1': 0.7254, 'MAP@1': 0.7254, 'nDCG@1': 0.7254, 'Recall@5': 0.9124, 'MRR@5': 0.802, 'MAP@5': 0.6621, 'nDCG@5': 0.7164, 'Recall@10': 0.957, 'MRR@10': 0.8081, 'MAP@10': 0.6629, 'nDCG@10': 0.7272}\n\n=== LAReQA RU→EN | FT_INST | para5 ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7186, 'Recall@1': 0.7186, 'MRR@1': 0.7186, 'MAP@1': 0.7186, 'nDCG@1': 0.7186, 'Recall@5': 0.9107, 'MRR@5': 0.7975, 'MAP@5': 0.6593, 'nDCG@5': 0.7137, 'Recall@10': 0.9553, 'MRR@10': 0.8037, 'MAP@10': 0.6598, 'nDCG@10': 0.7239}\nParaphrase mean/std: {'Hit@1_mean': 0.7208, 'Hit@1_std': 0.004, 'Recall@1_mean': 0.7208, 'Recall@1_std': 0.004, 'MRR@1_mean': 0.7208, 'MRR@1_std': 0.004, 'MAP@1_mean': 0.7208, 'MAP@1_std': 0.004, 'nDCG@1_mean': 0.7208, 'nDCG@1_std': 0.004, 'Recall@5_mean': 0.9141, 'Recall@5_std': 0.0027, 'MRR@5_mean': 0.7999, 'MRR@5_std': 0.0028, 'MAP@5_mean': 0.6602, 'MAP@5_std': 0.0021, 'nDCG@5_mean': 0.715, 'nDCG@5_std': 0.0019, 'Recall@10_mean': 0.9555, 'Recall@10_std': 0.0011, 'MRR@10_mean': 0.8055, 'MRR@10_std': 0.0028, 'MAP@10_mean': 0.6607, 'MAP@10_std': 0.0021, 'nDCG@10_mean': 0.7248, 'nDCG@10_std': 0.0021}\n\n--- Noise stress :: RU→EN :: FT_INST ---\n\n=== LAReQA RU→EN | FT_INST | noise:clean ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7195, 'Recall@1': 0.7195, 'MRR@1': 0.7195, 'MAP@1': 0.7195, 'nDCG@1': 0.7195, 'Recall@5': 0.9208, 'MRR@5': 0.8029, 'MAP@5': 0.6617, 'nDCG@5': 0.7181, 'Recall@10': 0.9596, 'MRR@10': 0.8081, 'MAP@10': 0.6614, 'nDCG@10': 0.7266}\n\n=== LAReQA RU→EN | FT_INST | noise:nopunct ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7178, 'Recall@1': 0.7178, 'MRR@1': 0.7178, 'MAP@1': 0.7178, 'nDCG@1': 0.7178, 'Recall@5': 0.9233, 'MRR@5': 0.8048, 'MAP@5': 0.6653, 'nDCG@5': 0.7214, 'Recall@10': 0.9587, 'MRR@10': 0.8095, 'MAP@10': 0.6649, 'nDCG@10': 0.7293}\n  drop vs clean (nopunct): {'ΔMRR@1': -0.001685, 'ΔMRR@5': 0.001853, 'ΔMRR@10': 0.001391}\n\n=== LAReQA RU→EN | FT_INST | noise:swap ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.706, 'Recall@1': 0.706, 'MRR@1': 0.706, 'MAP@1': 0.706, 'nDCG@1': 0.706, 'Recall@5': 0.9183, 'MRR@5': 0.7947, 'MAP@5': 0.656, 'nDCG@5': 0.7127, 'Recall@10': 0.9545, 'MRR@10': 0.7996, 'MAP@10': 0.6564, 'nDCG@10': 0.722}\n  drop vs clean (swap): {'ΔMRR@1': -0.013479, 'ΔMRR@5': -0.008242, 'ΔMRR@10': -0.008535}\n\n=== LAReQA RU→EN | FT_INST | noise:lower ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7068, 'Recall@1': 0.7068, 'MRR@1': 0.7068, 'MAP@1': 0.7068, 'nDCG@1': 0.7068, 'Recall@5': 0.9208, 'MRR@5': 0.7953, 'MAP@5': 0.6571, 'nDCG@5': 0.714, 'Recall@10': 0.9553, 'MRR@10': 0.8, 'MAP@10': 0.6564, 'nDCG@10': 0.7214}\n  drop vs clean (lower): {'ΔMRR@1': -0.012637, 'ΔMRR@5': -0.007638, 'ΔMRR@10': -0.008149}\n\n=== LAReQA RU→EN | FT_QUERY | plain ===\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"[cache] docs torch.Size([1180, 768]) saved -> /kaggle/working/robust_eval_cache/docs__lareqa_RU→EN__qonly__be7f5f38__L256.pt\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7439, 'Recall@1': 0.7439, 'MRR@1': 0.7439, 'MAP@1': 0.7439, 'nDCG@1': 0.7439, 'Recall@5': 0.9301, 'MRR@5': 0.8198, 'MAP@5': 0.6805, 'nDCG@5': 0.736, 'Recall@10': 0.9638, 'MRR@10': 0.8242, 'MAP@10': 0.6787, 'nDCG@10': 0.7417}\n\n=== LAReQA RU→EN | FT_QUERY | q+BANK ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7414, 'Recall@1': 0.7414, 'MRR@1': 0.7414, 'MAP@1': 0.7414, 'nDCG@1': 0.7414, 'Recall@5': 0.9233, 'MRR@5': 0.817, 'MAP@5': 0.6756, 'nDCG@5': 0.7308, 'Recall@10': 0.9587, 'MRR@10': 0.8218, 'MAP@10': 0.6744, 'nDCG@10': 0.7375}\n\n--- LAReQA p-MRR (BANK − plain) :: RU→EN :: FT_QUERY ---\n{'pMRR@1': -0.002527, 'pMRR@5': -0.002724, 'pMRR@10': -0.002419}\n\n--- Prompt ablation :: RU→EN :: FT_QUERY ---\n\n=== LAReQA RU→EN | FT_QUERY | prompt:plain ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7439, 'Recall@1': 0.7439, 'MRR@1': 0.7439, 'MAP@1': 0.7439, 'nDCG@1': 0.7439, 'Recall@5': 0.9301, 'MRR@5': 0.8198, 'MAP@5': 0.6805, 'nDCG@5': 0.736, 'Recall@10': 0.9638, 'MRR@10': 0.8242, 'MAP@10': 0.6787, 'nDCG@10': 0.7417}\n\n=== LAReQA RU→EN | FT_QUERY | prompt:minimal_ru ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7388, 'Recall@1': 0.7388, 'MRR@1': 0.7388, 'MAP@1': 0.7388, 'nDCG@1': 0.7388, 'Recall@5': 0.925, 'MRR@5': 0.816, 'MAP@5': 0.6751, 'nDCG@5': 0.7309, 'Recall@10': 0.9646, 'MRR@10': 0.8216, 'MAP@10': 0.6744, 'nDCG@10': 0.7388}\n\n=== LAReQA RU→EN | FT_QUERY | prompt:qa_ru ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.743, 'Recall@1': 0.743, 'MRR@1': 0.743, 'MAP@1': 0.743, 'nDCG@1': 0.743, 'Recall@5': 0.9309, 'MRR@5': 0.8203, 'MAP@5': 0.6789, 'nDCG@5': 0.735, 'Recall@10': 0.9621, 'MRR@10': 0.8246, 'MAP@10': 0.6776, 'nDCG@10': 0.7411}\n\n=== LAReQA RU→EN | FT_QUERY | prompt:search_ru ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7464, 'Recall@1': 0.7464, 'MRR@1': 0.7464, 'MAP@1': 0.7464, 'nDCG@1': 0.7464, 'Recall@5': 0.9267, 'MRR@5': 0.8202, 'MAP@5': 0.68, 'nDCG@5': 0.7347, 'Recall@10': 0.9612, 'MRR@10': 0.825, 'MAP@10': 0.6785, 'nDCG@10': 0.7409}\n\n=== LAReQA RU→EN | FT_QUERY | prompt:mfoll_ru ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.743, 'Recall@1': 0.743, 'MRR@1': 0.743, 'MAP@1': 0.743, 'nDCG@1': 0.743, 'Recall@5': 0.9309, 'MRR@5': 0.8202, 'MAP@5': 0.6781, 'nDCG@5': 0.7341, 'Recall@10': 0.9655, 'MRR@10': 0.8249, 'MAP@10': 0.6768, 'nDCG@10': 0.7406}\n\n--- Paraphrase robustness (5x) :: RU→EN :: FT_QUERY ---\n\n=== LAReQA RU→EN | FT_QUERY | para1 ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7363, 'Recall@1': 0.7363, 'MRR@1': 0.7363, 'MAP@1': 0.7363, 'nDCG@1': 0.7363, 'Recall@5': 0.9225, 'MRR@5': 0.8133, 'MAP@5': 0.6714, 'nDCG@5': 0.7268, 'Recall@10': 0.9612, 'MRR@10': 0.8185, 'MAP@10': 0.6715, 'nDCG@10': 0.7359}\n\n=== LAReQA RU→EN | FT_QUERY | para2 ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7372, 'Recall@1': 0.7372, 'MRR@1': 0.7372, 'MAP@1': 0.7372, 'nDCG@1': 0.7372, 'Recall@5': 0.9267, 'MRR@5': 0.8176, 'MAP@5': 0.6758, 'nDCG@5': 0.7312, 'Recall@10': 0.9621, 'MRR@10': 0.8225, 'MAP@10': 0.6758, 'nDCG@10': 0.7396}\n\n=== LAReQA RU→EN | FT_QUERY | para3 ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7489, 'Recall@1': 0.7489, 'MRR@1': 0.7489, 'MAP@1': 0.7489, 'nDCG@1': 0.7489, 'Recall@5': 0.9309, 'MRR@5': 0.8236, 'MAP@5': 0.6806, 'nDCG@5': 0.7359, 'Recall@10': 0.9621, 'MRR@10': 0.828, 'MAP@10': 0.6795, 'nDCG@10': 0.7424}\n\n=== LAReQA RU→EN | FT_QUERY | para4 ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7397, 'Recall@1': 0.7397, 'MRR@1': 0.7397, 'MAP@1': 0.7397, 'nDCG@1': 0.7397, 'Recall@5': 0.9259, 'MRR@5': 0.8181, 'MAP@5': 0.6759, 'nDCG@5': 0.7307, 'Recall@10': 0.9596, 'MRR@10': 0.8229, 'MAP@10': 0.6764, 'nDCG@10': 0.7395}\n\n=== LAReQA RU→EN | FT_QUERY | para5 ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7414, 'Recall@1': 0.7414, 'MRR@1': 0.7414, 'MAP@1': 0.7414, 'nDCG@1': 0.7414, 'Recall@5': 0.925, 'MRR@5': 0.8179, 'MAP@5': 0.6756, 'nDCG@5': 0.7303, 'Recall@10': 0.9587, 'MRR@10': 0.8226, 'MAP@10': 0.6755, 'nDCG@10': 0.7383}\nParaphrase mean/std: {'Hit@1_mean': 0.7407, 'Hit@1_std': 0.0045, 'Recall@1_mean': 0.7407, 'Recall@1_std': 0.0045, 'MRR@1_mean': 0.7407, 'MRR@1_std': 0.0045, 'MAP@1_mean': 0.7407, 'MAP@1_std': 0.0045, 'nDCG@1_mean': 0.7407, 'nDCG@1_std': 0.0045, 'Recall@5_mean': 0.9262, 'Recall@5_std': 0.0027, 'MRR@5_mean': 0.8181, 'MRR@5_std': 0.0033, 'MAP@5_mean': 0.6759, 'MAP@5_std': 0.0029, 'nDCG@5_mean': 0.731, 'nDCG@5_std': 0.0029, 'Recall@10_mean': 0.9607, 'Recall@10_std': 0.0014, 'MRR@10_mean': 0.8229, 'MRR@10_std': 0.003, 'MAP@10_mean': 0.6757, 'MAP@10_std': 0.0025, 'nDCG@10_mean': 0.7391, 'nDCG@10_std': 0.0021}\n\n--- Noise stress :: RU→EN :: FT_QUERY ---\n\n=== LAReQA RU→EN | FT_QUERY | noise:clean ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7439, 'Recall@1': 0.7439, 'MRR@1': 0.7439, 'MAP@1': 0.7439, 'nDCG@1': 0.7439, 'Recall@5': 0.9301, 'MRR@5': 0.8198, 'MAP@5': 0.6805, 'nDCG@5': 0.736, 'Recall@10': 0.9638, 'MRR@10': 0.8242, 'MAP@10': 0.6787, 'nDCG@10': 0.7417}\n\n=== LAReQA RU→EN | FT_QUERY | noise:nopunct ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7498, 'Recall@1': 0.7498, 'MRR@1': 0.7498, 'MAP@1': 0.7498, 'nDCG@1': 0.7498, 'Recall@5': 0.9284, 'MRR@5': 0.8242, 'MAP@5': 0.6828, 'nDCG@5': 0.7373, 'Recall@10': 0.9663, 'MRR@10': 0.8294, 'MAP@10': 0.6824, 'nDCG@10': 0.7458}\n  drop vs clean (nopunct): {'ΔMRR@1': 0.005897, 'ΔMRR@5': 0.004437, 'ΔMRR@10': 0.005122}\n\n=== LAReQA RU→EN | FT_QUERY | noise:swap ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7447, 'Recall@1': 0.7447, 'MRR@1': 0.7447, 'MAP@1': 0.7447, 'nDCG@1': 0.7447, 'Recall@5': 0.9242, 'MRR@5': 0.8195, 'MAP@5': 0.6781, 'nDCG@5': 0.7324, 'Recall@10': 0.9596, 'MRR@10': 0.8242, 'MAP@10': 0.6779, 'nDCG@10': 0.7408}\n  drop vs clean (swap): {'ΔMRR@1': 0.000842, 'ΔMRR@5': -0.000267, 'ΔMRR@10': -6.1e-05}\n\n=== LAReQA RU→EN | FT_QUERY | noise:lower ===\n[cache] docs torch.Size([1180, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1187, 'Hit@1': 0.7447, 'Recall@1': 0.7447, 'MRR@1': 0.7447, 'MAP@1': 0.7447, 'nDCG@1': 0.7447, 'Recall@5': 0.9259, 'MRR@5': 0.8198, 'MAP@5': 0.6792, 'nDCG@5': 0.7336, 'Recall@10': 0.9596, 'MRR@10': 0.8245, 'MAP@10': 0.6783, 'nDCG@10': 0.741}\n  drop vs clean (lower): {'ΔMRR@1': 0.000842, 'ΔMRR@5': 1.4e-05, 'ΔMRR@10': 0.000233}\n\n--- EN→RU --- Docs=1,219 | Queries=1,188 | QrelsQ=1,188\n\n=== LAReQA EN→RU | BASE | plain ===\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"[cache] docs torch.Size([1219, 768]) saved -> /kaggle/working/robust_eval_cache/docs__lareqa_EN→RU__base__5723c199__L256.pt\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.7449, 'Recall@1': 0.7449, 'MRR@1': 0.7449, 'MAP@1': 0.7449, 'nDCG@1': 0.7449, 'Recall@5': 0.9276, 'MRR@5': 0.8207, 'MAP@5': 0.7149, 'nDCG@5': 0.7613, 'Recall@10': 0.9596, 'MRR@10': 0.8251, 'MAP@10': 0.7168, 'nDCG@10': 0.7705}\n\n=== LAReQA EN→RU | BASE | q+BANK ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.7003, 'Recall@1': 0.7003, 'MRR@1': 0.7003, 'MAP@1': 0.7003, 'nDCG@1': 0.7003, 'Recall@5': 0.8939, 'MRR@5': 0.7788, 'MAP@5': 0.6739, 'nDCG@5': 0.722, 'Recall@10': 0.931, 'MRR@10': 0.7839, 'MAP@10': 0.6757, 'nDCG@10': 0.7313}\n\n--- LAReQA p-MRR (BANK − plain) :: EN→RU :: BASE ---\n{'pMRR@1': -0.044613, 'pMRR@5': -0.041835, 'pMRR@10': -0.041213}\n\n--- Prompt ablation :: EN→RU :: BASE ---\n\n=== LAReQA EN→RU | BASE | prompt:plain ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.7449, 'Recall@1': 0.7449, 'MRR@1': 0.7449, 'MAP@1': 0.7449, 'nDCG@1': 0.7449, 'Recall@5': 0.9276, 'MRR@5': 0.8207, 'MAP@5': 0.7149, 'nDCG@5': 0.7613, 'Recall@10': 0.9596, 'MRR@10': 0.8251, 'MAP@10': 0.7168, 'nDCG@10': 0.7705}\n\n=== LAReQA EN→RU | BASE | prompt:minimal_en ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.6237, 'Recall@1': 0.6237, 'MRR@1': 0.6237, 'MAP@1': 0.6237, 'nDCG@1': 0.6237, 'Recall@5': 0.8788, 'MRR@5': 0.7273, 'MAP@5': 0.6322, 'nDCG@5': 0.6852, 'Recall@10': 0.9226, 'MRR@10': 0.7333, 'MAP@10': 0.6343, 'nDCG@10': 0.6951}\n\n=== LAReQA EN→RU | BASE | prompt:qa_en ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.6852, 'Recall@1': 0.6852, 'MRR@1': 0.6852, 'MAP@1': 0.6852, 'nDCG@1': 0.6852, 'Recall@5': 0.8948, 'MRR@5': 0.7717, 'MAP@5': 0.6684, 'nDCG@5': 0.7176, 'Recall@10': 0.9377, 'MRR@10': 0.7775, 'MAP@10': 0.6718, 'nDCG@10': 0.7299}\n\n=== LAReQA EN→RU | BASE | prompt:search_en ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.6364, 'Recall@1': 0.6364, 'MRR@1': 0.6364, 'MAP@1': 0.6364, 'nDCG@1': 0.6364, 'Recall@5': 0.8914, 'MRR@5': 0.7402, 'MAP@5': 0.6453, 'nDCG@5': 0.6982, 'Recall@10': 0.9276, 'MRR@10': 0.7451, 'MAP@10': 0.6477, 'nDCG@10': 0.7081}\n\n=== LAReQA EN→RU | BASE | prompt:mfoll_en ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.6254, 'Recall@1': 0.6254, 'MRR@1': 0.6254, 'MAP@1': 0.6254, 'nDCG@1': 0.6254, 'Recall@5': 0.8822, 'MRR@5': 0.7264, 'MAP@5': 0.6305, 'nDCG@5': 0.6852, 'Recall@10': 0.9226, 'MRR@10': 0.732, 'MAP@10': 0.6325, 'nDCG@10': 0.6948}\n\n--- Paraphrase robustness (5x) :: EN→RU :: BASE ---\n\n=== LAReQA EN→RU | BASE | para1 ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.6212, 'Recall@1': 0.6212, 'MRR@1': 0.6212, 'MAP@1': 0.6212, 'nDCG@1': 0.6212, 'Recall@5': 0.8687, 'MRR@5': 0.7194, 'MAP@5': 0.626, 'nDCG@5': 0.6784, 'Recall@10': 0.9209, 'MRR@10': 0.7265, 'MAP@10': 0.6285, 'nDCG@10': 0.6902}\n\n=== LAReQA EN→RU | BASE | para2 ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.6305, 'Recall@1': 0.6305, 'MRR@1': 0.6305, 'MAP@1': 0.6305, 'nDCG@1': 0.6305, 'Recall@5': 0.8923, 'MRR@5': 0.7354, 'MAP@5': 0.6393, 'nDCG@5': 0.6939, 'Recall@10': 0.9259, 'MRR@10': 0.7399, 'MAP@10': 0.6405, 'nDCG@10': 0.7016}\n\n=== LAReQA EN→RU | BASE | para3 ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.6136, 'Recall@1': 0.6136, 'MRR@1': 0.6136, 'MAP@1': 0.6136, 'nDCG@1': 0.6136, 'Recall@5': 0.8855, 'MRR@5': 0.7219, 'MAP@5': 0.6277, 'nDCG@5': 0.6839, 'Recall@10': 0.9242, 'MRR@10': 0.7272, 'MAP@10': 0.629, 'nDCG@10': 0.6922}\n\n=== LAReQA EN→RU | BASE | para4 ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.5926, 'Recall@1': 0.5926, 'MRR@1': 0.5926, 'MAP@1': 0.5926, 'nDCG@1': 0.5926, 'Recall@5': 0.8822, 'MRR@5': 0.7092, 'MAP@5': 0.6158, 'nDCG@5': 0.6739, 'Recall@10': 0.9175, 'MRR@10': 0.7143, 'MAP@10': 0.6171, 'nDCG@10': 0.6818}\n\n=== LAReQA EN→RU | BASE | para5 ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.7071, 'Recall@1': 0.7071, 'MRR@1': 0.7071, 'MAP@1': 0.7071, 'nDCG@1': 0.7071, 'Recall@5': 0.9125, 'MRR@5': 0.7923, 'MAP@5': 0.6889, 'nDCG@5': 0.7376, 'Recall@10': 0.947, 'MRR@10': 0.7969, 'MAP@10': 0.6906, 'nDCG@10': 0.7465}\nParaphrase mean/std: {'Hit@1_mean': 0.633, 'Hit@1_std': 0.0391, 'Recall@1_mean': 0.633, 'Recall@1_std': 0.0391, 'MRR@1_mean': 0.633, 'MRR@1_std': 0.0391, 'MAP@1_mean': 0.633, 'MAP@1_std': 0.0391, 'nDCG@1_mean': 0.633, 'nDCG@1_std': 0.0391, 'Recall@5_mean': 0.8882, 'Recall@5_std': 0.0143, 'MRR@5_mean': 0.7356, 'MRR@5_std': 0.0295, 'MAP@5_mean': 0.6395, 'MAP@5_std': 0.0258, 'nDCG@5_mean': 0.6935, 'nDCG@5_std': 0.023, 'Recall@10_mean': 0.9271, 'Recall@10_std': 0.0103, 'MRR@10_mean': 0.741, 'MRR@10_std': 0.0291, 'MAP@10_mean': 0.6411, 'MAP@10_std': 0.0258, 'nDCG@10_mean': 0.7025, 'nDCG@10_std': 0.0229}\n\n--- Noise stress :: EN→RU :: BASE ---\n\n=== LAReQA EN→RU | BASE | noise:clean ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.7449, 'Recall@1': 0.7449, 'MRR@1': 0.7449, 'MAP@1': 0.7449, 'nDCG@1': 0.7449, 'Recall@5': 0.9276, 'MRR@5': 0.8207, 'MAP@5': 0.7149, 'nDCG@5': 0.7613, 'Recall@10': 0.9596, 'MRR@10': 0.8251, 'MAP@10': 0.7168, 'nDCG@10': 0.7705}\n\n=== LAReQA EN→RU | BASE | noise:nopunct ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.7449, 'Recall@1': 0.7449, 'MRR@1': 0.7449, 'MAP@1': 0.7449, 'nDCG@1': 0.7449, 'Recall@5': 0.9209, 'MRR@5': 0.8189, 'MAP@5': 0.7135, 'nDCG@5': 0.7584, 'Recall@10': 0.9571, 'MRR@10': 0.8238, 'MAP@10': 0.7166, 'nDCG@10': 0.7698}\n  drop vs clean (nopunct): {'ΔMRR@1': 0.0, 'ΔMRR@5': -0.001726, 'ΔMRR@10': -0.00135}\n\n=== LAReQA EN→RU | BASE | noise:swap ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.7247, 'Recall@1': 0.7247, 'MRR@1': 0.7247, 'MAP@1': 0.7247, 'nDCG@1': 0.7247, 'Recall@5': 0.9184, 'MRR@5': 0.8036, 'MAP@5': 0.6982, 'nDCG@5': 0.7456, 'Recall@10': 0.9478, 'MRR@10': 0.8076, 'MAP@10': 0.7005, 'nDCG@10': 0.755}\n  drop vs clean (swap): {'ΔMRR@1': -0.020202, 'ΔMRR@5': -0.017031, 'ΔMRR@10': -0.017553}\n\n=== LAReQA EN→RU | BASE | noise:lower ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.7382, 'Recall@1': 0.7382, 'MRR@1': 0.7382, 'MAP@1': 0.7382, 'nDCG@1': 0.7382, 'Recall@5': 0.9251, 'MRR@5': 0.8154, 'MAP@5': 0.7112, 'nDCG@5': 0.7589, 'Recall@10': 0.9638, 'MRR@10': 0.8207, 'MAP@10': 0.7141, 'nDCG@10': 0.7704}\n  drop vs clean (lower): {'ΔMRR@1': -0.006734, 'ΔMRR@5': -0.005205, 'ΔMRR@10': -0.004454}\n\n=== LAReQA EN→RU | FT_INST | plain ===\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"[cache] docs torch.Size([1219, 768]) saved -> /kaggle/working/robust_eval_cache/docs__lareqa_EN→RU__inst__5723c199__L256.pt\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.6995, 'Recall@1': 0.6995, 'MRR@1': 0.6995, 'MAP@1': 0.6995, 'nDCG@1': 0.6995, 'Recall@5': 0.915, 'MRR@5': 0.786, 'MAP@5': 0.6829, 'nDCG@5': 0.7346, 'Recall@10': 0.9537, 'MRR@10': 0.7913, 'MAP@10': 0.6866, 'nDCG@10': 0.7469}\n\n=== LAReQA EN→RU | FT_INST | q+BANK ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.6827, 'Recall@1': 0.6827, 'MRR@1': 0.6827, 'MAP@1': 0.6827, 'nDCG@1': 0.6827, 'Recall@5': 0.9007, 'MRR@5': 0.7706, 'MAP@5': 0.6671, 'nDCG@5': 0.7192, 'Recall@10': 0.9411, 'MRR@10': 0.7763, 'MAP@10': 0.6707, 'nDCG@10': 0.7316}\n\n--- LAReQA p-MRR (BANK − plain) :: EN→RU :: FT_INST ---\n{'pMRR@1': -0.016835, 'pMRR@5': -0.015362, 'pMRR@10': -0.015028}\n\n--- Prompt ablation :: EN→RU :: FT_INST ---\n\n=== LAReQA EN→RU | FT_INST | prompt:plain ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.6995, 'Recall@1': 0.6995, 'MRR@1': 0.6995, 'MAP@1': 0.6995, 'nDCG@1': 0.6995, 'Recall@5': 0.915, 'MRR@5': 0.786, 'MAP@5': 0.6829, 'nDCG@5': 0.7346, 'Recall@10': 0.9537, 'MRR@10': 0.7913, 'MAP@10': 0.6866, 'nDCG@10': 0.7469}\n\n=== LAReQA EN→RU | FT_INST | prompt:minimal_en ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.6944, 'Recall@1': 0.6944, 'MRR@1': 0.6944, 'MAP@1': 0.6944, 'nDCG@1': 0.6944, 'Recall@5': 0.9099, 'MRR@5': 0.7812, 'MAP@5': 0.6772, 'nDCG@5': 0.7293, 'Recall@10': 0.947, 'MRR@10': 0.7863, 'MAP@10': 0.6801, 'nDCG@10': 0.7402}\n\n=== LAReQA EN→RU | FT_INST | prompt:qa_en ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.6919, 'Recall@1': 0.6919, 'MRR@1': 0.6919, 'MAP@1': 0.6919, 'nDCG@1': 0.6919, 'Recall@5': 0.9082, 'MRR@5': 0.7788, 'MAP@5': 0.676, 'nDCG@5': 0.7278, 'Recall@10': 0.9495, 'MRR@10': 0.7846, 'MAP@10': 0.6803, 'nDCG@10': 0.7412}\n\n=== LAReQA EN→RU | FT_INST | prompt:search_en ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.6869, 'Recall@1': 0.6869, 'MRR@1': 0.6869, 'MAP@1': 0.6869, 'nDCG@1': 0.6869, 'Recall@5': 0.9049, 'MRR@5': 0.7749, 'MAP@5': 0.6719, 'nDCG@5': 0.7239, 'Recall@10': 0.9394, 'MRR@10': 0.7796, 'MAP@10': 0.6749, 'nDCG@10': 0.7346}\n\n=== LAReQA EN→RU | FT_INST | prompt:mfoll_en ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.697, 'Recall@1': 0.697, 'MRR@1': 0.697, 'MAP@1': 0.697, 'nDCG@1': 0.697, 'Recall@5': 0.9116, 'MRR@5': 0.7844, 'MAP@5': 0.6787, 'nDCG@5': 0.7311, 'Recall@10': 0.9503, 'MRR@10': 0.7898, 'MAP@10': 0.6825, 'nDCG@10': 0.7434}\n\n--- Paraphrase robustness (5x) :: EN→RU :: FT_INST ---\n\n=== LAReQA EN→RU | FT_INST | para1 ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.6936, 'Recall@1': 0.6936, 'MRR@1': 0.6936, 'MAP@1': 0.6936, 'nDCG@1': 0.6936, 'Recall@5': 0.9108, 'MRR@5': 0.781, 'MAP@5': 0.6762, 'nDCG@5': 0.7285, 'Recall@10': 0.9453, 'MRR@10': 0.7857, 'MAP@10': 0.6792, 'nDCG@10': 0.7392}\n\n=== LAReQA EN→RU | FT_INST | para2 ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.6877, 'Recall@1': 0.6877, 'MRR@1': 0.6877, 'MAP@1': 0.6877, 'nDCG@1': 0.6877, 'Recall@5': 0.9066, 'MRR@5': 0.775, 'MAP@5': 0.671, 'nDCG@5': 0.7238, 'Recall@10': 0.947, 'MRR@10': 0.7806, 'MAP@10': 0.6741, 'nDCG@10': 0.7354}\n\n=== LAReQA EN→RU | FT_INST | para3 ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.6995, 'Recall@1': 0.6995, 'MRR@1': 0.6995, 'MAP@1': 0.6995, 'nDCG@1': 0.6995, 'Recall@5': 0.9074, 'MRR@5': 0.7829, 'MAP@5': 0.679, 'nDCG@5': 0.7301, 'Recall@10': 0.9453, 'MRR@10': 0.788, 'MAP@10': 0.682, 'nDCG@10': 0.7411}\n\n=== LAReQA EN→RU | FT_INST | para4 ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.6961, 'Recall@1': 0.6961, 'MRR@1': 0.6961, 'MAP@1': 0.6961, 'nDCG@1': 0.6961, 'Recall@5': 0.9066, 'MRR@5': 0.7813, 'MAP@5': 0.6771, 'nDCG@5': 0.7286, 'Recall@10': 0.9436, 'MRR@10': 0.7865, 'MAP@10': 0.6801, 'nDCG@10': 0.7395}\n\n=== LAReQA EN→RU | FT_INST | para5 ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.7003, 'Recall@1': 0.7003, 'MRR@1': 0.7003, 'MAP@1': 0.7003, 'nDCG@1': 0.7003, 'Recall@5': 0.9108, 'MRR@5': 0.7847, 'MAP@5': 0.6807, 'nDCG@5': 0.7322, 'Recall@10': 0.9495, 'MRR@10': 0.7901, 'MAP@10': 0.6842, 'nDCG@10': 0.7442}\nParaphrase mean/std: {'Hit@1_mean': 0.6955, 'Hit@1_std': 0.0046, 'Recall@1_mean': 0.6955, 'Recall@1_std': 0.0046, 'MRR@1_mean': 0.6955, 'MRR@1_std': 0.0046, 'MAP@1_mean': 0.6955, 'MAP@1_std': 0.0046, 'nDCG@1_mean': 0.6955, 'nDCG@1_std': 0.0046, 'Recall@5_mean': 0.9084, 'Recall@5_std': 0.0019, 'MRR@5_mean': 0.781, 'MRR@5_std': 0.0033, 'MAP@5_mean': 0.6768, 'MAP@5_std': 0.0033, 'nDCG@5_mean': 0.7286, 'nDCG@5_std': 0.0028, 'Recall@10_mean': 0.9461, 'Recall@10_std': 0.002, 'MRR@10_mean': 0.7862, 'MRR@10_std': 0.0032, 'MAP@10_mean': 0.6799, 'MAP@10_std': 0.0034, 'nDCG@10_mean': 0.7399, 'nDCG@10_std': 0.0029}\n\n--- Noise stress :: EN→RU :: FT_INST ---\n\n=== LAReQA EN→RU | FT_INST | noise:clean ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.6995, 'Recall@1': 0.6995, 'MRR@1': 0.6995, 'MAP@1': 0.6995, 'nDCG@1': 0.6995, 'Recall@5': 0.915, 'MRR@5': 0.786, 'MAP@5': 0.6829, 'nDCG@5': 0.7346, 'Recall@10': 0.9537, 'MRR@10': 0.7913, 'MAP@10': 0.6866, 'nDCG@10': 0.7469}\n\n=== LAReQA EN→RU | FT_INST | noise:nopunct ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.6978, 'Recall@1': 0.6978, 'MRR@1': 0.6978, 'MAP@1': 0.6978, 'nDCG@1': 0.6978, 'Recall@5': 0.9116, 'MRR@5': 0.7848, 'MAP@5': 0.6816, 'nDCG@5': 0.733, 'Recall@10': 0.9529, 'MRR@10': 0.7905, 'MAP@10': 0.6858, 'nDCG@10': 0.7463}\n  drop vs clean (nopunct): {'ΔMRR@1': -0.001684, 'ΔMRR@5': -0.001235, 'ΔMRR@10': -0.000829}\n\n=== LAReQA EN→RU | FT_INST | noise:swap ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.6944, 'Recall@1': 0.6944, 'MRR@1': 0.6944, 'MAP@1': 0.6944, 'nDCG@1': 0.6944, 'Recall@5': 0.9049, 'MRR@5': 0.7789, 'MAP@5': 0.6771, 'nDCG@5': 0.7281, 'Recall@10': 0.9444, 'MRR@10': 0.7842, 'MAP@10': 0.6807, 'nDCG@10': 0.7408}\n  drop vs clean (swap): {'ΔMRR@1': -0.005051, 'ΔMRR@5': -0.007127, 'ΔMRR@10': -0.007158}\n\n=== LAReQA EN→RU | FT_INST | noise:lower ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.6961, 'Recall@1': 0.6961, 'MRR@1': 0.6961, 'MAP@1': 0.6961, 'nDCG@1': 0.6961, 'Recall@5': 0.9108, 'MRR@5': 0.784, 'MAP@5': 0.6817, 'nDCG@5': 0.7337, 'Recall@10': 0.9503, 'MRR@10': 0.7892, 'MAP@10': 0.685, 'nDCG@10': 0.7453}\n  drop vs clean (lower): {'ΔMRR@1': -0.003367, 'ΔMRR@5': -0.001964, 'ΔMRR@10': -0.002135}\n\n=== LAReQA EN→RU | FT_QUERY | plain ===\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"[cache] docs torch.Size([1219, 768]) saved -> /kaggle/working/robust_eval_cache/docs__lareqa_EN→RU__qonly__5723c199__L256.pt\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.7222, 'Recall@1': 0.7222, 'MRR@1': 0.7222, 'MAP@1': 0.7222, 'nDCG@1': 0.7222, 'Recall@5': 0.9285, 'MRR@5': 0.8044, 'MAP@5': 0.7005, 'nDCG@5': 0.7511, 'Recall@10': 0.9588, 'MRR@10': 0.8086, 'MAP@10': 0.7027, 'nDCG@10': 0.7602}\n\n=== LAReQA EN→RU | FT_QUERY | q+BANK ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.7264, 'Recall@1': 0.7264, 'MRR@1': 0.7264, 'MAP@1': 0.7264, 'nDCG@1': 0.7264, 'Recall@5': 0.92, 'MRR@5': 0.8027, 'MAP@5': 0.6982, 'nDCG@5': 0.7474, 'Recall@10': 0.9503, 'MRR@10': 0.8069, 'MAP@10': 0.7006, 'nDCG@10': 0.7569}\n\n--- LAReQA p-MRR (BANK − plain) :: EN→RU :: FT_QUERY ---\n{'pMRR@1': 0.004209, 'pMRR@5': -0.001655, 'pMRR@10': -0.001672}\n\n--- Prompt ablation :: EN→RU :: FT_QUERY ---\n\n=== LAReQA EN→RU | FT_QUERY | prompt:plain ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.7222, 'Recall@1': 0.7222, 'MRR@1': 0.7222, 'MAP@1': 0.7222, 'nDCG@1': 0.7222, 'Recall@5': 0.9285, 'MRR@5': 0.8044, 'MAP@5': 0.7005, 'nDCG@5': 0.7511, 'Recall@10': 0.9588, 'MRR@10': 0.8086, 'MAP@10': 0.7027, 'nDCG@10': 0.7602}\n\n=== LAReQA EN→RU | FT_QUERY | prompt:minimal_en ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.729, 'Recall@1': 0.729, 'MRR@1': 0.729, 'MAP@1': 0.729, 'nDCG@1': 0.729, 'Recall@5': 0.9209, 'MRR@5': 0.8047, 'MAP@5': 0.6997, 'nDCG@5': 0.7487, 'Recall@10': 0.952, 'MRR@10': 0.8091, 'MAP@10': 0.7027, 'nDCG@10': 0.759}\n\n=== LAReQA EN→RU | FT_QUERY | prompt:qa_en ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.7079, 'Recall@1': 0.7079, 'MRR@1': 0.7079, 'MAP@1': 0.7079, 'nDCG@1': 0.7079, 'Recall@5': 0.9099, 'MRR@5': 0.7888, 'MAP@5': 0.6845, 'nDCG@5': 0.7348, 'Recall@10': 0.9495, 'MRR@10': 0.7945, 'MAP@10': 0.6884, 'nDCG@10': 0.7472}\n\n=== LAReQA EN→RU | FT_QUERY | prompt:search_en ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.7029, 'Recall@1': 0.7029, 'MRR@1': 0.7029, 'MAP@1': 0.7029, 'nDCG@1': 0.7029, 'Recall@5': 0.9116, 'MRR@5': 0.7869, 'MAP@5': 0.6832, 'nDCG@5': 0.7337, 'Recall@10': 0.9495, 'MRR@10': 0.792, 'MAP@10': 0.6865, 'nDCG@10': 0.7454}\n\n=== LAReQA EN→RU | FT_QUERY | prompt:mfoll_en ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.7315, 'Recall@1': 0.7315, 'MRR@1': 0.7315, 'MAP@1': 0.7315, 'nDCG@1': 0.7315, 'Recall@5': 0.9234, 'MRR@5': 0.807, 'MAP@5': 0.6996, 'nDCG@5': 0.7493, 'Recall@10': 0.9554, 'MRR@10': 0.8115, 'MAP@10': 0.703, 'nDCG@10': 0.7602}\n\n--- Paraphrase robustness (5x) :: EN→RU :: FT_QUERY ---\n\n=== LAReQA EN→RU | FT_QUERY | para1 ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.7189, 'Recall@1': 0.7189, 'MRR@1': 0.7189, 'MAP@1': 0.7189, 'nDCG@1': 0.7189, 'Recall@5': 0.9167, 'MRR@5': 0.7978, 'MAP@5': 0.6947, 'nDCG@5': 0.7443, 'Recall@10': 0.9512, 'MRR@10': 0.8025, 'MAP@10': 0.6972, 'nDCG@10': 0.7541}\n\n=== LAReQA EN→RU | FT_QUERY | para2 ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.6961, 'Recall@1': 0.6961, 'MRR@1': 0.6961, 'MAP@1': 0.6961, 'nDCG@1': 0.6961, 'Recall@5': 0.915, 'MRR@5': 0.7828, 'MAP@5': 0.6784, 'nDCG@5': 0.7305, 'Recall@10': 0.9503, 'MRR@10': 0.7877, 'MAP@10': 0.6816, 'nDCG@10': 0.7416}\n\n=== LAReQA EN→RU | FT_QUERY | para3 ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.6936, 'Recall@1': 0.6936, 'MRR@1': 0.6936, 'MAP@1': 0.6936, 'nDCG@1': 0.6936, 'Recall@5': 0.9024, 'MRR@5': 0.7754, 'MAP@5': 0.6726, 'nDCG@5': 0.7228, 'Recall@10': 0.9495, 'MRR@10': 0.7819, 'MAP@10': 0.6771, 'nDCG@10': 0.7374}\n\n=== LAReQA EN→RU | FT_QUERY | para4 ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.6827, 'Recall@1': 0.6827, 'MRR@1': 0.6827, 'MAP@1': 0.6827, 'nDCG@1': 0.6827, 'Recall@5': 0.9099, 'MRR@5': 0.7731, 'MAP@5': 0.6705, 'nDCG@5': 0.7236, 'Recall@10': 0.9478, 'MRR@10': 0.7785, 'MAP@10': 0.6735, 'nDCG@10': 0.7348}\n\n=== LAReQA EN→RU | FT_QUERY | para5 ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.7256, 'Recall@1': 0.7256, 'MRR@1': 0.7256, 'MAP@1': 0.7256, 'nDCG@1': 0.7256, 'Recall@5': 0.9242, 'MRR@5': 0.805, 'MAP@5': 0.7011, 'nDCG@5': 0.7509, 'Recall@10': 0.9554, 'MRR@10': 0.8093, 'MAP@10': 0.7035, 'nDCG@10': 0.7604}\nParaphrase mean/std: {'Hit@1_mean': 0.7034, 'Hit@1_std': 0.0162, 'Recall@1_mean': 0.7034, 'Recall@1_std': 0.0162, 'MRR@1_mean': 0.7034, 'MRR@1_std': 0.0162, 'MAP@1_mean': 0.7034, 'MAP@1_std': 0.0162, 'nDCG@1_mean': 0.7034, 'nDCG@1_std': 0.0162, 'Recall@5_mean': 0.9136, 'Recall@5_std': 0.0073, 'MRR@5_mean': 0.7868, 'MRR@5_std': 0.0125, 'MAP@5_mean': 0.6835, 'MAP@5_std': 0.0122, 'nDCG@5_mean': 0.7344, 'nDCG@5_std': 0.0113, 'Recall@10_mean': 0.9508, 'Recall@10_std': 0.0025, 'MRR@10_mean': 0.792, 'MRR@10_std': 0.0119, 'MAP@10_mean': 0.6866, 'MAP@10_std': 0.0117, 'nDCG@10_mean': 0.7456, 'nDCG@10_std': 0.0099}\n\n--- Noise stress :: EN→RU :: FT_QUERY ---\n\n=== LAReQA EN→RU | FT_QUERY | noise:clean ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.7222, 'Recall@1': 0.7222, 'MRR@1': 0.7222, 'MAP@1': 0.7222, 'nDCG@1': 0.7222, 'Recall@5': 0.9285, 'MRR@5': 0.8044, 'MAP@5': 0.7005, 'nDCG@5': 0.7511, 'Recall@10': 0.9588, 'MRR@10': 0.8086, 'MAP@10': 0.7027, 'nDCG@10': 0.7602}\n\n=== LAReQA EN→RU | FT_QUERY | noise:nopunct ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.7298, 'Recall@1': 0.7298, 'MRR@1': 0.7298, 'MAP@1': 0.7298, 'nDCG@1': 0.7298, 'Recall@5': 0.9209, 'MRR@5': 0.8067, 'MAP@5': 0.7013, 'nDCG@5': 0.7504, 'Recall@10': 0.9579, 'MRR@10': 0.8119, 'MAP@10': 0.7042, 'nDCG@10': 0.7614}\n  drop vs clean (nopunct): {'ΔMRR@1': 0.007576, 'ΔMRR@5': 0.002287, 'ΔMRR@10': 0.003349}\n\n=== LAReQA EN→RU | FT_QUERY | noise:swap ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.7189, 'Recall@1': 0.7189, 'MRR@1': 0.7189, 'MAP@1': 0.7189, 'nDCG@1': 0.7189, 'Recall@5': 0.9158, 'MRR@5': 0.7966, 'MAP@5': 0.6922, 'nDCG@5': 0.742, 'Recall@10': 0.9537, 'MRR@10': 0.8018, 'MAP@10': 0.6956, 'nDCG@10': 0.7537}\n  drop vs clean (swap): {'ΔMRR@1': -0.003367, 'ΔMRR@5': -0.007814, 'ΔMRR@10': -0.006763}\n\n=== LAReQA EN→RU | FT_QUERY | noise:lower ===\n[cache] docs torch.Size([1219, 768]) loaded\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"{'N': 1188, 'Hit@1': 0.7298, 'Recall@1': 0.7298, 'MRR@1': 0.7298, 'MAP@1': 0.7298, 'nDCG@1': 0.7298, 'Recall@5': 0.92, 'MRR@5': 0.8045, 'MAP@5': 0.7012, 'nDCG@5': 0.7496, 'Recall@10': 0.9537, 'MRR@10': 0.8094, 'MAP@10': 0.7048, 'nDCG@10': 0.7611}\n  drop vs clean (lower): {'ΔMRR@1': 0.007576, 'ΔMRR@5': 9.8e-05, 'ΔMRR@10': 0.000762}\n\n=============== DONE ===============\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}