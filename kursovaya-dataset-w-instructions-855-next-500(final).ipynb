{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12962398,"sourceType":"datasetVersion","datasetId":8203748}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\nSRC = \"/kaggle/input/tevatron-msmarco-translated-ru/tevatron_msmarco_ru.parquet\"\nDST = \"/kaggle/working/tevatron_msmarco_ru_rows_855_1355.parquet\"\n\nstart_pos = 855          \nend_pos_inclusive = 1355 \n\ndf = pd.read_parquet(SRC, engine=\"pyarrow\")\n\nn = len(df)\nstart = max(0, start_pos)\nend_exclusive = min(n, end_pos_inclusive + 1)\n\ndf_slice = df.iloc[start:end_exclusive].copy()\n\ndf_slice.to_parquet(DST, index=False, engine=\"pyarrow\")\n\nprint(\"Total rows:\", n)\nprint(\"Slice shape:\", df_slice.shape)\nprint(\"First/last positions in slice (by original order):\", start, end_exclusive - 1)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-04T16:53:42.812986Z","iopub.execute_input":"2025-09-04T16:53:42.813226Z","iopub.status.idle":"2025-09-04T16:53:46.224455Z","shell.execute_reply.started":"2025-09-04T16:53:42.813198Z","shell.execute_reply":"2025-09-04T16:53:46.223707Z"}},"outputs":[{"name":"stdout","text":"Total rows: 3600\nSlice shape: (501, 6)\nFirst/last positions in slice (by original order): 855 1355\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip -q install --upgrade transformers accelerate bitsandbytes safetensors sentencepiece \"fsspec<=2025.3.0\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T16:53:49.551879Z","iopub.execute_input":"2025-09-04T16:53:49.552602Z","iopub.status.idle":"2025-09-04T16:55:30.078642Z","shell.execute_reply.started":"2025-09-04T16:53:49.552567Z","shell.execute_reply":"2025-09-04T16:55:30.077626Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.9/374.9 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.8/485.8 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os, re, gc, json, time, random, hashlib, math\nfrom typing import List, Dict, Any, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom tqdm import tqdm\nfrom transformers import (\n    AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n)\n\nINPUT_PATH = \"/kaggle/working/tevatron_msmarco_ru_rows_855_1355.parquet\"\nOUTPUT_JSONL = \"/kaggle/working/out_instructions_855_1355.jsonl\"\nDEBUG_LOG    = \"/kaggle/working/debug_bad_generations_855_1355.txt\"\n\nSESSION_LIMIT = 500\nSEED = 42\nrandom.seed(SEED)\n\nMODEL_NAME = \"RefalMachine/RuadaptQwen2.5-7B-Lite-Beta\"\nbnb_cfg = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n)\n\nMAX_NEW_TOKENS = 180\nTEMPERATURE    = 0.8\nTOP_P          = 0.9\n\nMAX_NEGS_PER_PROMPT = 5\n\nLENGTH_PROFILES = [\n    (\"short_strict\",    \"1–2 предложения, строгий тон, без подсказки ответа\"),\n    (\"persona\",         \"2–4 простых предложения для старшеклассников, но без ответа\"),\n    (\"background_long\", \"4–6 предложений с контекстом и ограничениями, но без ответа\"),\n]\n\nSYSTEM_MSG = (\n    \"Ты — генератор инструкций для задачи dense retrieval. \"\n    \"Тебе дан запрос, один РЕЛЕВАНТНЫЙ документ [1] и несколько НЕРЕЛЕВАНТНЫХ документов [2..N]. \"\n    \"Сгенерируй ДОПОЛНИТЕЛЬНУЮ ИНСТРУКЦИЮ (на русском), которую можно приписать к концу запроса так, \"\n    \"чтобы документ [1] оставался релевантным, а ВСЕ остальные документы [2..N] стали нерелевантны. \"\n    \"Не цитируй документы и не раскрывай ответ; формулируй критерии релевантности. \"\n    \"Верни ТОЛЬКО JSON-объект без дополнительного текста.\"\n)\n\nUSER_TEMPLATE = (\n    \"## Input Data\\n\"\n    \"У меня есть следующий запрос и 1 документ, помеченный как релевантный, и {nneg} — как нерелевантные.\\n\"\n    \"Query: {query}\\n\\n\"\n    \"Relevant document [1]:\\n{pos}\\n\\n\"\n    \"Non-relevant documents:\\n{neg_block}\\n\\n\"\n    \"## Your task\\n\"\n    \"Нужно придумать инструкцию, чтобы релевантным остался ТОЛЬКО документ [1], \"\n    \"а все остальные [2..{max_id}] стали нерелевантны. НЕ подсказывай ответ и НЕ цитируй документы. \"\n    \"Инструкция должна быть длины: {length_format}.\\n\"\n    \"Опиши конкретные критерии релевантности, чтобы [1] подходил, а [2..{max_id}] — нет. \"\n    \"Верни JSON ТОЛЬКО с ключами:\\n\"\n    '  \"instruction\" : str,\\n'\n    '  \"relevant_docs\" : \"[1]\",\\n'\n    '  \"non-relevant_docs\" : \"[2,3,...]\".\\n'\n    \"## Your output (JSON only):\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T16:55:37.625591Z","iopub.execute_input":"2025-09-04T16:55:37.625888Z","iopub.status.idle":"2025-09-04T16:56:09.594854Z","shell.execute_reply.started":"2025-09-04T16:55:37.625863Z","shell.execute_reply":"2025-09-04T16:56:09.594215Z"}},"outputs":[{"name":"stderr","text":"2025-09-04 16:55:52.238216: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757004952.583156      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757004952.683211      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def load_rows(path: str) -> List[Dict[str, Any]]:\n    if path.endswith(\".jsonl\"):\n        return [json.loads(l) for l in open(path, \"r\", encoding=\"utf-8\")]\n    df = pd.read_parquet(path)\n    return df.to_dict(\"records\")\n\ndef normalize_hnegs(val) -> List[str]:\n    out = []\n    if isinstance(val, list):\n        it = val\n    elif isinstance(val, str) and val.strip():\n        try:\n            j = json.loads(val)\n            it = j if isinstance(j, list) else [val]\n        except Exception:\n            it = [val]\n    else:\n        it = []\n    for x in it:\n        s = str(x).strip()\n        if s:\n            out.append(s)\n    seen = set()\n    uniq = []\n    for s in out:\n        if s not in seen:\n            uniq.append(s); seen.add(s)\n    return uniq\n\ndef stable_row_id(q: str, p: str) -> str:\n    h = hashlib.md5()\n    h.update((q + \"\\n\" + p).encode(\"utf-8\"))\n    return h.hexdigest()\n\ndef load_done_ids(out_path: str) -> set:\n    done = set()\n    if os.path.exists(out_path):\n        with open(out_path, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                try:\n                    item = json.loads(line)\n                    rid = item.get(\"_row_id\")\n                    if rid:\n                        done.add(rid)\n                except Exception:\n                    pass\n    return done\n\ndef to_py(o):\n    if isinstance(o, dict):\n        return {k: to_py(v) for k, v in o.items()}\n    if isinstance(o, (list, tuple, set)):\n        return [to_py(v) for v in o]\n    if isinstance(o, np.ndarray):\n        return o.tolist()\n    if isinstance(o, (np.integer,)):\n        return int(o)\n    if isinstance(o, (np.floating,)):\n        return float(o)\n    if isinstance(o, (np.bool_,)):\n        return bool(o)\n    return o\n\ndef append_row(out_path: str, row: Dict[str, Any]):\n    with open(out_path, \"a\", encoding=\"utf-8\") as f:\n        f.write(json.dumps(to_py(row), ensure_ascii=False) + \"\\n\")\n\ndef log_bad(query: str, pos: str, negs: List[str], raw: str, reason: str):\n    os.makedirs(os.path.dirname(DEBUG_LOG), exist_ok=True)\n    with open(DEBUG_LOG, \"a\", encoding=\"utf-8\") as f:\n        f.write(\"=\"*80 + \"\\n\")\n        f.write(\"QUERY: \" + (query[:300] if query else \"\") + \"\\n\")\n        f.write(\"POS   : \" + (pos[:600] if pos else \"\") + \"\\n\")\n        if negs:\n            for j, t in enumerate(negs[:MAX_NEGS_PER_PROMPT], start=2):\n                f.write(f\"NEG[{j}]: {t[:400]}\\n\")\n        f.write(\"RAW   : \" + raw[:1200] + \"\\n\")\n        f.write(\"REASON: \" + reason + \"\\n\")\n\ndef is_russian_text(s: str, threshold: float = 0.4) -> bool:\n    if not s:\n        return False\n    cyr = sum('а' <= ch.lower() <= 'я' or ch.lower() == 'ё' for ch in s)\n    return (cyr / max(1, len(s))) >= threshold\n\ndef basic_filter(inst_text: str) -> bool:\n    if not is_russian_text(inst_text, 0.4):\n        return False\n    non_ws_len = len(re.sub(r\"\\s+\", \"\", inst_text))\n    if non_ws_len > 800:\n        return False\n    return True\n\ndef parse_promptriever_json(text: str) -> Dict[str, Any] | None:\n\n    m = re.search(r\"\\{[\\s\\S]*\\}\", text)\n    if not m:\n        return None\n    try:\n        obj = json.loads(m.group(0))\n        if not isinstance(obj, dict):\n            return None\n    except Exception:\n        return None\n\n    nr_key = None\n    for k in obj.keys():\n        if k.lower().replace(\"_\", \"-\") == \"non-relevant-docs\" or k.lower().replace(\"_\", \"-\") == \"non-relevant_docs\":\n            nr_key = k\n            break\n    if nr_key is None and \"non-relevant_docs\" in obj:\n        nr_key = \"non-relevant_docs\"\n\n    out = {\n        \"instruction\": obj.get(\"instruction\", \"\").strip(),\n        \"relevant_docs\": obj.get(\"relevant_docs\", \"\").strip(),\n        \"non-relevant_docs\": obj.get(nr_key, \"\").strip() if nr_key else \"\",\n    }\n    return out\n\ndef make_neg_block(negs: List[str]) -> str:\n    lines = []\n    for idx, t in enumerate(negs, start=2):\n        lines.append(f\"[{idx}] {t}\")\n    return \"\\n\".join(lines)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T16:56:15.125119Z","iopub.execute_input":"2025-09-04T16:56:15.125777Z","iopub.status.idle":"2025-09-04T16:56:15.144920Z","shell.execute_reply.started":"2025-09-04T16:56:15.125751Z","shell.execute_reply":"2025-09-04T16:56:15.144033Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"assert torch.cuda.is_available(), \"Enable GPU (Runtime > Accelerator > GPU).\"\n\nprint(\"Loading tokenizer/model:\", MODEL_NAME)\ntok = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\ntok.padding_side = \"left\"\nif tok.pad_token_id is None:\n    tok.pad_token = tok.eos_token\nprint(\"padding_side:\", tok.padding_side, \"pad_token_id:\", tok.pad_token_id, \"eos_token_id:\", tok.eos_token_id)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n    quantization_config=bnb_cfg,\n    trust_remote_code=True,\n)\nmodel.config.pad_token_id = tok.pad_token_id\n\ngenerate = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tok,\n    return_full_text=False,\n)\n\neos_id = tok.eos_token_id if tok.eos_token_id is not None else model.config.eos_token_id\nprint(\"Model loaded. EOS id:\", eos_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T16:56:19.824755Z","iopub.execute_input":"2025-09-04T16:56:19.825219Z","iopub.status.idle":"2025-09-04T16:57:29.942382Z","shell.execute_reply.started":"2025-09-04T16:56:19.825195Z","shell.execute_reply":"2025-09-04T16:57:29.941528Z"}},"outputs":[{"name":"stdout","text":"Loading tokenizer/model: RefalMachine/RuadaptQwen2.5-7B-Lite-Beta\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34078a73df9f4a509fa884b03486ce28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a3b4aaa3d004a09939400e8a8ebab77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7975bdf4bf1e415185a3e75bdd3adad3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/12.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5db67c64c83b4240b64e682982401dc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"627b96364d9a4e97b6e3a0cfaf00ff85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/759 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f56407ecb7b84c4595f9ca0bbb684902"}},"metadata":{}},{"name":"stdout","text":"padding_side: left pad_token_id: 145109 eos_token_id: 145111\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/840 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e082aec4d7b047db8e3ef4e1a546f10e"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1bd592246984902b7a9a6321f944f45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"031656484def408e85ca55068a76f050"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfc7c07ea1e14f748d6aabe295c1e83b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.14G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c5c3188bc8f4063863bf88d7ed7fec2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.04G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"430d028e844b44cc8314f8d517a2537e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"645d6f7ddc5b4a189c2ccb76f2467f5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f2bc299d63945898319d8b404e33182"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/195 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31a035a85ace4ebc8fc08148d64955a1"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Model loaded. EOS id: 145111\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"rows = load_rows(INPUT_PATH)\nprint(\"Total rows in dataset:\", len(rows))\n\ndone_ids = load_done_ids(OUTPUT_JSONL)\nprint(\"Already done:\", len(done_ids))\n\nprocessed_this_session = 0\n\nema_sec_per_ex = None\nt0 = time.time()\n\npbar = tqdm(total=min(SESSION_LIMIT, len(rows)), desc=\"Generating instructions (session)\")\n\nfor r in rows:\n    if processed_this_session >= SESSION_LIMIT:\n        break\n\n    q = (r.get(\"query_ru\") or \"\").strip()\n    p = (r.get(\"positive_ru\") or \"\").strip()\n    if not q or not p:\n        continue\n\n    rid = stable_row_id(q, p)\n    if rid in done_ids:\n        continue\n\n    raw_negs = normalize_hnegs(r.get(\"hard_negs_ru\", []))\n    negs = raw_negs[:MAX_NEGS_PER_PROMPT]\n    if not negs:\n        negs = [\"(пусто) Нерелевантный фрагмент отсутствует — добавлен заполнитель.\"]\n\n    neg_block = make_neg_block(negs)\n    max_id = 1 + len(negs)\n\n    sub_prompts = []\n    meta_profiles = []\n    for style_name, length_fmt in LENGTH_PROFILES:\n        user = USER_TEMPLATE.format(\n            query=q,\n            pos=p,\n            nneg=len(negs),\n            neg_block=neg_block,\n            max_id=max_id,\n            length_format=length_fmt\n        )\n        messages = [\n            {\"role\": \"system\", \"content\": SYSTEM_MSG},\n            {\"role\": \"user\", \"content\": user},\n        ]\n        prompt = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n        sub_prompts.append(prompt)\n        meta_profiles.append((style_name, length_fmt))\n\n    t_start = time.time()\n    try:\n        outs = generate(\n            sub_prompts,\n            do_sample=True,\n            temperature=TEMPERATURE,\n            top_p=TOP_P,\n            max_new_tokens=MAX_NEW_TOKENS,\n            eos_token_id=eos_id,\n        )\n    except torch.cuda.OutOfMemoryError:\n        torch.cuda.empty_cache()\n        time.sleep(2)\n        outs = generate(\n            sub_prompts,\n            do_sample=True,\n            temperature=TEMPERATURE,\n            top_p=TOP_P,\n            max_new_tokens=MAX_NEW_TOKENS,\n            eos_token_id=eos_id,\n        )\n    t_end = time.time()\n\n    kept_objs = []\n    for out_item, (style_name, length_fmt) in zip(outs, meta_profiles):\n        gen_text = out_item[0][\"generated_text\"] if isinstance(out_item, list) else out_item[\"generated_text\"]\n\n        parsed = parse_promptriever_json(gen_text)\n        if parsed is None:\n            log_bad(q, p, negs, gen_text, \"No JSON object found\")\n            continue\n\n        instr = parsed.get(\"instruction\", \"\").strip()\n\n        rel_docs = parsed.get(\"relevant_docs\", \"\").strip()\n        nonrel_docs = parsed.get(\"non-relevant_docs\", \"\").strip()\n        if not rel_docs:\n            rel_docs = \"[1]\"\n        if not nonrel_docs:\n            nonrel_docs = \"[\" + \",\".join(str(i) for i in range(2, max_id+1)) + \"]\"\n\n        ok_rel = \"1\" in re.sub(r\"\\D+\", \"\", rel_docs) or rel_docs.strip() == \"[1]\"\n        if not ok_rel:\n            log_bad(q, p, negs, gen_text, \"relevant_docs does not contain [1]\")\n            continue\n        if not basic_filter(instr):\n            log_bad(q, p, negs, gen_text, \"basic_filter failed (lang or length)\")\n            continue\n\n        kept_objs.append({\n            \"style\": style_name,\n            \"length_format\": length_fmt,\n            \"instruction\": instr,\n            \"relevant_docs\": rel_docs,\n            \"non-relevant_docs\": nonrel_docs,\n        })\n\n    out_obj = dict(r)\n    out_obj[\"_row_id\"] = rid\n    out_obj[\"instructions\"] = kept_objs\n    append_row(OUTPUT_JSONL, out_obj)\n    done_ids.add(rid)\n    processed_this_session += 1\n\n    dt = t_end - t_start\n    if dt <= 0:\n        dt = 1e-6\n    if ema_sec_per_ex is None:\n        ema_sec_per_ex = dt\n    else:\n        ema_sec_per_ex = 0.9 * ema_sec_per_ex + 0.1 * dt\n    remain = SESSION_LIMIT - processed_this_session\n    eta_sec = max(0, int(ema_sec_per_ex * remain))\n    pbar.set_postfix_str(f\"EMA s/ex={ema_sec_per_ex:.2f}, ETA≈{eta_sec//3600:d}:{(eta_sec%3600)//60:02d}:{eta_sec%60:02d}\")\n    pbar.update(1)\n\npbar.close()\nprint(f\"Session finished. Added {processed_this_session} rows to {OUTPUT_JSONL}\")\nprint(\"Bad generations log (if any):\", DEBUG_LOG)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T16:57:33.681061Z","iopub.execute_input":"2025-09-04T16:57:33.681415Z","iopub.status.idle":"2025-09-04T21:24:50.031582Z","shell.execute_reply.started":"2025-09-04T16:57:33.681391Z","shell.execute_reply":"2025-09-04T21:24:50.030873Z"}},"outputs":[{"name":"stdout","text":"Total rows in dataset: 501\nAlready done: 0\n","output_type":"stream"},{"name":"stderr","text":"Generating instructions (session):   2%|▏         | 10/500 [05:39<4:40:40, 34.37s/it, EMA s/ex=33.96, ETA≈4:37:18]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\nGenerating instructions (session): 100%|██████████| 500/500 [4:27:16<00:00, 32.07s/it, EMA s/ex=32.84, ETA≈0:00:00]  ","output_type":"stream"},{"name":"stdout","text":"Session finished. Added 500 rows to /kaggle/working/out_instructions_855_1355.jsonl\nBad generations log (if any): /kaggle/working/debug_bad_generations_855_1355.txt\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":6}]}